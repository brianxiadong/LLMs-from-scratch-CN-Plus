{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "136a4efe-fb99-4311-8679-e0a5b6282755",
      "metadata": {},
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary 代码 for 这个 <一个 href=\"http://mng.bz/orYv\">构建 一个 大语言模型 From Scratch</一个> book by <一个 href=\"https://sebastianraschka.com\">Sebastian Raschka</一个><br>\n",
        "<br>代码 repository: <一个 href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</一个>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<一个 href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></一个>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1910a06-e8a3-40ac-8201-ff70615b1ba4",
      "metadata": {
        "tags": []
      },
      "source": [
        "# Generating 一个 Preference 数据集 With Llama 3.1 70B 和 Ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a128651b-f326-4232-a994-42f38b7ed520",
      "metadata": {},
      "source": [
        "- Preference finetuning is 一个 处理 to align 一个 instruction-finetuned 大语言模型 with human preferences\n",
        "- 那里 are multiple ways to 创建 一个 数据集 for preference finetuning 一个 大语言模型\n",
        "  1. 我们 使用 这个 instruction-finetuned 大语言模型 to 生成 multiple responses 和 have humans rank them based on their preference 和/或者 given preference criteria\n",
        "  2. 我们 使用 这个 instruction-finetuned 大语言模型 to 生成 multiple responses 和 have LLMs rank them based on given preference criteria\n",
        "  3. 我们 使用 一个 大语言模型 to 生成 preferred 和 dispreferred responses given certain preference criteria\n",
        "- In 这个 笔记本, 我们 consider approach 3\n",
        "- 这个 笔记本 uses 一个 70-billion-参数 Llama 3.1-Instruct 模型 through ollama to 生成 preference labels for 一个 instruction 数据集\n",
        "- 这个 expected format of 这个 instruction 数据集 is as follows:\n",
        "\n",
        "\n",
        "### 输入\n",
        "\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"instruction\": \"什么 is 这个 state capital of California?\",\n",
        "        \"输入\": \"\",\n",
        "        \"输出\": \"这个 state capital of California is Sacramento.\",\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Provide 一个 synonym for 'fast'.\",\n",
        "        \"输入\": \"\",\n",
        "        \"输出\": \"一个 synonym for 'fast' is 'quick'.\",\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"什么 is 这个 capital of Greece?\",\n",
        "        \"输入\": \"\",\n",
        "        \"输出\": \"这个 capital of Greece is Athens.\",\n",
        "\n",
        "    },\n",
        "...\n",
        "]\n",
        "```\n",
        "\n",
        "这个 输出 数据集 will look as follows, 哪里 more polite responses are preferred (`'chosen'`), 和 more impolite responses are dispreferred (`'rejected'`):\n",
        "\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"instruction\": \"什么 is 这个 state capital of California?\",\n",
        "        \"输入\": \"\",\n",
        "        \"输出\": \"这个 state capital of California is Sacramento.\",\n",
        "        \"rejected\": \"Look, 这个 state capital of California is obviously Sacramento.\",\n",
        "        \"chosen\": \"这个 state capital of California is Sacramento.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Provide 一个 synonym for 'fast'.\",\n",
        "        \"输入\": \"\",\n",
        "        \"输出\": \"一个 synonym for 'fast' is 'quick'.\",\n",
        "        \"chosen\": \"一个 suitable alternative to 'fast' would be 'quick'.\",\n",
        "        \"rejected\": \"一个 synonym for 'fast' is 'quick'.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"什么 is 这个 capital of Greece?\",\n",
        "        \"输入\": \"\",\n",
        "        \"输出\": \"这个 capital of Greece is Athens.\",\n",
        "        \"chosen\": \"I'd be happy to help! 这个 capital of Greece is indeed Athens.\",\n",
        "        \"rejected\": \"这个 capital of Greece is Athens.\"\n",
        "    },\n",
        "...\n",
        "]\n",
        "```\n",
        "\n",
        "### 输出\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- 这个 代码 doesn't require 一个 GPU 和 runs on 一个 laptop given enough RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "63610acc-db94-437f-8d38-e99dca0299cb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tqdm version: 4.66.4\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n\npkgs = [\"tqdm\",    # Progress bar\n        ]\n\nfor p in pkgs:\n    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bcdcb34-ac75-4f4f-9505-3ce0666c42d5",
      "metadata": {},
      "source": [
        "## Installing Ollama 和 Downloading Llama 3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a092280-5462-4709-a3fe-8669a4a8a0a6",
      "metadata": {},
      "source": [
        "- Ollama is 一个 application to 运行 LLMs efficiently\n",
        "- 它 is 一个 wrapper around [llama.cpp](https://github.com/ggerganov/llama.cpp), 哪个 implements LLMs in pure C/C++ to maximize efficiency\n",
        "- Note 那个 它 is 一个 tool for using LLMs to 生成 text (推理), not 训练 或者 finetuning LLMs\n",
        "- Prior to running 这个 代码 below, 安装 ollama by visiting [https://ollama.com](https://ollama.com) 和 following 这个 instructions (for instance, clicking on 这个 \"Download\" button 和 downloading 这个 ollama application for your operating system)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9558a522-650d-401a-84fc-9fd7b1f39da7",
      "metadata": {},
      "source": [
        "- For macOS 和 Windows users, click on 这个 ollama application 你 downloaded; 如果 它 prompts 你 to 安装 这个 command line usage, say \"yes\"\n",
        "- Linux users can 使用 这个 安装 command provided on 这个 ollama website\n",
        "\n",
        "- In general, before 我们 can 使用 ollama from 这个 command line, 我们 have to either 开始 这个 ollama application 或者 运行 `ollama serve` in 一个 separate terminal\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/ollama-eval/ollama-serve.webp?1\">\n",
        "\n",
        "\n",
        "- With 这个 ollama application 或者 `ollama serve` running, in 一个 different terminal, on 这个 command line, 执行 这个 following command to try out 这个 70-billion-参数 Llama 3.1 模型 \n",
        "\n",
        "```bash\n",
        "# 70B 模型\n",
        "ollama 运行 llama3.1:70b\n",
        "```\n",
        "\n",
        "\n",
        "这个 输出 looks like as follows:\n",
        "\n",
        "```\n",
        "$ ollama 运行 llama3.1:70b\n",
        "pulling manifest\n",
        "pulling aa81b541aae6... 100% ▕████████████████▏ 39 GB\n",
        "pulling 8cf247399e57... 100% ▕████████████████▏ 1.7 KB\n",
        "pulling f1cd752815fc... 100% ▕████████████████▏ 12 KB\n",
        "pulling 56bb8bd477a5... 100% ▕████████████████▏ 96 B\n",
        "pulling 3c1c2d3df5b3... 100% ▕████████████████▏ 486 B\n",
        "verifying sha256 digest\n",
        "writing manifest\n",
        "removing any unused layers\n",
        "success\n",
        "```\n",
        "\n",
        "- Note 那个 `llama3.1:70b` refers to 这个 instruction finetuned 70-billion-参数 Llama 3.1 模型\n",
        "\n",
        "- Alternatively, 你 can also 使用 这个 smaller, more resource-effiicent 8-billion-parameters Llama 3.1 模型, by replacing `llama3.1:70b` with `llama3.1`\n",
        "\n",
        "- After 这个 download has been completed, 你 will see 一个 command line prompt 那个 allows 你 to chat with 这个 模型\n",
        "\n",
        "- Try 一个 prompt like \"什么 do llamas eat?\", 哪个 should 返回 一个 输出 similar to 这个 following:\n",
        "\n",
        "```\n",
        ">>> 什么 do llamas eat?\n",
        "Llamas are ruminant animals, 哪个 means they have 一个 four-chambered \n",
        "stomach 和 eat plants 那个 are high in fiber. In 这个 wild, llamas \n",
        "typically feed on:\n",
        "1. Grasses: They love to graze on various types of grasses, including tall \n",
        "grasses, wheat, oats, 和 barley.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b5addcb-fc7d-455d-bee9-6cc7a0d684c7",
      "metadata": {},
      "source": [
        "- 你 can 结束 这个 session using 这个 输入 `/bye`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda155ee-cf36-44d3-b634-20ba8e1ca38a",
      "metadata": {},
      "source": [
        "## Using Ollama's REST API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89343a84-0ddc-42fc-bf50-298a342b93c0",
      "metadata": {},
      "source": [
        "- 现在, 一个 alternative way to interact with 这个 模型 is via its REST API in Python via 这个 following 函数\n",
        "- Before 你 运行 这个 接下来 cells in 这个 笔记本, make sure 那个 ollama is still running, as described above, via\n",
        "  - `ollama serve` in 一个 terminal\n",
        "  - 这个 ollama application\n",
        "- 接下来, 运行 这个 following 代码 cell to query 这个 模型"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16642a48-1cab-40d2-af08-ab8c2fbf5876",
      "metadata": {},
      "source": [
        "- 首先, 让我们 try 这个 API with 一个 simple 示例 to make sure 它 works as intended:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "65b0ba76-1fb1-4306-a7c2-8f3bb637ccdb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Llamas are herbivores, which means they primarily eat plants and plant-based foods. Their diet consists of:\n",
            "\n",
            "1. **Grasses**: Various types of grasses, including timothy grass, orchard grass, and brome grass.\n",
            "2. **Hay**: High-quality hay, such as alfalfa or clover hay, is a staple in a llama's diet.\n",
            "3. **Leaves**: Leaves from trees and shrubs, like willow, cottonwood, and mesquite, are also eaten.\n",
            "4. **Fruits and vegetables**: Llamas enjoy fruits like apples, carrots, and sweet potatoes, as well as leafy greens like kale and spinach.\n",
            "5. **Grains**: In moderation, llamas can eat grains like oats, barley, and corn.\n",
            "\n",
            "It's essential to note that llamas have a unique digestive system, with a three-part stomach and a large cecum (a specialized part of the large intestine). This allows them to break down and extract nutrients from plant material more efficiently than many other animals.\n",
            "\n",
            "A typical llama diet might consist of:\n",
            "\n",
            "* 1-2% of their body weight in hay per day\n",
            "* 0.5-1% of their body weight in grains per day (if fed)\n",
            "* Free-choice access to fresh water\n",
            "* Limited amounts of fruits and vegetables as treats\n",
            "\n",
            "It's also important to ensure that llamas have access to a mineral supplement, such as a salt lick or loose minerals, to help maintain optimal health.\n",
            "\n",
            "Remember, every llama is different, and their dietary needs may vary depending on factors like age, size, and activity level. Consult with a veterinarian or experienced llama breeder for specific guidance on feeding your llama.\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\nimport json\n\n\ndef query_model(prompt, model=\"llama3.1:70b\", url=\"http://localhost:11434/api/chat\"):\n    # 创建 这个 data payload as 一个 dictionary\n    data = {\n        \"model\": model,\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ],\n        \"options\": {\n            \"seed\": 123,\n            \"temperature\": 0,\n        }\n    }\n\n    # 转换 这个 dictionary to 一个 JSON formatted string 和 encode 它 to bytes\n    payload = json.dumps(data).encode(\"utf-8\")\n\n    # 创建 一个 request object, setting 这个 方法 to POST 和 adding necessary headers\n    request = urllib.request.Request(url, data=payload, method=\"POST\")\n    request.add_header(\"Content-Type\", \"application/json\")\n\n    # Send 这个 request 和 capture 这个 response\n    response_data = \"\"\n    with urllib.request.urlopen(request) as response:\n        # Read 和 decode 这个 response\n        while True:\n            line = response.readline().decode(\"utf-8\")\n            if not line:\n                break\n            response_json = json.loads(line)\n            response_data += response_json[\"message\"][\"content\"]\n\n    return response_data\n\n\nresult = query_model(\"What do Llamas eat?\")\nprint(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "162a4739-6f03-4092-a5c2-f57a0b6a4c4d",
      "metadata": {},
      "source": [
        "## 加载 JSON Entries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca011a8b-20c5-4101-979e-9b5fccf62f8a",
      "metadata": {},
      "source": [
        "- 现在, 让我们 获取 to 这个 data generation part\n",
        "- 这里, for 一个 hands-on 示例, 我们 使用 这个 `instruction-data.json` file 那个 我们 originally used to instruction-finetune 这个 模型 in 第 7:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8b2d393a-aa92-4190-9d44-44326a6f699b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of entries: 1100\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n\njson_file = Path(\"..\", \"01_main-chapter-code\", \"instruction-data.json\")\n\nwith open(json_file, \"r\") as file:\n    json_data = json.load(file)\n\nprint(\"Number of entries:\", len(json_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c9751b-59b7-43fe-acc7-14e8daf2fa66",
      "metadata": {},
      "source": [
        "- 这个 structure of 这个 file is as follows, 哪里 我们 have 这个 given response in 这个 测试 数据集 (`'输出'`) 那个 我们 trained 这个 模型 to 生成 via instruction finetuning based on 这个 `'输入'` 和 `'instruction'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7222fdc0-5684-4f2b-b741-3e341851359e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'instruction': 'Evaluate the following phrase by transforming it into the spelling given.',\n",
              " 'input': 'freind --> friend',\n",
              " 'output': 'The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "json_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcf0331b-6024-4bba-89a9-a088b14a1046",
      "metadata": {},
      "source": [
        "- Below is 一个 small utility 函数 那个 formats 这个 instruction 和 输入:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "43263cd3-e5fb-4ab5-871e-3ad6e7d21a8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_input(entry):\n    instruction_text = (\n        f\"Below is an instruction that describes a task. Write a response that \"\n        f\"appropriately completes the request.\"\n        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n    )\n\n    input_text = f\"\\n\\n### 输入:\\n{entry['输入']}\" 如果 entry[\"输入\"] else \"\"\n    instruction_text + input_text\n\n    return instruction_text + input_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39a55283-7d51-4136-ba60-f799d49f4098",
      "metadata": {},
      "source": [
        "- 现在, 让我们 try 这个 ollama API to 生成 一个 `'chosen'` 和 `'rejected'` response for preference tuning 一个 模型\n",
        "- 这里, to for illustration purposes, 我们 创建 answers 那个 are more 或者 less polite\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "735cc089-d127-480a-b39d-0782581f0c41",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset response:\n",
            ">> The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n",
            "\n",
            "impolite response:\n",
            ">> The spelling of the given phrase \"freind\" is flat out wrong, get it together, the correct spelling is \"friend\".\n",
            "\n",
            "Dataset response:\n",
            ">> He goes to the park every day.\n",
            "\n",
            "polite response:\n",
            ">> He goes to the park daily, if I'm not mistaken.\n",
            "\n",
            "Dataset response:\n",
            ">> 45 kilometers is 45000 meters.\n",
            "\n",
            "polite response:\n",
            ">> 45 kilometers is equivalent to 45000 meters.\n",
            "\n",
            "Dataset response:\n",
            ">> Although it was raining, they went for a walk.\n",
            "\n",
            "polite response:\n",
            ">> Although it was raining outside, they still decided to go for a walk.\n",
            "\n",
            "Dataset response:\n",
            ">> 1, 4, 9, 16, 25, 36, 49, 64, 81, 100.\n",
            "\n",
            "impolite response:\n",
            ">> Here are your precious square numbers: 1, 4, 9, 16, 25, 36, 49, 64, 81, 100.\n"
          ]
        }
      ],
      "source": [
        "import random\n\n\nfor entry in json_data[:5]:\n    \n    politeness = random.choice([\"polite\", \"impolite\"])    \n    prompt = (\n        f\"Given the input `{format_input(entry)}` \"\n        f\"and correct output `{entry['output']}`, \"\n        f\"slightly rewrite the output to be more {politeness}.\"\n        \"Keep the modification minimal.\"\n        \"Only return return the generated response and nothing else.\"\n    )\n    print(\"\\nDataset response:\")\n    print(\">>\", entry['output'])\n    print(f\"\\n{politeness} response:\")\n    print(\">>\", query_model(prompt))    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "142dfaa7-429f-4eb0-b74d-ff327f79547a",
      "metadata": {},
      "source": [
        "- 如果 我们 find 那个 这个 generated responses above look reasonable, 我们 can go to 这个 接下来 step 和 应用 这个 prompt to 这个 whole 数据集\n",
        "- 这里, 我们 添加 一个 `'chosen'` key for 这个 preferred response 和 一个 `'rejected'` response for 这个 dispreferred response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3349dbbc-963f-4af3-9790-12dbfdca63c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\nfrom tqdm import tqdm\n\ndef generate_model_responses(json_data):\n\n    for i, entry in enumerate(tqdm(json_data, desc=\"Writing entries\")):\n        politeness = random.choice([\"polite\", \"impolite\"])    \n        prompt = (\n            f\"Given the input `{format_input(entry)}` \"\n            f\"and correct output `{entry['output']}`, \"\n            f\"slightly rewrite the output to be more {politeness}.\"\n            \"Keep the modification minimal.\"\n            \"Only return return the generated response and nothing else.\"\n        )\n        response = query_model(prompt)\n        \n        if politeness == \"polite\":\n            json_data[i][\"chosen\"] = response\n            json_data[i][\"rejected\"] = entry[\"output\"]\n        else:\n            json_data[i][\"rejected\"] = response\n            json_data[i][\"chosen\"] = entry[\"output\"]    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b071ce84-1866-427f-a272-b46700f364b2",
      "metadata": {},
      "source": [
        "- 让我们 现在 应用 这个 evaluation to 这个 whole 数据集 和 计算 这个 average score of each 模型 (这个 takes about 1 minute per 模型 on 一个 M3 MacBook Air laptop)\n",
        "- Note 那个 ollama is not fully deterministic across operating systems (as of 这个 writing) so 这个 numbers 你 are getting might slightly differ from 这个 ones shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4f700d4b-19e5-4404-afa7-b0f093024232",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Writing entries: 100%|██████████| 1100/1100 [17:20<00:00,  1.06it/s]\n"
          ]
        }
      ],
      "source": [
        "generate_model_responses(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "838d9747-0f7d-46fe-aab5-9ee6b765d021",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"instruction-data-with-preference.json\", \"w\") as file:\n    json.dump(json_data, file, indent=4)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}