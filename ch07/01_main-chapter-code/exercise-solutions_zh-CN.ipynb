{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ba450fb1-8a26-4894-ab7a-5d7bfefe90ce",
      "metadata": {},
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary 代码 for 这个 <一个 href=\"http://mng.bz/orYv\">构建 一个 大语言模型 From Scratch</一个> book by <一个 href=\"https://sebastianraschka.com\">Sebastian Raschka</一个><br>\n",
        "<br>代码 repository: <一个 href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</一个>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<一个 href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></一个>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
      "metadata": {},
      "source": [
        "# 第 7 练习 解答"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2625ddc4-9cce-42bd-947d-4e2203fdc55c",
      "metadata": {},
      "source": [
        "## 练习 7.1: Changing prompt styles"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be25a95-2a33-433b-a698-2365b5fc9357",
      "metadata": {},
      "source": [
        "Suppose 我们 have 这个 following data entry:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"instruction\": \"Identify 这个 correct spelling of 这个 following word.\",\n",
        "  \"输入\": \"Ocassion\",\n",
        "  \"输出\": \"这个 correct spelling is 'Occasion.'\"\n",
        "}\n",
        "```\n",
        "\n",
        "In 这个 main 第, 我们 formatted 它 according to 这个 Alpaca-style prompt template:\n",
        "\n",
        "```\n",
        "Below is 一个 instruction 那个 describes 一个 task. Write 一个 response 那个 appropriately completes 这个 request.\n",
        "\n",
        "### Instruction:\n",
        "Identify 这个 correct spelling of 这个 following word.\n",
        "\n",
        "### 输入:\n",
        "Occassion\n",
        "\n",
        "### Response:\n",
        "这个 correct spelling is 'Occasion.'\n",
        "```\n",
        "\n",
        "In 这个 练习, 我们 现在 使用 这个 Phi-3 prompt template instead, 哪个 formats 这个 data entry as follows:\n",
        "\n",
        "```\n",
        "<user>\n",
        "Identify 这个 correct spelling of 这个 following word: 'Occasion'\n",
        "\n",
        "<assistant>\n",
        "这个 correct spelling is 'Occasion'.\n",
        "```\n",
        "\n",
        "Note 那个 这个 prompt template is substantially shorter, 哪个 reduces 这个 runtime 和 hardware 依赖 for finetuning 这个 大语言模型 和 generating text since 这个 输入 prompts are shorter.\n",
        "To make 这个 改变, 我们 更新 这个 `format_input` 函数 as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f99baa1e-c24c-417f-89d0-13e6d061ea6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_input(entry):\n    instruction_text = (\n        f\"<|user|>\\n{entry['instruction']}\"\n    )\n\n    input_text = f\"\\n{entry['input']}\" if entry[\"input\"] else \"\"\n\n    return instruction_text + input_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ba538f-64b9-495d-847b-d9f1d324bc50",
      "metadata": {},
      "source": [
        "让我们 make sure 那个 它 works as intended by applying 它 to two 输入 samples, one with 和 one without content in 这个 `'输入'` field:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "877a57e2-535f-4363-b32a-a093edd951b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|user|>\n",
            "Identify the correct spelling of the following word.\n",
            "Ocassion\n",
            "\n",
            "<|user|>\n",
            "What is an antonym of 'complicated'?\n"
          ]
        }
      ],
      "source": [
        "sample_data = [\n    {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}, \n    {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n]\n\nprint(format_input(sample_data[0]))\nprint()\nprint(format_input(sample_data[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa2a6704-6c61-4a09-b8f5-ffc5a77d6aa3",
      "metadata": {},
      "source": [
        "接下来, 我们 also 更新 这个 `InstructionDataset` 类 to 使用 这个 <|assistant|> prompt template for 这个 response:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f0d9c8-8f41-4455-b9ae-6b17de610cc3",
      "metadata": {},
      "source": [
        "```python\n",
        "导入 tiktoken\n",
        "from torch.utils.data 导入 数据集\n",
        "\n",
        "类 InstructionDataset(数据集):\n",
        "    def __init__(self, data, 分词器):\n",
        "        self.data = data\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "\n",
        "            ###################################################################\n",
        "            # NEW: 使用 `format_input_phi` 和 adjust 这个 response text template\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n<|assistant|>:\\n{entry['输出']}\"\n",
        "            ###################################################################\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(\n",
        "                分词器.encode(full_text)\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        返回 self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        返回 len(self.data)\n",
        "\n",
        "\n",
        "分词器 = tiktoken.get_encoding(\"gpt2\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0650926-c39f-4442-8116-cb7494416f28",
      "metadata": {},
      "source": [
        "Lastly, 我们 also have to 更新 这个 way 我们 extract 这个 generated response 当 我们 collect 这个 测试 设置 responses:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9253041-812f-4a5f-9ab1-d7e4cb1407fb",
      "metadata": {},
      "source": [
        "```python\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "    分词器=分词器\n",
        "\n",
        "    token_ids = 生成(\n",
        "        模型=模型,\n",
        "        idx=text_to_token_ids(input_text, 分词器).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, 分词器)\n",
        "\n",
        "    # New: Adjust ###Response -> <|assistant|>\n",
        "    response_text = generated_text[len(input_text):].replace(\"<|assistant|>:\", \"\").strip()\n",
        "\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29cd557c-3838-45e4-a26a-baed4b11175a",
      "metadata": {},
      "source": [
        "For your convenience, 这个 练习 solution is implemented in 这个 [exercise_experiments.py](exercise_experiments.py) 脚本, 哪个 你 can 运行 as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd8158e9-cc70-4e0f-88b0-73c3e1d8c030",
      "metadata": {},
      "source": [
        "```bash\n",
        "python exercise_experiments.py --exercise_solution phi3_prompt\n",
        "```\n",
        "\n",
        "输出:\n",
        "\n",
        "```\n",
        "matplotlib version: 3.7.1\n",
        "tiktoken version: 0.7.0\n",
        "torch version: 2.3.0+cu121\n",
        "tqdm version: 4.66.4\n",
        "tensorflow version: 2.15.0\n",
        "--------------------------------------------------\n",
        "训练 设置 length: 935\n",
        "验证 设置 length: 55\n",
        "测试 设置 length: 110\n",
        "--------------------------------------------------\n",
        "Device: cuda\n",
        "--------------------------------------------------\n",
        "...\n",
        "Loaded 模型: gpt2-medium (355M)\n",
        "--------------------------------------------------\n",
        "Initial losses\n",
        "   训练 loss: 3.71630220413208\n",
        "   验证 loss: 3.6440994262695314\n",
        "Ep 1 (Step 000000): Train loss 2.633, Val loss 2.622\n",
        "...\n",
        "Ep 2 (Step 000230): Train loss 0.424, Val loss 0.928\n",
        "<|user|> 转换 这个 active sentence to passive: '这个 chef cooks 这个 meal every day.' <|assistant|>: 这个 meal is prepared every day by 这个 chef....\n",
        "训练 completed in 1.50 minutes.\n",
        "绘图 saved as loss-绘图-phi3-prompt.pdf\n",
        "--------------------------------------------------\n",
        "Generating responses\n",
        "100% 110/110 [00:11<00:00,  9.27it/s]\n",
        "Responses saved as instruction-data-with-response-phi3-prompt.json\n",
        "模型 saved as gpt2-medium355M-sft-phi3-prompt.pth\n",
        "```\n",
        "\n",
        "For comparison, 你 can 运行 这个 original 第 7 finetuning 代码 via `python exercise_experiments.py --exercise_solution baseline`. \n",
        "\n",
        "Note 那个 on 一个 Nvidia L4 GPU, 这个 代码 above, using 这个 Phi-3 prompt template, takes 1.5 min to 运行. In comparison, 这个 Alpaca-style template takes 1.80 minutes to 运行. So, 这个 Phi-3 template is approximately 17% faster since 它 results in shorter 模型 inputs. \n",
        "\n",
        "让我们 take 一个 look at some of 这个 responses to make sure they have been formatted correctly:\n",
        "\n",
        "```json\n",
        "    {\n",
        "        \"instruction\": \"Rewrite 这个 sentence using 一个 simile.\",\n",
        "        \"输入\": \"这个 car is very fast.\",\n",
        "        \"输出\": \"这个 car is as fast as lightning.\",\n",
        "        \"model_response\": \"这个 car is as fast as 一个 cheetah.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"什么 type of cloud is typically associated with thunderstorms?\",\n",
        "        \"输入\": \"\",\n",
        "        \"输出\": \"这个 type of cloud typically associated with thunderstorms is cumulonimbus.\",\n",
        "        \"model_response\": \"这个 type of cloud associated with thunderstorms is 一个 cumulus cloud.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Name 这个 author of 'Pride 和 Prejudice'.\",\n",
        "        \"输入\": \"\",\n",
        "        \"输出\": \"Jane Austen.\",\n",
        "        \"model_response\": \"这个 author of 'Pride 和 Prejudice' is Jane Austen.\"\n",
        "    },\n",
        "```\n",
        "\n",
        "我们 can evaluate 这个 性能 using 这个 Ollama Llama 3 方法, 哪个 is for your convenience, also implemented in 这个 `python exercise_experiments.py` 脚本, 哪个 我们 can 运行 as follows:\n",
        "\n",
        "```bash\n",
        "python ollama_evaluate.py --file_path instruction-data-with-response-phi3-prompt.json\n",
        "```\n",
        "\n",
        "输出:\n",
        "\n",
        "```\n",
        "Ollama running: True\n",
        "Scoring entries: 100%|████████████████████████| 110/110 [01:08<00:00,  1.60it/s]\n",
        "Number of scores: 110 of 110\n",
        "Average score: 48.87\n",
        "```\n",
        "\n",
        "这个 score is close to 50, 哪个 is in 这个 same ballpark as 这个 score 我们 previously achieved with 这个 Alpaca-style prompts.\n",
        "\n",
        "那里 is no inherent advantage 或者 rationale 为什么 这个 Phi prompt-style should be better, 但是 它 can be more concise 和 efficient, except for 这个 caveat mentioned in 这个 *Tip* section below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156bc574-3f3e-4479-8f58-c8c8c472416e",
      "metadata": {},
      "source": [
        "#### Tip: Considering special tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65cacf90-21c2-48f2-8f21-5c0c86749ff2",
      "metadata": {},
      "source": [
        "- Note 那个 这个 Phi-3 prompt template contains special tokens such as `<|user|>` 和 `<|assistant|>`, 哪个 can be suboptimal for 这个 GPT-2 分词器\n",
        "- While 这个 GPT-2 分词器 recognizes `<|endoftext|>` as 一个 special 词元 (encoded into 词元 ID 50256), 它 is inefficient at handling other special tokens, such as 这个 aforementioned ones\n",
        "- For instance, `<|user|>` is encoded into 5 individual 词元 IDs (27, 91, 7220, 91, 29), 哪个 is very inefficient\n",
        "- 我们 could 添加 `<|user|>` as 一个 new special 词元 in `tiktoken` via 这个 `allowed_special` argument, 但是 please keep in mind 那个 这个 GPT-2 vocabulary would not be able to handle 它 without additional modification\n",
        "- 如果 你 are curious about 如何 一个 分词器 和 大语言模型 can be extended to handle special tokens, please see 这个 [extend-tiktoken.ipynb](../../ch05/09_extending-tokenizers/extend-tiktoken.ipynb) bonus materials (note 那个 这个 is not required 这里 但是 is just 一个 interesting/bonus consideration for curious readers)\n",
        "- Furthermore, 我们 can hypothesize 那个 models 那个 support these special tokens of 一个 prompt template via their vocabulary may perform more efficiently 和 better overall"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
      "metadata": {},
      "source": [
        "&nbsp;\n",
        "## 练习 7.2: Instruction 和 输入 masking\n",
        "\n",
        "To mask out 这个 instructions as shown in 这个 following figure, 我们 need to make slight modifications to 这个 `InstructionDataset` 类 和 `custom_collate_fn`.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/mask-instructions.webp\" width=600px>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4405196a-db81-470b-be39-167a059587b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 这个 `format_input` 函数 is copied from 这个 original 第 7 代码\n\ndef format_input(entry):\n    instruction_text = (\n        f\"Below is an instruction that describes a task. \"\n        f\"Write a response that appropriately completes the request.\"\n        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n    )\n\n    input_text = f\"\\n\\n### 输入:\\n{entry['输入']}\" 如果 entry[\"输入\"] else \"\"\n\n    return instruction_text + input_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83658c09-af8a-425a-b940-eb1f06e43c0b",
      "metadata": {},
      "source": [
        "我们 can 修改 这个 `InstructionDataset` 类 to collect 这个 lengths of 这个 instructions, 哪个 我们 will 使用 in 这个 collate 函数 to locate 这个 instruction content positions in 这个 targets 当 我们 代码 这个 collate 函数, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e5e6188a-f182-4f26-b9e5-ccae3ecadae0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\nfrom torch.utils.data import Dataset\n\n\nclass InstructionDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n\n        ##########################################################################################\n        # New: Separate list for instruction lengths\n        self.instruction_lengths = []\n        ##########################################################################################\n        \n        self.encoded_texts = []\n        \n        for entry in data:\n            instruction_plus_input = format_input(entry)\n            response_text = f\"\\n\\n### Response:\\n{entry['输出']}\"\n            full_text = instruction_plus_input + response_text\n            \n            self.encoded_texts.append(\n                tokenizer.encode(full_text)\n            )\n\n            ##########################################################################################\n            # New: collect instruction lengths\n            instruction_length = len(tokenizer.encode(instruction_plus_input))\n            self.instruction_lengths.append(instruction_length)\n            ##########################################################################################\n            \n    def __getitem__(self, index):\n        # New: 返回 both instruction lengths 和 texts separately\n        return self.instruction_lengths[index], self.encoded_texts[index]\n\n    def __len__(self):\n        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0163b7d1-acb8-456c-8efe-86307b58f4bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a186394-4960-424d-bb6a-f58459dd5994",
      "metadata": {},
      "source": [
        "接下来, 我们 更新 这个 `custom_collate_fn` 哪里 each `batch` is 现在 一个 tuple containing `(instruction_length, item)` instead of just `item` due to 这个 changes in 这个 `InstructionDataset` 数据集. In addition, 我们 现在 mask 这个 corresponding instruction tokens in 这个 目标 ID list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f815e6fc-8e54-4105-aecd-d4c6e890ff9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_collate_fn(\n    batch,\n    pad_token_id=50256,\n    ignore_index=-100,\n    allowed_max_length=None,\n    device=\"cpu\"\n):\n    # Find 这个 longest sequence in 这个 batch\n    batch_max_length = max(len(item)+1 for instruction_length, item in batch)   # New: batch is 现在 一个 tuple\n\n    # Pad 和 prepare inputs 和 targets\n    inputs_lst, targets_lst = [], []\n\n    for instruction_length, item in batch:  # New: batch is 现在 一个 tuple\n        new_item = item.copy()\n        # 添加 一个 <|endoftext|> 词元\n        new_item += [pad_token_id]\n        # Pad sequences to max_length\n        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n        inputs = torch.tensor(padded[:-1])  # Truncate 这个 last 词元 for inputs\n        targets = torch.tensor(padded[1:])  # Shift +1 to 这个 right for targets\n\n        # Replace all 但是 这个 首先 padding tokens in targets by ignore_index\n        mask = targets == pad_token_id\n        indices = torch.nonzero(mask).squeeze()\n        if indices.numel() > 1:\n            targets[indices[1:]] = ignore_index\n\n        ##########################################################################################\n        # New: Mask all 输入 和 instruction tokens in 这个 targets\n        targets[:instruction_length-1] = -100\n        ##########################################################################################\n        \n        # Optionally truncate to maximum sequence length\n        if allowed_max_length is not None:\n            inputs = inputs[:allowed_max_length]\n            targets = targets[:allowed_max_length]\n        \n        inputs_lst.append(inputs)\n        targets_lst.append(targets)\n\n    # 转换 list of inputs 和 targets to tensors 和 transfer to 目标 device\n    inputs_tensor = torch.stack(inputs_lst).to(device)\n    targets_tensor = torch.stack(targets_lst).to(device)\n\n    return inputs_tensor, targets_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a4a4815-850e-42c4-b70d-67e8ce5ebd57",
      "metadata": {},
      "source": [
        "让我们 try 它 out on some sample data below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8da8a5b1-a8e2-4389-b21c-25b67be6dd1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_data = [\n    {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"},\n    {'instruction': 'Sort the following list in alphabetical order.', 'input': 'Zebra, Elephant, Crocodile', 'output': 'Crocodile, Elephant, Zebra'},\n    {'instruction': 'Arrange the given numbers in descending order.', 'input': '5, 12, 8, 3, 15', 'output': '15, 12, 8, 5, 3.'}\n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "435b0816-0fc8-4650-a84a-eceffa4d85e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n\ntrain_dataset = InstructionDataset(sample_data, tokenizer)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=len(sample_data),\n    collate_fn=custom_collate_fn,\n    num_workers=0\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "106bbbd7-7286-4eb6-b343-43419332a80f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loader:\n",
            "torch.Size([3, 64]) torch.Size([3, 64])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train loader:\")\nfor inputs, targets in train_loader:\n    print(inputs.shape, targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9bb3288b-84a9-4962-ae59-a7a29fd34bce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs:\n",
            " tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
            "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
            "        21017, 46486,    25,   198, 42758,   262,  1708,  1351,   287, 24830,\n",
            "          605,  1502,    13,   198,   198, 21017, 23412,    25,   198,    57,\n",
            "        37052,    11, 42651,    11,  9325, 19815,   576,   198,   198, 21017,\n",
            "        18261,    25,   198,    34, 12204,   375,   576,    11, 42651,    11,\n",
            "         1168, 37052, 50256, 50256])\n",
            "\n",
            "\n",
            "Targets:\n",
            " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,   198,   198, 21017, 18261,\n",
            "           25,   198,    34, 12204,   375,   576,    11, 42651,    11,  1168,\n",
            "        37052, 50256,  -100,  -100])\n"
          ]
        }
      ],
      "source": [
        "print(\"Inputs:\\n\", inputs[1])\nprint(\"\\n\\nTargets:\\n\", targets[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc40347b-2ca7-44e1-862d-0fd0c92f0628",
      "metadata": {},
      "source": [
        "As 我们 can see based on 这个 `targets` tensor, both 这个 instruction 和 padding tokens are 现在 masked using 这个 -100 placeholder tokens. \n",
        "让我们 decode 这个 inputs just to make sure 那个 they look correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "76a9e6fa-3d75-4e39-b139-c3e05048f42b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Sort the following list in alphabetical order.\n",
            "\n",
            "### Input:\n",
            "Zebra, Elephant, Crocodile\n",
            "\n",
            "### Response:\n",
            "Crocodile, Elephant, Zebra<|endoftext|><|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(list(inputs[1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "845ebd36-f63f-4b58-a76e-7767e4d2ccbd",
      "metadata": {},
      "source": [
        "接下来, 让我们 decode 这个 non-masked 目标 词元 IDS:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4d54a152-b778-455a-8941-e375e2a17e8f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "### Response:\n",
            "Crocodile, Elephant, Zebra<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "non_masked_targets = targets[1][targets[1] != -100]\n\nprint(tokenizer.decode(list(non_masked_targets)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3912bbf5-e9e2-474b-9552-d522e7510aa6",
      "metadata": {},
      "source": [
        "As shown above, 这个 non-masked 目标 tokens exclude 这个 `\"Instruction\"` 和 `\"输入\"` fields, as intended. 现在, 我们 can 运行 这个 modified 代码 to see 如何 well 这个 大语言模型 performs 当 finetuned using 这个 masking strategy.\n",
        "\n",
        "For your convenience, 你 can 使用 这个 `exercise_experiments.py` 代码 to 运行 一个 comparison as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56a76097-9114-479d-8803-443b0ff48581",
      "metadata": {},
      "source": [
        "```bash\n",
        "python exercise_experiments.py --exercise_solution mask_instructions\n",
        "```\n",
        "\n",
        "输出:\n",
        "\n",
        "```\n",
        "matplotlib version: 3.7.1\n",
        "tiktoken version: 0.7.0\n",
        "torch version: 2.3.0+cu121\n",
        "tqdm version: 4.66.4\n",
        "tensorflow version: 2.15.0\n",
        "--------------------------------------------------\n",
        "训练 设置 length: 935\n",
        "验证 设置 length: 55\n",
        "测试 设置 length: 110\n",
        "--------------------------------------------------\n",
        "Device: cuda\n",
        "--------------------------------------------------\n",
        "...\n",
        "Loaded 模型: gpt2-medium (355M)\n",
        "--------------------------------------------------\n",
        "Initial losses\n",
        "   训练 loss: 2.280539035797119\n",
        "   验证 loss: 2.262560224533081\n",
        "Ep 1 (Step 000000): Train loss 1.636, Val loss 1.620\n",
        "...\n",
        "Ep 2 (Step 000230): Train loss 0.143, Val loss 0.727\n",
        "...\n",
        "训练 completed in 1.77 minutes.\n",
        "绘图 saved as loss-绘图-mask-instructions.pdf\n",
        "--------------------------------------------------\n",
        "Generating responses\n",
        "100% 110/110 [02:10<00:00,  1.19s/它]\n",
        "Responses saved as instruction-data-with-response-mask-instructions.json\n",
        "模型 saved as gpt2-medium355M-sft-mask-instructions.pth\n",
        "```\n",
        "\n",
        "接下来, 让我们 evaluate 这个 性能 of 这个 resulting 大语言模型:\n",
        "\n",
        "```bash\n",
        "python ollama_evaluate.py --file_path instruction-data-with-response-mask-instructions.json\n",
        "```\n",
        "\n",
        "```\n",
        "Ollama running: True\n",
        "Scoring entries: 100%|██████████████████████████████████████████████████████████████████████████████████████| 110/110 [01:23<00:00,  1.31it/s]\n",
        "Number of scores: 110 of 110\n",
        "Average score: 47.73\n",
        "```\n",
        "\n",
        "As 我们 can see based on 这个 scores, 这个 instruction masking does perform slightly worse, 哪个 is consistent with 这个 observation in 这个 \"Instruction Tuning With Loss Over Instructions\" paper (https://arxiv.org/abs/2405.14394)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94a0f758-29da-44ee-b7af-32473b3c086e",
      "metadata": {},
      "source": [
        "&nbsp;\n",
        "## 练习 7.3: Finetuning on 这个 original Alpaca 数据集"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68df7616-679f-4e53-954d-6e7cf2e2ef55",
      "metadata": {},
      "source": [
        "To finetune 这个 模型 on 这个 original Stanford Alpaca 数据集 ([https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)), 你 just need to 改变 这个 file URL from\n",
        "\n",
        "```python\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-第-代码/instruction-data.json\"\n",
        "```\n",
        "\n",
        "to\n",
        "\n",
        "```python\n",
        "url = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\"\n",
        "```\n",
        "\n",
        "Note 那个 这个 数据集 contains 52k entries (50x more than in 第 7), 和 这个 entries are longer than 这个 ones 我们 worked with in 第 7.\n",
        "Thus, 它's highly recommended 那个 这个 训练 be 运行 on 一个 GPU.\n",
        "\n",
        "如果 你 encounter out-of-memory errors, consider reducing 这个 批量大小 from 8 to 4, 2, 或者 1. In addition to lowering 这个 批量大小, 你 may also want to consider lowering 这个 `allowed_max_length` from 1024 to 512 或者 256."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d94c9621-2c3f-4551-b5b8-87cd96e38c9c",
      "metadata": {},
      "source": [
        "For your convenience, 你 can 使用 这个 `exercise_experiments.py` 代码 to finetune 这个 模型 on 这个 52k Alpaca 数据集 with 一个 批量大小 of 4 和 一个 `allowed_max_length` of 512 as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40a76486-73e6-4415-94dc-bfe2aa36ea52",
      "metadata": {},
      "source": [
        "```bash\n",
        "python exercise_experiments.py --exercise_solution alpaca_52k\n",
        "```\n",
        "\n",
        "```\n",
        "matplotlib version: 3.7.1\n",
        "tiktoken version: 0.7.0\n",
        "torch version: 2.3.0+cu121\n",
        "tqdm version: 4.66.4\n",
        "tensorflow version: 2.15.0\n",
        "--------------------------------------------------\n",
        "训练 设置 length: 44201\n",
        "验证 设置 length: 2601\n",
        "测试 设置 length: 5200\n",
        "--------------------------------------------------\n",
        "Device: cuda\n",
        "--------------------------------------------------\n",
        "...\n",
        "Loaded 模型: gpt2-medium (355M)\n",
        "--------------------------------------------------\n",
        "Initial losses\n",
        "   训练 loss: 3.3681655883789063\n",
        "   验证 loss: 3.4122894287109373\n",
        "Ep 1 (Step 000000): Train loss 2.477, Val loss 2.750\n",
        "...\n",
        "Ep 2 (Step 022095): Train loss 0.761, Val loss 1.557\n",
        "...\n",
        "训练 completed in 196.38 minutes.\n",
        "绘图 saved as loss-绘图-alpaca52k.pdf\n",
        "--------------------------------------------------\n",
        "Generating responses\n",
        "100% 5200/5200 [2:56:33<00:00,  2.04s/它]\n",
        "Responses saved as instruction-data-with-response-alpaca52k.json\n",
        "模型 saved as gpt2-medium355M-sft-alpaca52k.pth\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8a1bd1-15f4-4a35-87b5-369038db0aa7",
      "metadata": {},
      "source": [
        "Below are 一个 few examples from 这个 Alpaca 数据集, including 这个 generated 模型 responses:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be9ab66f-5819-4b01-9a03-c45aa3b7c5b8",
      "metadata": {},
      "source": [
        "```json\n",
        "[\n",
        "    {\n",
        "        \"instruction\": \"Edit 这个 following sentence to increase readability: \\\"He made 一个 huge effort 和 was so successful.\\\"\",\n",
        "        \"输入\": \"\",\n",
        "        \"输出\": \"He exerted 一个 tremendous effort, 和 thus enjoyed great success.\",\n",
        "        \"model_response\": \"He put in 一个 immense effort 和 was rewarded with success.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Rewrite 这个 following sentence to make 它 more concise: \\\"I was displeased with 这个 result of 这个 experiment 那个 I conducted.\\\"\",\n",
        "        \"输入\": \"\",\n",
        "        \"输出\": \"I was unhappy with my experiment's outcome.\",\n",
        "        \"model_response\": \"I was displeased with 这个 results of 这个 experiment.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"如何 can 我们 构建 一个 more efficient GPT 模型?\",\n",
        "        \"输入\": \"\",\n",
        "        \"输出\": \"我们 can 构建 一个 more efficient GPT 模型 by optimizing 这个 architecture of 这个 模型, using smaller 模型 sizes 和 训练 with fewer parameters. 我们 can also leverage techniques such as knowledge distillation, transfer learning, dynamic sparsity 和 hybrid computing to further improve 这个 efficiency of 这个 模型.\",\n",
        "        \"model_response\": \"Building 一个 more efficient GPT 模型 requires careful planning 和 optimization. 首先, 它 is important to identify 这个 目标 language 和 这个 context in 哪个 这个 模型 is used. 然后, 它 is important to select 这个 appropriate 模型 architecture, such as 反向传播, hyperparameters, 和 hyperparameters. 最后, 它 is important to select 这个 appropriate 模型 weights 和 optimizers, such as 反向传播, hyperparameters, 和 hyperparameters.\"\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "840e2076-f2e6-44a2-86fd-f191f9674267",
      "metadata": {},
      "source": [
        "最后, 我们 can evaluate 这个 finetuned 大语言模型 using 这个 [ollama_evaluate.py](ollama_evaluate.py) utility 函数:\n",
        "\n",
        "```bash\n",
        "python ollama_evaluate.py --file_path instruction-data-with-response-alpaca52k.json\n",
        "```\n",
        "\n",
        "```\n",
        "Scoring entries: 100%|████████████████████| 5200/5200 [1:07:52<00:00, 1.28it/s]\n",
        "Number of scores: 5188 of 5200\n",
        "Average score: 48.16\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d14b3c60-00a1-43a9-9fcd-592aaadf1ef4",
      "metadata": {},
      "source": [
        "这个 score is slightly lower than 这个 score 我们 obtained on 这个 数据集 我们 used in 这个 第. However, note 那个 这个 Alpaca 测试 设置 contains more diverse 和 partly more challenging instructions than 这个 数据集 我们 used in 这个 main 第."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca61fa6c-4e1d-4618-9e5e-d091f8303e30",
      "metadata": {},
      "source": [
        "## 练习 7.4: 参数-efficient finetuning with LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01742cec-1f41-4415-8788-009d31b1ad38",
      "metadata": {},
      "source": [
        "To instruction finetune 这个 模型 using LoRA, 使用 这个 relevant classes 和 functions from 附录 E:\n",
        "\n",
        "```python\n",
        "from appendix_E 导入 LoRALayer, LinearWithLoRA, replace_linear_with_lora\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871dca8f-3411-4735-b7b0-9d0e6e0599ac",
      "metadata": {},
      "source": [
        "接下来, 添加 这个 following lines of 代码 below 这个 模型 loading 代码 in section 7.5:\n",
        "\n",
        "\n",
        "```python\n",
        "total_params = sum(p.numel() for p in 模型.parameters() 如果 p.requires_grad)\n",
        "打印(f\"Total trainable parameters before: {total_params:,}\")\n",
        "\n",
        "for param in 模型.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "total_params = sum(p.numel() for p in 模型.parameters() 如果 p.requires_grad)\n",
        "打印(f\"Total trainable parameters after: {total_params:,}\")\n",
        "replace_linear_with_lora(模型, rank=16, alpha=16)\n",
        "\n",
        "total_params = sum(p.numel() for p in 模型.parameters() 如果 p.requires_grad)\n",
        "打印(f\"Total trainable LoRA parameters: {total_params:,}\")\n",
        "模型.to(device)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b26b925-dc95-4b91-b050-9676dd9608a4",
      "metadata": {},
      "source": [
        "For your convenience, 你 can 使用 这个 `exercise_experiments.py` 代码 to finetune 这个 模型, using LoRA with rank 16 和 alpa 16, as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01f02c7e-3b15-44b8-bf41-7892cd755766",
      "metadata": {},
      "source": [
        "```bash\n",
        "python exercise_experiments.py --exercise_solution lora\n",
        "```\n",
        "\n",
        "输出:\n",
        "\n",
        "```\n",
        "matplotlib version: 3.7.1\n",
        "tiktoken version: 0.7.0\n",
        "torch version: 2.3.0+cu121\n",
        "tqdm version: 4.66.4\n",
        "tensorflow version: 2.15.0\n",
        "--------------------------------------------------\n",
        "训练 设置 length: 935\n",
        "验证 设置 length: 55\n",
        "测试 设置 length: 110\n",
        "--------------------------------------------------\n",
        "Device: cuda\n",
        "--------------------------------------------------\n",
        "File already exists 和 is up-to-date: gpt2/355M/checkpoint\n",
        "File already exists 和 is up-to-date: gpt2/355M/encoder.json\n",
        "File already exists 和 is up-to-date: gpt2/355M/hparams.json\n",
        "File already exists 和 is up-to-date: gpt2/355M/模型.ckpt.data-00000-of-00001\n",
        "File already exists 和 is up-to-date: gpt2/355M/模型.ckpt.index\n",
        "File already exists 和 is up-to-date: gpt2/355M/模型.ckpt.meta\n",
        "File already exists 和 is up-to-date: gpt2/355M/vocab.bpe\n",
        "Loaded 模型: gpt2-medium (355M)\n",
        "--------------------------------------------------\n",
        "Total trainable parameters before: 406,286,336\n",
        "Total trainable parameters after: 0\n",
        "Total trainable LoRA parameters: 7,898,384\n",
        "Initial losses\n",
        "   训练 loss: 3.7684114456176756\n",
        "   验证 loss: 3.7619335651397705\n",
        "Ep 1 (Step 000000): Train loss 2.509, Val loss 2.519\n",
        "...\n",
        "Ep 2 (Step 000230): Train loss 0.308, Val loss 0.652\n",
        "...\n",
        "--------------------------------------------------\n",
        "Generating responses\n",
        "100% 110/110 [01:52<00:00,  1.03s/它]\n",
        "Responses saved as instruction-data-with-response-lora.json\n",
        "模型 saved as gpt2-medium355M-sft-lora.pth\n",
        "```\n",
        "\n",
        "For comparison, 你 can 运行 这个 original 第 7 finetuning 代码 via `python exercise_experiments.py --exercise_solution baseline`. \n",
        "\n",
        "Note 那个 on 一个 Nvidia L4 GPU, 这个 代码 above, using LoRA, takes 1.30 min to 运行. In comparison, 这个 baseline takes 1.80 minutes to 运行. So, LoRA is approximately 28% faster.\n",
        "\n",
        "\n",
        "我们 can evaluate 这个 性能 using 这个 Ollama Llama 3 方法, 哪个 is for your convenience, also implemented in 这个 `python exercise_experiments.py` 脚本, 哪个 我们 can 运行 as follows:\n",
        "\n",
        "```bash\n",
        "python ollama_evaluate.py --file_path instruction-data-with-response-lora.json\n",
        "```\n",
        "\n",
        "输出:\n",
        "\n",
        "```\n",
        "Ollama running: True\n",
        "Scoring entries: 100%|████████████████████████| 110/110 [01:13<00:00,  1.50it/s]\n",
        "Number of scores: 110 of 110\n",
        "Average score: 50.23\n",
        "```\n",
        "\n",
        "这个 score is around 50, 哪个 is in 这个 same ballpark as 这个 original 模型."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}