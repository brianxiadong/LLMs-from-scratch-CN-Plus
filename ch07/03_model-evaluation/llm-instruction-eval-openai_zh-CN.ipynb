{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "136a4efe-fb99-4311-8679-e0a5b6282755",
      "metadata": {},
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary 代码 for 这个 <一个 href=\"http://mng.bz/orYv\">构建 一个 大语言模型 From Scratch</一个> book by <一个 href=\"https://sebastianraschka.com\">Sebastian Raschka</一个><br>\n",
        "<br>代码 repository: <一个 href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</一个>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<一个 href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></一个>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1910a06-e8a3-40ac-8201-ff70615b1ba4",
      "metadata": {
        "tags": []
      },
      "source": [
        "# Evaluating Instruction Responses Using 这个 OpenAI API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a128651b-f326-4232-a994-42f38b7ed520",
      "metadata": {},
      "source": [
        "- 这个 笔记本 uses OpenAI's GPT-4 API to evaluate responses by 一个 instruction finetuned LLMs based on 一个 数据集 in JSON format 那个 includes 这个 generated 模型 responses, for 示例:\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "{\n",
        "    \"instruction\": \"什么 is 这个 atomic number of helium?\",\n",
        "    \"输入\": \"\",\n",
        "    \"输出\": \"这个 atomic number of helium is 2.\",               # <-- 这个 目标 given in 这个 测试 设置\n",
        "    \"模型 1 response\": \"\\nThe atomic number of helium is 2.0.\", # <-- Response by 一个 大语言模型\n",
        "    \"模型 2 response\": \"\\nThe atomic number of helium is 3.\"    # <-- Response by 一个 2nd 大语言模型\n",
        "},\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "267ba0d1-b884-42df-85bd-0be746fd47a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip 安装 -r 依赖-extra.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "63610acc-db94-437f-8d38-e99dca0299cb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "openai version: 1.30.3\n",
            "tqdm version: 4.66.2\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n\npkgs = [\"openai\",  # OpenAI API\n        \"tqdm\",    # Progress bar\n        ]\n\nfor p in pkgs:\n    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bcdcb34-ac75-4f4f-9505-3ce0666c42d5",
      "metadata": {},
      "source": [
        "## 测试 OpenAI API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9558a522-650d-401a-84fc-9fd7b1f39da7",
      "metadata": {},
      "source": [
        "- 首先, 让我们 测试 如果 这个 OpenAI API is correctly 设置 up\n",
        "- 如果 你 don't have 一个 account yet, 你 need to 创建 one at https://platform.openai.com/\n",
        "- Note 那个 你 will also have to transfer some funds to your account as 这个 GPT-4 API is not free (see https://platform.openai.com/settings/organization/billing/overview)\n",
        "- Running 这个 experiments 和 creating 这个 ~200 evaluations using 这个 代码 in 这个 笔记本 costs about $0.26 (26 cents) as of 这个 writing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89343a84-0ddc-42fc-bf50-298a342b93c0",
      "metadata": {},
      "source": [
        "- 首先, 我们 need to provide our OpenAI API secret key, 哪个 can be found at https://platform.openai.com/api-keys\n",
        "- Make sure not to share 这个 key with anyone\n",
        "- 添加 这个 secret key (`\"sk-...\"`) to 这个 `config.json` file in 这个 folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "65b0ba76-1fb1-4306-a7c2-8f3bb637ccdb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\nfrom openai import OpenAI\n\n# 加载 API key from 一个 JSON file.\n# Make sure to replace \"sk-...\" with your actual API key from https://platform.openai.com/api-keys\nwith open(\"config.json\", \"r\") as config_file:\n    config = json.load(config_file)\n    api_key = config[\"OPENAI_API_KEY\"]\n\nclient = OpenAI(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16642a48-1cab-40d2-af08-ab8c2fbf5876",
      "metadata": {},
      "source": [
        "- 首先, 让我们 try 这个 API with 一个 simple 示例 to make sure 它 works as intended:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "08e9ef2e-e816-4283-840e-43625791ad33",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello world'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def run_chatgpt(prompt, client, model=\"gpt-4-turbo\"):\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.0,\n        seed=123,\n    )\n    return response.choices[0].message.content\n\n\nprompt = \"Respond with 'hello world' if you got this message.\"\nrun_chatgpt(prompt, client)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "162a4739-6f03-4092-a5c2-f57a0b6a4c4d",
      "metadata": {},
      "source": [
        "## 加载 JSON Entries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca011a8b-20c5-4101-979e-9b5fccf62f8a",
      "metadata": {},
      "source": [
        "- 这里, 我们 assume 那个 我们 saved 这个 测试 数据集 和 这个 模型 responses as 一个 JSON file 那个 我们 can 加载 as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8b2d393a-aa92-4190-9d44-44326a6f699b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of entries: 100\n"
          ]
        }
      ],
      "source": [
        "json_file = \"eval-example-data.json\"\n\nwith open(json_file, \"r\") as file:\n    json_data = json.load(file)\n\nprint(\"Number of entries:\", len(json_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c9751b-59b7-43fe-acc7-14e8daf2fa66",
      "metadata": {},
      "source": [
        "- 这个 structure of 这个 file is as follows, 哪里 我们 have 这个 given response in 这个 测试 数据集 (`'输出'`) 和 responses by two different models (`'模型 1 response'` 和 `'模型 2 response'`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7222fdc0-5684-4f2b-b741-3e341851359e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'instruction': 'Calculate the hypotenuse of a right triangle with legs of 6 cm and 8 cm.',\n",
              " 'input': '',\n",
              " 'output': 'The hypotenuse of the triangle is 10 cm.',\n",
              " 'model 1 response': '\\nThe hypotenuse of the triangle is 3 cm.',\n",
              " 'model 2 response': '\\nThe hypotenuse of the triangle is 12 cm.'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "json_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcf0331b-6024-4bba-89a9-a088b14a1046",
      "metadata": {},
      "source": [
        "- Below is 一个 small utility 函数 那个 formats 这个 输入 for visualization purposes later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "43263cd3-e5fb-4ab5-871e-3ad6e7d21a8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_input(entry):\n    instruction_text = (\n        f\"Below is an instruction that describes a task. Write a response that \"\n        f\"appropriately completes the request.\"\n        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n    )\n\n    input_text = f\"\\n\\n### 输入:\\n{entry['输入']}\" 如果 entry[\"输入\"] else \"\"\n    instruction_text + input_text\n\n    return instruction_text + input_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39a55283-7d51-4136-ba60-f799d49f4098",
      "metadata": {},
      "source": [
        "- 现在, 让我们 try 这个 OpenAI API to compare 这个 模型 responses (我们 only evaluate 这个 首先 5 responses for 一个 visual comparison):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "735cc089-d127-480a-b39d-0782581f0c41",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset response:\n",
            ">> The hypotenuse of the triangle is 10 cm.\n",
            "\n",
            "Model response:\n",
            ">> \n",
            "The hypotenuse of the triangle is 3 cm.\n",
            "\n",
            "Score:\n",
            ">> The model response \"The hypotenuse of the triangle is 3 cm.\" is incorrect. The correct calculation of the hypotenuse for a right triangle with legs of 6 cm and 8 cm can be found using the Pythagorean theorem, which states that the square of the hypotenuse (c) is equal to the sum of the squares of the other two sides (a and b). Mathematically, this is expressed as:\n",
            "\n",
            "\\[ c = \\sqrt{a^2 + b^2} \\]\n",
            "\\[ c = \\sqrt{6^2 + 8^2} \\]\n",
            "\\[ c = \\sqrt{36 + 64} \\]\n",
            "\\[ c = \\sqrt{100} \\]\n",
            "\\[ c = 10 \\text{ cm} \\]\n",
            "\n",
            "The correct answer should be 10 cm. The response given as 3 cm is not only incorrect but also significantly off from the correct value. This error could lead to misunderstandings or incorrect applications in practical scenarios where precise measurements are crucial.\n",
            "\n",
            "Given the scale from 0 to 100, where 100 is the best score, the response would score very low due to its inaccuracy. However, since the response format is correct (stating the measurement and unit), it does not score the absolute minimum.\n",
            "\n",
            "**Score: 10/100**\n",
            "\n",
            "This score reflects that while the format of the response is correct, the content is highly inaccurate.\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> 1. Squirrel\n",
            "2. Eagle\n",
            "3. Tiger\n",
            "\n",
            "Model response:\n",
            ">> \n",
            "1. Squirrel\n",
            "2. Tiger\n",
            "3. Eagle\n",
            "4. Cobra\n",
            "5. Tiger\n",
            "6. Cobra\n",
            "\n",
            "Score:\n",
            ">> The model response lists six animals, three of which (squirrel, tiger, eagle) are indeed active during the day, making them correct responses to the instruction. However, the instruction specifically asked for three different animals, and the model response includes repetitions (tiger and cobra are each listed twice) and also exceeds the requested number of animals.\n",
            "\n",
            "The inclusion of \"cobra\" is incorrect as most cobras are not diurnal (active during the day); they are generally more active during the early morning and late evening, which can be considered crepuscular rather than diurnal.\n",
            "\n",
            "### Scoring Breakdown:\n",
            "- **Relevance to the task**: The response correctly identifies three diurnal animals but also includes additional animals, which was not requested.\n",
            "- **Accuracy**: Including animals not active during the day (cobra) and repeating animals reduces the accuracy.\n",
            "- **Adherence to instructions**: The task was to name three different animals, but the response included six names with repetitions.\n",
            "\n",
            "Given these points, the response partially meets the requirements but also deviates significantly in terms of the number of animals and the inclusion of incorrect and repeated entries.\n",
            "\n",
            "### Score: 50/100\n",
            "This score reflects that while the response did include three correct animals, it failed to strictly follow the instructions by listing only three different animals and included incorrect information.\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> I must ascertain what is incorrect.\n",
            "\n",
            "Model response:\n",
            ">> \n",
            "What is incorrect?\n",
            "\n",
            "Score:\n",
            ">> The model response \"What is incorrect?\" scores low in terms of fulfilling the instruction to rewrite the sentence in a more formal way. The original sentence \"I need to find out what's wrong.\" expresses a personal obligation and a process of discovery, which is not captured in the model response. The model response turns the sentence into a direct question and loses the nuance of needing to discover or investigate the issue.\n",
            "\n",
            "**Score: 20/100**\n",
            "\n",
            "**Reasoning:**\n",
            "- **Formality:** The response is slightly more formal than casual speech but does not elevate the formality significantly or appropriately. It does use \"incorrect\" which is slightly more formal than \"wrong.\"\n",
            "- **Completeness:** The response fails to include the aspect of needing to find out or ascertain, which is a critical part of the original sentence.\n",
            "- **Accuracy:** The response changes the structure and intent by converting it into a direct question, which does not align with the instruction to rewrite the statement while maintaining its original intent.\n",
            "\n",
            "Overall, the response does not adequately meet the requirements of the task as it significantly alters the meaning and omits key elements of the original sentence.\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The interjection in the sentence is 'Wow'.\n",
            "\n",
            "Model response:\n",
            ">> \n",
            "The interjection in the sentence is 'Wow'.\n",
            "\n",
            "Score:\n",
            ">> The model response `The interjection in the sentence is 'Wow'.` accurately identifies the interjection in the provided sentence. The response is clear, directly addresses the instruction, and correctly identifies \"Wow\" as the interjection, which is used to express surprise or admiration, fitting the context of the sentence. Therefore, the response is fully correct and meets all the requirements of the task.\n",
            "\n",
            "Score: 100/100\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The type of sentence is interrogative.\n",
            "\n",
            "Model response:\n",
            ">> \n",
            "The type of sentence is exclamatory.\n",
            "\n",
            "Score:\n",
            ">> The model response \"The type of sentence is exclamatory.\" is incorrect. The input sentence \"Did you finish the report?\" is clearly an interrogative sentence as it is asking a question, indicated by the question mark at the end and the structure of the sentence.\n",
            "\n",
            "Given the scoring criteria where 100 is the best score and should be awarded to a correct and precise response, the model's response should receive a low score because it incorrectly identifies the type of sentence. An exclamatory sentence typically expresses strong emotion and ends with an exclamation mark, which is not the case here.\n",
            "\n",
            "Therefore, the score for the model response would be 0 out of 100, as it completely misidentifies the type of sentence, providing incorrect information.\n",
            "\n",
            "-------------------------\n"
          ]
        }
      ],
      "source": [
        "for entry in json_data[:5]:\n    prompt = (f\"Given the input `{format_input(entry)}` \"\n              f\"and correct output `{entry['output']}`, \"\n              f\"score the model response `{entry['model 1 response']}`\"\n              f\" on a scale from 0 to 100, where 100 is the best score. \"\n              )\n    print(\"\\nDataset response:\")\n    print(\">>\", entry['output'])\n    print(\"\\nModel response:\")\n    print(\">>\", entry[\"model 1 response\"])\n    print(\"\\nScore:\")\n    print(\">>\", run_chatgpt(prompt, client))\n    print(\"\\n-------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "142dfaa7-429f-4eb0-b74d-ff327f79547a",
      "metadata": {},
      "source": [
        "- Note 那个 这个 responses are very verbose; to quantify 哪个 模型 is better, 我们 only want to 返回 这个 scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3552bdfb-7511-42ac-a9ec-da672e2a5468",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n\n\ndef generate_model_scores(json_data, json_key, client):\n    scores = []\n    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n        prompt = (\n            f\"Given the input `{format_input(entry)}` \"\n            f\"and correct output `{entry['output']}`, \"\n            f\"score the model response `{entry[json_key]}`\"\n            f\" on a scale from 0 to 100, where 100 is the best score. \"\n            f\"Respond with the number only.\"\n        )\n        score = run_chatgpt(prompt, client)\n        try:\n            scores.append(int(score))\n        except ValueError:\n            continue\n\n    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71974dea-31ed-49af-abba-5c858bbbf49c",
      "metadata": {},
      "source": [
        "- Please note 那个 这个 response scores may vary because OpenAI's GPT models are not deterministic despite setting 一个 random number seed, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b071ce84-1866-427f-a272-b46700f364b2",
      "metadata": {},
      "source": [
        "- 让我们 现在 应用 这个 evaluation to 这个 whole 数据集 和 计算 这个 average score of each 模型:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4f700d4b-19e5-4404-afa7-b0f093024232",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scoring entries: 100%|████████████████████████| 100/100 [01:03<00:00,  1.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "model 1 response\n",
            "Number of scores: 100 of 100\n",
            "Average score: 74.09\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Scoring entries: 100%|████████████████████████| 100/100 [01:06<00:00,  1.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "model 2 response\n",
            "Number of scores: 100 of 100\n",
            "Average score: 56.57\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n\nfor model in (\"model 1 response\", \"model 2 response\"):\n\n    scores = generate_model_scores(json_data, model, client)\n    print(f\"\\n{model}\")\n    print(f\"Number of scores: {len(scores)} of {len(json_data)}\")\n    print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n\n    # Optionally 保存 这个 scores\n    save_path = Path(\"scores\") / f\"gpt4-{model.replace(' ', '-')}.json\"\n    with open(save_path, \"w\") as file:\n        json.dump(scores, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8169d534-1fec-43c4-9550-5cb701ff7f05",
      "metadata": {},
      "source": [
        "- Based on 这个 evaluation above, 我们 can say 那个 这个 1st 模型 is substantially better than 这个 2nd 模型"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}