{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1ae38945-39dd-45dc-ad4f-da7a4404241f",
      "metadata": {},
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary 代码 for 这个 <一个 href=\"http://mng.bz/orYv\">构建 一个 大语言模型 From Scratch</一个> book by <一个 href=\"https://sebastianraschka.com\">Sebastian Raschka</一个><br>\n",
        "<br>代码 repository: <一个 href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</一个>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<一个 href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></一个>\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bfa70ec-5c4c-40e8-b923-16f8167e3181",
      "metadata": {},
      "source": [
        "# 第 3: Coding 注意力机制 Mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c29bcbe8-a034-43a2-b557-997b03c9882d",
      "metadata": {},
      "source": [
        "Packages 那个 are being used in 这个 笔记本:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e58f33e8-5dc9-4dd5-ab84-5a011fa11d92",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 2.4.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n\nprint(\"torch version:\", version(\"torch\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a4474d-7c68-4846-8702-37906cf08197",
      "metadata": {},
      "source": [
        "- 这个 第 covers 注意力机制 mechanisms, 这个 engine of LLMs:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a11208-d9d3-44b1-8e0d-0c8414110b93",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/01.webp?123\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e020fd-9690-4343-80df-da96678bef5e",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/02.webp\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecc4dcee-34ea-4c05-9085-2f8887f70363",
      "metadata": {},
      "source": [
        "## 3.1 这个 problem with modeling long sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a55aa49c-36c2-48da-b1d9-70f416e46a6a",
      "metadata": {},
      "source": [
        "- No 代码 in 这个 section\n",
        "- Translating 一个 text word by word isn't feasible due to 这个 differences in grammatical structures between 这个 source 和 目标 languages:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55c0c433-aa4b-491e-848a-54905ebb05ad",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/03.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db03c48a-3429-48ea-9d4a-2e53b0e516b1",
      "metadata": {},
      "source": [
        "- Prior to 这个 介绍 of Transformer models, encoder-decoder RNNs were commonly used for machine translation tasks\n",
        "- In 这个 setup, 这个 encoder processes 一个 sequence of tokens from 这个 source language, using 一个 hidden state—一个 kind of intermediate 层 within 这个 神经网络—to 生成 一个 condensed representation of 这个 entire 输入 sequence:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d8df2c-c1c2-4df0-9977-ade9713088b2",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/04.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3602c585-b87a-41c7-a324-c5e8298849df",
      "metadata": {},
      "source": [
        "## 3.2 Capturing data dependencies with 注意力机制 mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6fde64c-6034-421d-81d9-8244932086ea",
      "metadata": {},
      "source": [
        "- No 代码 in 这个 section\n",
        "- Through 一个 注意力机制 mechanism, 这个 text-generating decoder segment of 这个 network is capable of selectively accessing all 输入 tokens, implying 那个 certain 输入 tokens hold more significance than others in 这个 generation of 一个 specific 输出 词元:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc4f6293-8ab5-4aeb-a04c-50ee158485b1",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/05.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8044be1f-e6a2-4a1f-a6dd-e325d3bad05e",
      "metadata": {},
      "source": [
        "- Self-注意力机制 in transformers is 一个 technique designed to enhance 输入 representations by enabling each position in 一个 sequence to engage with 和 determine 这个 relevance of every other position within 这个 same sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6565dc9f-b1be-4c78-b503-42ccc743296c",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/06.webp\" width=\"300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5efe05ff-b441-408e-8d66-cde4eb3397e3",
      "metadata": {},
      "source": [
        "## 3.3 Attending to different parts of 这个 输入 with self-注意力机制"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d9af516-7c37-4400-ab53-34936d5495a9",
      "metadata": {},
      "source": [
        "### 3.3.1 一个 simple self-注意力机制 mechanism without trainable weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d269e9f1-df11-4644-b575-df338cf46cdf",
      "metadata": {},
      "source": [
        "- 这个 section explains 一个 very simplified variant of self-注意力机制, 哪个 does not contain any trainable weights\n",
        "- 这个 is purely for illustration purposes 和 NOT 这个 注意力机制 mechanism 那个 is used in transformers\n",
        "- 这个 接下来 section, section 3.3.2, will extend 这个 simple 注意力机制 mechanism to 实现 这个 real self-注意力机制 mechanism\n",
        "- Suppose 我们 are given 一个 输入 sequence $x^{(1)}$ to $x^{(T)}$\n",
        "  - 这个 输入 is 一个 text (for 示例, 一个 sentence like \"Your journey starts with one step\") 那个 has already been converted into 词元 embeddings as described in 第 2\n",
        "  - For instance, $x^{(1)}$ is 一个 d-dimensional vector representing 这个 word \"Your\", 和 so forth\n",
        "- **Goal:** 计算 context vectors $z^{(i)}$ for each 输入 sequence element $x^{(i)}$ in $x^{(1)}$ to $x^{(T)}$ (哪里 $z$ 和 $x$ have 这个 same dimension)\n",
        "    - 一个 context vector $z^{(i)}$ is 一个 weighted sum over 这个 inputs $x^{(1)}$ to $x^{(T)}$\n",
        "    - 这个 context vector is \"context\"-specific to 一个 certain 输入\n",
        "      - Instead of $x^{(i)}$ as 一个 placeholder for 一个 arbitrary 输入 词元, 让我们 consider 这个 second 输入, $x^{(2)}$\n",
        "      - 和 to continue with 一个 concrete 示例, instead of 这个 placeholder $z^{(i)}$, 我们 consider 这个 second 输出 context vector, $z^{(2)}$\n",
        "      - 这个 second context vector, $z^{(2)}$, is 一个 weighted sum over all inputs $x^{(1)}$ to $x^{(T)}$ weighted with respect to 这个 second 输入 element, $x^{(2)}$\n",
        "      - 这个 注意力机制 weights are 这个 weights 那个 determine 如何 much each of 这个 输入 elements contributes to 这个 weighted sum 当 computing $z^{(2)}$\n",
        "      - In short, think of $z^{(2)}$ as 一个 modified version of $x^{(2)}$ 那个 also incorporates information about all other 输入 elements 那个 are relevant to 一个 given task at hand"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcc7c7a2-b6ab-478f-ae37-faa8eaa8049a",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/07.webp\" width=\"400px\">\n",
        "\n",
        "- (Please note 那个 这个 numbers in 这个 figure are truncated to one\n",
        "digit after 这个 decimal point to reduce visual clutter; similarly, other figures may also contain truncated values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff856c58-8382-44c7-827f-798040e6e697",
      "metadata": {},
      "source": [
        "- By convention, 这个 unnormalized 注意力机制 weights are referred to as **\"注意力机制 scores\"** whereas 这个 normalized 注意力机制 scores, 哪个 sum to 1, are referred to as **\"注意力机制 weights\"**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b10344-128d-462a-823f-2178dff5fd58",
      "metadata": {},
      "source": [
        "- 这个 代码 below walks through 这个 figure above step by step\n",
        "\n",
        "<br>\n",
        "\n",
        "- **Step 1:** 计算 unnormalized 注意力机制 scores $\\omega$\n",
        "- Suppose 我们 使用 这个 second 输入 词元 as 这个 query, 那个 is, $q^{(2)} = x^{(2)}$, 我们 计算 这个 unnormalized 注意力机制 scores via dot products:\n",
        "    - $\\omega_{21} = x^{(1)} q^{(2)\\top}$\n",
        "    - $\\omega_{22} = x^{(2)} q^{(2)\\top}$\n",
        "    - $\\omega_{23} = x^{(3)} q^{(2)\\top}$\n",
        "    - ...\n",
        "    - $\\omega_{2T} = x^{(T)} q^{(2)\\top}$\n",
        "- Above, $\\omega$ is 这个 Greek letter \"omega\" used to symbolize 这个 unnormalized 注意力机制 scores\n",
        "    - 这个 subscript \"21\" in $\\omega_{21}$ means 那个 输入 sequence element 2 was used as 一个 query against 输入 sequence element 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e55f7a-f2d0-4f24-858b-228e4fe88fb3",
      "metadata": {},
      "source": [
        "- Suppose 我们 have 这个 following 输入 sentence 那个 is already embedded in 3-dimensional vectors as described in 第 3 (我们 使用 一个 very small 嵌入 dimension 这里 for illustration purposes, so 那个 它 fits onto 这个 page without line breaks):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "22b9556a-aaf8-4ab4-a5b4-973372b0b2c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n\ninputs = torch.tensor(\n  [[0.43, 0.15, 0.89], # Your     (x^1)\n   [0.55, 0.87, 0.66], # journey  (x^2)\n   [0.57, 0.85, 0.64], # starts   (x^3)\n   [0.22, 0.58, 0.33], # with     (x^4)\n   [0.77, 0.25, 0.10], # one      (x^5)\n   [0.05, 0.80, 0.55]] # step     (x^6)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "299baef3-b1a8-49ba-bad4-f62c8a416d83",
      "metadata": {},
      "source": [
        "- (In 这个 book, 我们 follow 这个 common 机器学习 和 深度学习 convention 哪里 训练 examples are represented as rows 和 特征 values as columns; in 这个 case of 这个 tensor shown above, each row represents 一个 word, 和 each column represents 一个 嵌入 dimension)\n",
        "\n",
        "- 这个 primary objective of 这个 section is to demonstrate 如何 这个 context vector $z^{(2)}$\n",
        "  is calculated using 这个 second 输入 sequence, $x^{(2)}$, as 一个 query\n",
        "\n",
        "- 这个 figure depicts 这个 initial step in 这个 处理, 哪个 involves calculating 这个 注意力机制 scores ω between $x^{(2)}$\n",
        "  和 all other 输入 elements through 一个 dot product operation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cb3453a-58fa-42c4-b225-86850bc856f8",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/08.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77be52fb-82fd-4886-a4c8-f24a9c87af22",
      "metadata": {},
      "source": [
        "- 我们 使用 输入 sequence element 2, $x^{(2)}$, as 一个 示例 to 计算 context vector $z^{(2)}$; later in 这个 section, 我们 will generalize 这个 to 计算 all context vectors.\n",
        "- 这个 首先 step is to 计算 这个 unnormalized 注意力机制 scores by computing 这个 dot product between 这个 query $x^{(2)}$ 和 all other 输入 tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6fb5b2f8-dd2c-4a6d-94ef-a0e9ad163951",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ],
      "source": [
        "query = inputs[1]  # 2nd 输入 词元 is 这个 query\n\nattn_scores_2 = torch.empty(inputs.shape[0])\nfor i, x_i in enumerate(inputs):\n    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary 这里 since they are 1-dim vectors)\n\nprint(attn_scores_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df09ae0-199f-4b6f-81a0-2f70546684b8",
      "metadata": {},
      "source": [
        "- Side note: 一个 dot product is essentially 一个 shorthand for multiplying two vectors elements-wise 和 summing 这个 resulting products:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9842f39b-1654-410e-88bf-d1b899bf0241",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.9544)\n",
            "tensor(0.9544)\n"
          ]
        }
      ],
      "source": [
        "res = 0.\n\nfor idx, element in enumerate(inputs[0]):\n    res += inputs[0][idx] * query[idx]\n\nprint(res)\nprint(torch.dot(inputs[0], query))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d444d76-e19e-4e9a-a268-f315d966609b",
      "metadata": {},
      "source": [
        "- **Step 2:** normalize 这个 unnormalized 注意力机制 scores (\"omegas\", $\\omega$) so 那个 they sum up to 1\n",
        "- 这里 is 一个 simple way to normalize 这个 unnormalized 注意力机制 scores to sum up to 1 (一个 convention, useful for interpretation, 和 important for 训练 stability):"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd965d6-980c-476a-93d8-9efe603b1b3b",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/09.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e3ccc99c-33ce-4f11-b7f2-353cf1cbdaba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum: tensor(1.0000)\n"
          ]
        }
      ],
      "source": [
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n\nprint(\"Attention weights:\", attn_weights_2_tmp)\nprint(\"Sum:\", attn_weights_2_tmp.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75dc0a57-f53e-41bf-8793-daa77a819431",
      "metadata": {},
      "source": [
        "- However, in practice, using 这个 softmax 函数 for 归一化, 哪个 is better at handling extreme values 和 has more desirable 梯度 properties during 训练, is common 和 recommended.\n",
        "- 这里's 一个 naive 实现 of 一个 softmax 函数 for scaling, 哪个 also normalizes 这个 vector elements such 那个 they sum up to 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "07b2e58d-a6ed-49f0-a1cd-2463e8d53a20",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "def softmax_naive(x):\n    return torch.exp(x) / torch.exp(x).sum(dim=0)\n\nattn_weights_2_naive = softmax_naive(attn_scores_2)\n\nprint(\"Attention weights:\", attn_weights_2_naive)\nprint(\"Sum:\", attn_weights_2_naive.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0a1cbbb-4744-41cb-8910-f5c1355555fb",
      "metadata": {},
      "source": [
        "- 这个 naive 实现 above can suffer from numerical instability issues for large 或者 small 输入 values due to overflow 和 underflow issues\n",
        "- Hence, in practice, 它's recommended to 使用 这个 PyTorch 实现 of softmax instead, 哪个 has been highly optimized for 性能:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2d99cac4-45ea-46b3-b3c1-e000ad16e158",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n\nprint(\"Attention weights:\", attn_weights_2)\nprint(\"Sum:\", attn_weights_2.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e43e36c7-90b2-427f-94f6-bb9d31b2ab3f",
      "metadata": {},
      "source": [
        "- **Step 3**: 计算 这个 context vector $z^{(2)}$ by multiplying 这个 embedded 输入 tokens, $x^{(i)}$ with 这个 注意力机制 weights 和 sum 这个 resulting vectors:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1c9f5ac-8d3d-4847-94e3-fd783b7d4d3d",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/10.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8fcb96f0-14e5-4973-a50e-79ea7c6af99f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ],
      "source": [
        "query = inputs[1] # 2nd 输入 词元 is 这个 query\n\ncontext_vec_2 = torch.zeros(query.shape)\nfor i,x_i in enumerate(inputs):\n    context_vec_2 += attn_weights_2[i]*x_i\n\nprint(context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a454262-40eb-430e-9ca4-e43fb8d6cd89",
      "metadata": {},
      "source": [
        "### 3.3.2 Computing 注意力机制 weights for all 输入 tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a02bb73-fc19-4c88-b155-8314de5d63a8",
      "metadata": {},
      "source": [
        "#### Generalize to all 输入 sequence tokens:\n",
        "\n",
        "- Above, 我们 computed 这个 注意力机制 weights 和 context vector for 输入 2 (as illustrated in 这个 highlighted row in 这个 figure below)\n",
        "- 接下来, 我们 are generalizing 这个 computation to 计算 all 注意力机制 weights 和 context vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11c0fb55-394f-42f4-ba07-d01ae5c98ab4",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/11.webp\" width=\"400px\">\n",
        "\n",
        "- (Please note 那个 这个 numbers in 这个 figure are truncated to two\n",
        "digits after 这个 decimal point to reduce visual clutter; 这个 values in each row should 添加 up to 1.0 或者 100%; similarly, digits in other figures are truncated)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b789b990-fb51-4beb-9212-bf58876b5983",
      "metadata": {},
      "source": [
        "- In self-注意力机制, 这个 处理 starts with 这个 calculation of 注意力机制 scores, 哪个 are subsequently normalized to derive 注意力机制 weights 那个 total 1\n",
        "- These 注意力机制 weights are 然后 utilized to 生成 这个 context vectors through 一个 weighted summation of 这个 inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9bffe4b-56fe-4c37-9762-24bd924b7d3c",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/12.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa652506-f2c8-473c-a905-85c389c842cc",
      "metadata": {},
      "source": [
        "- 应用 previous **step 1** to all pairwise elements to 计算 这个 unnormalized 注意力机制 score matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "04004be8-07a1-468b-ab33-32e16a551b45",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = torch.empty(6, 6)\n\nfor i, x_i in enumerate(inputs):\n    for j, x_j in enumerate(inputs):\n        attn_scores[i, j] = torch.dot(x_i, x_j)\n\nprint(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1539187f-1ece-47b7-bc9b-65a97115f1d4",
      "metadata": {},
      "source": [
        "- 我们 can achieve 这个 same as above more efficiently via matrix multiplication:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2cea69d0-9a47-45da-8d5a-47ceef2df673",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = inputs @ inputs.T\nprint(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02c4bac4-acfd-427f-9b11-c436ac71748d",
      "metadata": {},
      "source": [
        "- Similar to **step 2** previously, 我们 normalize each row so 那个 这个 values in each row sum to 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fa4ef062-de81-47ee-8415-bfe1708c81b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(attn_scores, dim=-1)\nprint(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fa6d02b-7f15-4eb4-83a7-0b8a819e7a0c",
      "metadata": {},
      "source": [
        "- Quick verification 那个 这个 values in each row indeed sum to 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "112b492c-fb6f-4e6d-8df5-518ae83363d5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Row 2 sum: 1.0\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nprint(\"Row 2 sum:\", row_2_sum)\n\nprint(\"All row sums:\", attn_weights.sum(dim=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138b0b5c-d813-44c7-b373-fde9540ddfd1",
      "metadata": {},
      "source": [
        "- 应用 previous **step 3** to 计算 all context vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ba8eafcf-f7f7-4989-b8dc-61b50c4f81dc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ],
      "source": [
        "all_context_vecs = attn_weights @ inputs\nprint(all_context_vecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b245b8-7732-4fab-aa1c-e3d333195605",
      "metadata": {},
      "source": [
        "- As 一个 sanity 检查, 这个 previously computed context vector $z^{(2)} = [0.4419, 0.6515, 0.5683]$ can be found in 这个 2nd row in above: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2570eb7d-aee1-457a-a61e-7544478219fa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ],
      "source": [
        "print(\"Previous 2nd context vector:\", context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a303b6fb-9f7e-42bb-9fdb-2adabf0a6525",
      "metadata": {},
      "source": [
        "## 3.4 Implementing self-注意力机制 with trainable weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88363117-93d8-41fb-8240-f7cfe08b14a3",
      "metadata": {},
      "source": [
        "- 一个 conceptual framework illustrating 如何 这个 self-注意力机制 mechanism developed in 这个 section integrates into 这个 overall narrative 和 structure of 这个 book 和 第"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac9492ba-6f66-4f65-bd1d-87cf16d59928",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/13.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b90a77e-d746-4704-9354-1ddad86e6298",
      "metadata": {},
      "source": [
        "### 3.4.1 Computing 这个 注意力机制 weights step by step"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46e95a46-1f67-4b71-9e84-8e2db84ab036",
      "metadata": {},
      "source": [
        "- In 这个 section, 我们 are implementing 这个 self-注意力机制 mechanism 那个 is used in 这个 original Transformer architecture, 这个 GPT models, 和 most other popular LLMs\n",
        "- 这个 self-注意力机制 mechanism is also called \"scaled dot-product 注意力机制\"\n",
        "- 这个 overall idea is similar to before:\n",
        "  - 我们 want to 计算 context vectors as weighted sums over 这个 输入 vectors specific to 一个 certain 输入 element\n",
        "  - For 这个 above, 我们 need 注意力机制 weights\n",
        "- As 你 will see, 那里 are only slight differences compared to 这个 basic 注意力机制 mechanism introduced earlier:\n",
        "  - 这个 most notable difference is 这个 介绍 of 权重 matrices 那个 are updated during 模型 训练\n",
        "  - These trainable 权重 matrices are crucial so 那个 这个 模型 (specifically, 这个 注意力机制 模块 inside 这个 模型) can learn to 产生 \"good\" context vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59db4093-93e8-4bee-be8f-c8fac8a08cdd",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/14.webp\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d996671-87aa-45c9-b2e0-07a7bcc9060a",
      "metadata": {},
      "source": [
        "- Implementing 这个 self-注意力机制 mechanism step by step, 我们 will 开始 by introducing 这个 three 训练 权重 matrices $W_q$, $W_k$, 和 $W_v$\n",
        "- These three matrices are used to project 这个 embedded 输入 tokens, $x^{(i)}$, into query, key, 和 value vectors via matrix multiplication:\n",
        "\n",
        "  - Query vector: $q^{(i)} = x^{(i)}\\,W_q $\n",
        "  - Key vector: $k^{(i)} = x^{(i)}\\,W_k $\n",
        "  - Value vector: $v^{(i)} = x^{(i)}\\,W_v $\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f334313-5fd0-477b-8728-04080a427049",
      "metadata": {},
      "source": [
        "- 这个 嵌入 dimensions of 这个 输入 $x$ 和 这个 query vector $q$ can be 这个 same 或者 different, depending on 这个 模型's design 和 specific 实现\n",
        "- In GPT models, 这个 输入 和 输出 dimensions are usually 这个 same, 但是 for illustration purposes, to better follow 这个 computation, 我们 choose different 输入 和 输出 dimensions 这里:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8250fdc6-6cd6-4c5b-b9c0-8c643aadb7db",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_2 = inputs[1] # second 输入 element\nd_in = inputs.shape[1] # 这个 输入 嵌入 size, d=3\nd_out = 2 # 这个 输出 嵌入 size, d=2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f528cfb3-e226-47dd-b363-cc2caaeba4bf",
      "metadata": {},
      "source": [
        "- Below, 我们 初始化 这个 three 权重 matrices; note 那个 我们 are setting `requires_grad=False` to reduce clutter in 这个 outputs for illustration purposes, 但是 如果 我们 were to 使用 这个 权重 matrices for 模型 训练, 我们 would 设置 `requires_grad=True` to 更新 these matrices during 模型 训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "bfd7259a-f26c-4cea-b8fc-282b5cae1e00",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n\nW_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abfd0b50-7701-4adb-821c-e5433622d9c4",
      "metadata": {},
      "source": [
        "- 接下来 我们 计算 这个 query, key, 和 value vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "73cedd62-01e1-4196-a575-baecc6095601",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ],
      "source": [
        "query_2 = x_2 @ W_query # _2 because 它's with respect to 这个 2nd 输入 element\nkey_2 = x_2 @ W_key \nvalue_2 = x_2 @ W_value\n\nprint(query_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be308b3-aca3-421b-b182-19c3a03b71c7",
      "metadata": {},
      "source": [
        "- As 我们 can see below, 我们 successfully projected 这个 6 输入 tokens from 一个 3D onto 一个 2D 嵌入 space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8c1c3949-fc08-4d19-a41e-1c235b4e631b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n"
          ]
        }
      ],
      "source": [
        "keys = inputs @ W_key \nvalues = inputs @ W_value\n\nprint(\"keys.shape:\", keys.shape)\nprint(\"values.shape:\", values.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac5dfd6-ade8-4e7b-b0c1-bed40aa24481",
      "metadata": {},
      "source": [
        "- In 这个 接下来 step, **step 2**, 我们 计算 这个 unnormalized 注意力机制 scores by computing 这个 dot product between 这个 query 和 each key vector:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ed0a2b7-5c50-4ede-90cf-7ad74412b3aa",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/15.webp\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "64cbc253-a182-4490-a765-246979ea0a28",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ],
      "source": [
        "keys_2 = keys[1] # Python starts index at 0\nattn_score_22 = query_2.dot(keys_2)\nprint(attn_score_22)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e9d15c0-c24e-4e6f-a160-6349b418f935",
      "metadata": {},
      "source": [
        "- Since 我们 have 6 inputs, 我们 have 6 注意力机制 scores for 这个 given query vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b14e44b5-d170-40f9-8847-8990804af26d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ],
      "source": [
        "attn_scores_2 = query_2 @ keys.T # All 注意力机制 scores for given query\nprint(attn_scores_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8622cf39-155f-4eb5-a0c0-82a03ce9b999",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/16.webp\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1609edb-f089-461a-8de2-c20c1bb29836",
      "metadata": {},
      "source": [
        "- 接下来, in **step 3**, 我们 计算 这个 注意力机制 weights (normalized 注意力机制 scores 那个 sum up to 1) using 这个 softmax 函数 我们 used earlier\n",
        "- 这个 difference to earlier is 那个 我们 现在 scale 这个 注意力机制 scores by dividing them by 这个 square root of 这个 嵌入 dimension, $\\sqrt{d_k}$ (i.e., `d_k**0.5`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "146f5587-c845-4e30-9894-c7ed3a248153",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
          ]
        }
      ],
      "source": [
        "d_k = keys.shape[1]\nattn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\nprint(attn_weights_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8f61a28-b103-434a-aee1-ae7cbd821126",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/17.webp\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1890e3f9-db86-4ab8-9f3b-53113504a61f",
      "metadata": {},
      "source": [
        "- In **step 4**, 我们 现在 计算 这个 context vector for 输入 query vector 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e138f033-fa7e-4e3a-8764-b53a96b26397",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ],
      "source": [
        "context_vec_2 = attn_weights_2 @ values\nprint(context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d7b2907-e448-473e-b46c-77735a7281d8",
      "metadata": {},
      "source": [
        "### 3.4.2 Implementing 一个 compact SelfAttention 类"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04313410-3155-4d90-a7a3-2f3386e73677",
      "metadata": {},
      "source": [
        "- Putting 它 all together, 我们 can 实现 这个 self-注意力机制 mechanism as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "51590326-cdbe-4e62-93b1-17df71c11ee4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n\nclass SelfAttention_v1(nn.Module):\n\n    def __init__(self, d_in, d_out):\n        super().__init__()\n        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n\n    def forward(self, x):\n        keys = x @ self.W_key\n        queries = x @ self.W_query\n        values = x @ self.W_value\n        \n        attn_scores = queries @ keys.T # omega\n        attn_weights = torch.softmax(\n            attn_scores / keys.shape[-1]**0.5, dim=-1\n        )\n\n        context_vec = attn_weights @ values\n        return context_vec\n\ntorch.manual_seed(123)\nsa_v1 = SelfAttention_v1(d_in, d_out)\nprint(sa_v1(inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ee1a024-84a5-425a-9567-54ab4e4ed445",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/18.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048e0c16-d911-4ec8-b0bc-45ceec75c081",
      "metadata": {},
      "source": [
        "- 我们 can streamline 这个 实现 above using PyTorch's Linear layers, 哪个 are equivalent to 一个 matrix multiplication 如果 我们 disable 这个 偏置 units\n",
        "- Another big advantage of using `nn.Linear` over our manual `nn.参数(torch.rand(...)` approach is 那个 `nn.Linear` has 一个 preferred 权重 initialization scheme, 哪个 leads to more stable 模型 训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "73f411e3-e231-464a-89fe-0a9035e5f839",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "class SelfAttention_v2(nn.Module):\n\n    def __init__(self, d_in, d_out, qkv_bias=False):\n        super().__init__()\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n    def forward(self, x):\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n        \n        attn_scores = queries @ keys.T\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n\n        context_vec = attn_weights @ values\n        return context_vec\n\ntorch.manual_seed(789)\nsa_v2 = SelfAttention_v2(d_in, d_out)\nprint(sa_v2(inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "915cd8a5-a895-42c9-8b8e-06b5ae19ffce",
      "metadata": {},
      "source": [
        "- Note 那个 `SelfAttention_v1` 和 `SelfAttention_v2` give different outputs because they 使用 different initial weights for 这个 权重 matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5025b37-0f2c-4a67-a7cb-1286af7026ab",
      "metadata": {},
      "source": [
        "## 3.5 Hiding future words with causal 注意力机制"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aef0a6b8-205a-45bf-9d26-8fd77a8a03c3",
      "metadata": {},
      "source": [
        "- In causal 注意力机制, 这个 注意力机制 weights above 这个 diagonal are masked, ensuring 那个 for any given 输入, 这个 大语言模型 is unable to 利用 future tokens while calculating 这个 context vectors with 这个 注意力机制 权重"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e91bb5-5aae-4f05-8a95-973b3f988a35",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/19.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82f405de-cd86-4e72-8f3c-9ea0354946ba",
      "metadata": {},
      "source": [
        "### 3.5.1 Applying 一个 causal 注意力机制 mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "014f28d0-8218-48e4-8b9c-bdc5ce489218",
      "metadata": {},
      "source": [
        "- In 这个 section, 我们 are converting 这个 previous self-注意力机制 mechanism into 一个 causal self-注意力机制 mechanism\n",
        "- Causal self-注意力机制 ensures 那个 这个 模型's 预测 for 一个 certain position in 一个 sequence is only dependent on 这个 known outputs at previous positions, not on future positions\n",
        "- In simpler words, 这个 ensures 那个 each 接下来 word 预测 should only depend on 这个 preceding words\n",
        "- To achieve 这个, for each given 词元, 我们 mask out 这个 future tokens (这个 ones 那个 come after 这个 current 词元 in 这个 输入 text):"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57f99af3-32bc-48f5-8eb4-63504670ca0a",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/20.webp\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbfaec7a-68f2-4157-a4b5-2aeceed199d9",
      "metadata": {},
      "source": [
        "- To illustrate 和 实现 causal self-注意力机制, 让我们 work with 这个 注意力机制 scores 和 weights from 这个 previous section: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "1933940d-0fa5-4b17-a3ce-388e5314a1bb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Reuse 这个 query 和 key 权重 matrices of 这个\n# SelfAttention_v2 object from 这个 previous section for convenience\nqueries = sa_v2.W_query(inputs)\nkeys = sa_v2.W_key(inputs) \nattn_scores = queries @ keys.T\n\nattn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\nprint(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89020a96-b34d-41f8-9349-98c3e23fd5d6",
      "metadata": {},
      "source": [
        "- 这个 simplest way to mask out future 注意力机制 weights is by creating 一个 mask via PyTorch's tril 函数 with elements below 这个 main diagonal (including 这个 diagonal itself) 设置 to 1 和 above 这个 main diagonal 设置 to 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "43f3d2e3-185b-4184-9f98-edde5e6df746",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "context_length = attn_scores.shape[0]\nmask_simple = torch.tril(torch.ones(context_length, context_length))\nprint(mask_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efce2b08-3583-44da-b3fc-cabdd38761f6",
      "metadata": {},
      "source": [
        "- 然后, 我们 can multiply 这个 注意力机制 weights with 这个 mask to zero out 这个 注意力机制 scores above 这个 diagonal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9f531e2e-f4d2-4fea-a87f-4c132e48b9e7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "masked_simple = attn_weights*mask_simple\nprint(masked_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eb35787-cf12-4024-b66d-e7215e175500",
      "metadata": {},
      "source": [
        "- However, 如果 这个 mask were applied after softmax, like above, 它 would disrupt 这个 probability distribution created by softmax\n",
        "- Softmax ensures 那个 all 输出 values sum to 1\n",
        "- Masking after softmax would require re-normalizing 这个 outputs to sum to 1 again, 哪个 complicates 这个 处理 和 might lead to unintended effects"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94db92d7-c397-4e42-bd8a-6a2b3e237e0f",
      "metadata": {},
      "source": [
        "- To make sure 那个 这个 rows sum to 1, 我们 can normalize 这个 注意力机制 weights as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "6d392083-fd81-4f70-9bdf-8db985e673d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "row_sums = masked_simple.sum(dim=-1, keepdim=True)\nmasked_simple_norm = masked_simple / row_sums\nprint(masked_simple_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "512e7cf4-dc0e-4cec-948e-c7a3c4eb6877",
      "metadata": {},
      "source": [
        "- While 我们 are technically done with coding 这个 causal 注意力机制 mechanism 现在, 让我们 briefly look at 一个 more efficient approach to achieve 这个 same as above\n",
        "- So, instead of zeroing out 注意力机制 weights above 这个 diagonal 和 renormalizing 这个 results, 我们 can mask 这个 unnormalized 注意力机制 scores above 这个 diagonal with negative infinity before they enter 这个 softmax 函数:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb682900-8df2-4767-946c-a82bee260188",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/21.webp\" width=\"450px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "a2be2f43-9cf0-44f6-8d8b-68ef2fb3cc39",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ],
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\nmasked = attn_scores.masked_fill(mask.bool(), -torch.inf)\nprint(masked)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d5f803-d735-4543-b9da-00ac10fb9c50",
      "metadata": {},
      "source": [
        "- As 我们 can see below, 现在 这个 注意力机制 weights in each row correctly sum to 1 again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b1cd6d7f-16f2-43c1-915e-0824f1a4bc52",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\nprint(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7636fc5f-6bc6-461e-ac6a-99ec8e3c0912",
      "metadata": {},
      "source": [
        "### 3.5.2 Masking additional 注意力机制 weights with dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec3dc7ee-6539-4fab-804a-8f31a890c85a",
      "metadata": {},
      "source": [
        "- In addition, 我们 also 应用 dropout to reduce overfitting during 训练\n",
        "- Dropout can be applied in several places:\n",
        "  - for 示例, after computing 这个 注意力机制 weights;\n",
        "  - 或者 after multiplying 这个 注意力机制 weights with 这个 value vectors\n",
        "- 这里, 我们 will 应用 这个 dropout mask after computing 这个 注意力机制 weights because 它's more common\n",
        "\n",
        "- Furthermore, in 这个 specific 示例, 我们 使用 一个 dropout rate of 50%, 哪个 means randomly masking out half of 这个 注意力机制 weights. (当 我们 train 这个 GPT 模型 later, 我们 will 使用 一个 lower dropout rate, such as 0.1 或者 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee799cf6-6175-45f2-827e-c174afedb722",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/22.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a575458-a6da-4e54-8688-83e155f2de06",
      "metadata": {},
      "source": [
        "- 如果 我们 应用 一个 dropout rate of 0.5 (50%), 这个 non-dropped values will be scaled accordingly by 一个 factor of 1/0.5 = 2\n",
        "- 这个 scaling is calculated by 这个 formula 1 / (1 - `dropout_rate`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "0de578db-8289-41d6-b377-ef645751e33f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2., 2., 0., 2., 2., 0.],\n",
            "        [0., 0., 0., 2., 0., 2.],\n",
            "        [2., 2., 2., 2., 0., 2.],\n",
            "        [0., 2., 2., 0., 0., 2.],\n",
            "        [0., 2., 0., 2., 0., 2.],\n",
            "        [0., 2., 2., 2., 2., 0.]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\ndropout = torch.nn.Dropout(0.5) # dropout rate of 50%\nexample = torch.ones(6, 6) # 创建 一个 matrix of ones\n\nprint(dropout(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "b16c5edb-942b-458c-8e95-25e4e355381e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\nprint(dropout(attn_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "269df5c8-3e25-49d0-95d3-bb232287404f",
      "metadata": {},
      "source": [
        "- Note 那个 这个 resulting dropout outputs may look different depending on your operating system; 你 can read more about 这个 inconsistency [这里 on 这个 PyTorch issue tracker](https://github.com/pytorch/pytorch/issues/121595)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc14639-5f0f-4840-aa9d-8eb36ea90fb7",
      "metadata": {},
      "source": [
        "### 3.5.3 Implementing 一个 compact causal self-注意力机制 类"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09c41d29-1933-43dc-ada6-2dbb56287204",
      "metadata": {},
      "source": [
        "- 现在, 我们 are ready to 实现 一个 working 实现 of self-注意力机制, including 这个 causal 和 dropout masks\n",
        "- One more thing is to 实现 这个 代码 to handle batches consisting of more than one 输入 so 那个 our `CausalAttention` 类 supports 这个 batch outputs produced by 这个 数据加载器 我们 implemented in 第 2\n",
        "- For simplicity, to simulate such batch 输入, 我们 duplicate 这个 输入 text 示例:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "977a5fa7-a9d5-4e2e-8a32-8e0331ccfe28",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ],
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\nprint(batch.shape) # 2 inputs with 6 tokens each, 和 each 词元 has 嵌入 dimension 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "60d8c2eb-2d8e-4d2c-99bc-9eef8cc53ca0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "class CausalAttention(nn.Module):\n\n    def __init__(self, d_in, d_out, context_length,\n                 dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.dropout = nn.Dropout(dropout) # New\n        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape # New batch dimension b\n        # For inputs 哪里 `num_tokens` exceeds `context_length`, 这个 will result in errors\n        # in 这个 mask creation further below.\n        # In practice, 这个 is not 一个 problem since 这个 大语言模型 (chapters 4-7) ensures 那个 inputs  \n        # do not exceed `context_length` before reaching 这个 forward 方法. \n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n        attn_scores.masked_fill_(  # New, _ ops are in-place\n            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases 哪里 这个 number of tokens in 这个 batch is smaller than 这个 supported context_size\n        attn_weights = torch.softmax(\n            attn_scores / keys.shape[-1]**0.5, dim=-1\n        )\n        attn_weights = self.dropout(attn_weights) # New\n\n        context_vec = attn_weights @ values\n        return context_vec\n\ntorch.manual_seed(123)\n\ncontext_length = batch.shape[1]\nca = CausalAttention(d_in, d_out, context_length, 0.0)\n\ncontext_vecs = ca(batch)\n\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4333d12-17e4-4bb5-9d83-54b3a32618cd",
      "metadata": {},
      "source": [
        "- Note 那个 dropout is only applied during 训练, not during 推理"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a554cf47-558c-4f45-84cd-bf9b839a8d50",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/23.webp\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8bef90f-cfd4-4289-b0e8-6a00dc9be44c",
      "metadata": {},
      "source": [
        "## 3.6 Extending single-head 注意力机制 to multi-head 注意力机制"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11697757-9198-4a1c-9cee-f450d8bbd3b9",
      "metadata": {},
      "source": [
        "### 3.6.1 Stacking multiple single-head 注意力机制 layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70766faf-cd53-41d9-8a17-f1b229756a5a",
      "metadata": {},
      "source": [
        "- Below is 一个 summary of 这个 self-注意力机制 implemented previously (causal 和 dropout masks not shown for simplicity)\n",
        "\n",
        "- 这个 is also called single-head 注意力机制:\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/24.webp\" width=\"400px\">\n",
        "\n",
        "- 我们 simply stack multiple single-head 注意力机制 modules to obtain 一个 multi-head 注意力机制 模块:\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/25.webp\" width=\"400px\">\n",
        "\n",
        "- 这个 main idea behind multi-head 注意力机制 is to 运行 这个 注意力机制 mechanism multiple times (in parallel) with different, learned linear projections. 这个 allows 这个 模型 to jointly attend to information from different representation subspaces at different positions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "b9a66e11-7105-4bb4-be84-041f1a1f3bd2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        self.heads = nn.ModuleList(\n            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n             for _ in range(num_heads)]\n        )\n\n    def forward(self, x):\n        return torch.cat([head(x) for head in self.heads], dim=-1)\n\n\ntorch.manual_seed(123)\n\ncontext_length = batch.shape[1] # 这个 is 这个 number of tokens\nd_in, d_out = 3, 2\nmha = MultiHeadAttentionWrapper(\n    d_in, d_out, context_length, 0.0, num_heads=2\n)\n\ncontext_vecs = mha(batch)\n\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193d3d2b-2578-40ba-b791-ea2d49328e48",
      "metadata": {},
      "source": [
        "- In 这个 实现 above, 这个 嵌入 dimension is 4, because 我们 `d_out=2` as 这个 嵌入 dimension for 这个 key, query, 和 value vectors as well as 这个 context vector. 和 since 我们 have 2 注意力机制 heads, 我们 have 这个 输出 嵌入 dimension 2*2=4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6836b5da-ef82-4b4c-bda1-72a462e48d4e",
      "metadata": {},
      "source": [
        "### 3.6.2 Implementing multi-head 注意力机制 with 权重 splits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b48d0d-71ba-4fa0-b714-ca80cabcb6f7",
      "metadata": {},
      "source": [
        "- While 这个 above is 一个 intuitive 和 fully functional 实现 of multi-head 注意力机制 (wrapping 这个 single-head 注意力机制 `CausalAttention` 实现 from earlier), 我们 can write 一个 stand-alone 类 called `MultiHeadAttention` to achieve 这个 same\n",
        "\n",
        "- 我们 don't concatenate single 注意力机制 heads for 这个 stand-alone `MultiHeadAttention` 类\n",
        "- Instead, 我们 创建 single W_query, W_key, 和 W_value 权重 matrices 和 然后 split those into individual matrices for each 注意力机制 head:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "110b0188-6e9e-4e56-a988-10523c6c8538",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]],\n",
            "\n",
            "        [[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert (d_out % num_heads == 0), \\\n            \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads # Reduce 这个 projection dim to match desired 输出 dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear 层 to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\n            \"mask\",\n            torch.triu(torch.ones(context_length, context_length),\n                       diagonal=1)\n        )\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n        # As in `CausalAttention`, for inputs 哪里 `num_tokens` exceeds `context_length`, \n        # 这个 will result in errors in 这个 mask creation further below. \n        # In practice, 这个 is not 一个 problem since 这个 大语言模型 (chapters 4-7) ensures 那个 inputs  \n        # do not exceed `context_length` before reaching 这个 forwar\n\n        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # 我们 implicitly split 这个 matrix by adding 一个 `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # 计算 scaled dot-product 注意力机制 (aka self-注意力机制) with 一个 causal mask\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to 这个 number of tokens 和 converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # 使用 这个 mask to fill 注意力机制 scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n        \n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2) \n        \n        # Combine heads, 哪里 self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec) # optional projection\n\n        return context_vec\n\ntorch.manual_seed(123)\n\nbatch_size, context_length, d_in = batch.shape\nd_out = 2\nmha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n\ncontext_vecs = mha(batch)\n\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d334dfb5-2b6c-4c33-82d5-b4e9db5867bb",
      "metadata": {},
      "source": [
        "- Note 那个 这个 above is essentially 一个 rewritten version of `MultiHeadAttentionWrapper` 那个 is more efficient\n",
        "- 这个 resulting 输出 looks 一个 bit different since 这个 random 权重 initializations differ, 但是 both are fully functional implementations 那个 can be used in 这个 GPT 类 我们 will 实现 in 这个 upcoming chapters\n",
        "- Note 那个 in addition, 我们 added 一个 linear projection 层 (`self.out_proj `) to 这个 `MultiHeadAttention` 类 above. 这个 is simply 一个 linear transformation 那个 doesn't 改变 这个 dimensions. 它's 一个 standard convention to 使用 such 一个 projection 层 in 大语言模型 实现, 但是 它's not strictly necessary (recent research has shown 那个 它 can be removed without affecting 这个 modeling 性能; see 这个 further reading section at 这个 结束 of 这个 第)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe5d396-c990-45dc-9908-2c621461f851",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/26.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b0ed78c-e8ac-4f8f-a479-a98242ae8f65",
      "metadata": {},
      "source": [
        "- Note 那个 如果 你 are interested in 一个 compact 和 efficient 实现 of 这个 above, 你 can also consider 这个 [`torch.nn.MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) 类 in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "363701ad-2022-46c8-9972-390d2a2b9911",
      "metadata": {},
      "source": [
        "- Since 这个 above 实现 may look 一个 bit complex at 首先 glance, 让我们 look at 什么 happens 当 executing `attn_scores = queries @ keys.transpose(2, 3)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "e8cfc1ae-78ab-4faa-bc73-98bd054806c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[1.3208, 1.1631, 1.2879],\n",
            "          [1.1631, 2.2150, 1.8424],\n",
            "          [1.2879, 1.8424, 2.0402]],\n",
            "\n",
            "         [[0.4391, 0.7003, 0.5903],\n",
            "          [0.7003, 1.3737, 1.0620],\n",
            "          [0.5903, 1.0620, 0.9912]]]])\n"
          ]
        }
      ],
      "source": [
        "# (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)\na = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n                    [0.8993, 0.0390, 0.9268, 0.7388],\n                    [0.7179, 0.7058, 0.9156, 0.4340]],\n\n                   [[0.0772, 0.3565, 0.1479, 0.5331],\n                    [0.4066, 0.2318, 0.4545, 0.9737],\n                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n\nprint(a @ a.transpose(2, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0587b946-c8f2-4888-adbf-5a5032fbfd7b",
      "metadata": {},
      "source": [
        "- In 这个 case, 这个 matrix multiplication 实现 in PyTorch will handle 这个 4-dimensional 输入 tensor so 那个 这个 matrix multiplication is carried out between 这个 2 last dimensions (num_tokens, head_dim) 和 然后 repeated for 这个 individual heads \n",
        "\n",
        "- For instance, 这个 following becomes 一个 more compact way to 计算 这个 matrix multiplication for each head separately:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "053760f1-1a02-42f0-b3bf-3d939e407039",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First head:\n",
            " tensor([[1.3208, 1.1631, 1.2879],\n",
            "        [1.1631, 2.2150, 1.8424],\n",
            "        [1.2879, 1.8424, 2.0402]])\n",
            "\n",
            "Second head:\n",
            " tensor([[0.4391, 0.7003, 0.5903],\n",
            "        [0.7003, 1.3737, 1.0620],\n",
            "        [0.5903, 1.0620, 0.9912]])\n"
          ]
        }
      ],
      "source": [
        "first_head = a[0, 0, :, :]\nfirst_res = first_head @ first_head.T\nprint(\"First head:\\n\", first_res)\n\nsecond_head = a[0, 1, :, :]\nsecond_res = second_head @ second_head.T\nprint(\"\\nSecond head:\\n\", second_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dec671bf-7938-4304-ad1e-75d9920e7f43",
      "metadata": {},
      "source": [
        "# Summary 和 takeaways"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa3e4113-ffca-432c-b3ec-7a50bd15da25",
      "metadata": {},
      "source": [
        "- See 这个 [./multihead-注意力机制.ipynb](./multihead-注意力机制.ipynb) 代码 笔记本, 哪个 is 一个 concise version of 这个 数据加载器 (第 2) plus 这个 multi-head 注意力机制 类 那个 我们 implemented in 这个 第 和 will need for 训练 这个 GPT 模型 in upcoming chapters\n",
        "- 你 can find 这个 练习 解答 in [./练习-解答.ipynb](./练习-解答.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}