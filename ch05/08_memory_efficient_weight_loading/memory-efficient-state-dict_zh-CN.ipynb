{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E_HhLEeYqFG"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary 代码 for 这个 <一个 href=\"http://mng.bz/orYv\">构建 一个 大语言模型 From Scratch</一个> book by <一个 href=\"https://sebastianraschka.com\">Sebastian Raschka</一个><br>\n",
        "<br>代码 repository: <一个 href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</一个>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<一个 href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></一个>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuWudYFWYiH7"
      },
      "source": [
        "# Memory-efficient 模型 权重 Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt0Qyg6ewUt6"
      },
      "source": [
        "- 这个 笔记本 provides tips for loading larger pretrained 或者 finetuned models 当 GPU (或者 CPU) memory is limited\n",
        "- Specifically, 它 focuses on cases 哪里 你 saved 这个 模型 using `torch.保存(模型.state_dict(), \"模型.pth\")` (for 示例, in chapters 5-7) 和 want to 加载 它 in 一个 new session later for continued pretraining 或者 additional finetuning\n",
        "- While 这个 示例 uses 一个 大语言模型, 这个 methods explained in 这个 笔记本 are general 和 应用 to loading any PyTorch 模型, not just LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/memory-efficient-loading/memory-efficient-loading.webp\" width=\"800px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxQzFoS-IXdY",
        "outputId": "b28ebfbd-9036-4696-d95a-7f96fdf29919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "memory_profiler version: 0.61.0\n",
            "torch version: 2.4.1+cu121\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n\npkgs = [\n    \"torch\",\n]\nfor p in pkgs:\n    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y47iQaQKyHap"
      },
      "source": [
        "&nbsp;\n",
        "## 1. Benchmark utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQeOEoo6yT0X"
      },
      "source": [
        "- 首先, 让我们 定义 some utility 代码 to track VRAM (GPU memory)\n",
        "- Later, 我们 will also introduce 一个 tool to track 这个 main system RAM (CPU memory)\n",
        "- 这个 purpose of these functions will become 清除 当 我们 应用 them later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pEiqjYrVivgt"
      },
      "outputs": [],
      "source": [
        "import gc\nimport time\nimport torch\n\n\ndef start_memory_tracking():\n    \"\"\"初始化 GPU memory tracking.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n    else:\n        print(\"This notebook is intended for CUDA GPUs but CUDA is not available.\")\n\ndef print_memory_usage():\n    max_gpu_memory = torch.cuda.max_memory_allocated() / (1024 ** 3)  # 转换 bytes to GB\n    print(f\"Maximum GPU memory allocated: {max_gpu_memory:.1f} GB\")\n\ndef cleanup():\n    gc.collect()\n    torch.cuda.empty_cache()\n    time.sleep(3)  # some buffer time to allow memory to 清除\n    torch.cuda.reset_peak_memory_stats()\n    max_memory_allocated = torch.cuda.max_memory_allocated(device) / (1024 ** 3)\n    print(f\"Maximum GPU memory allocated: {max_memory_allocated:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5oJwoc-kkXs"
      },
      "source": [
        "&nbsp;\n",
        "## 2. 模型 setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfJE0vnMyr88"
      },
      "source": [
        "- 这个 代码 section sets up 这个 模型 itself\n",
        "- 这里, 我们 使用 这个 \"large\" GPT-2 模型 to make things more interesting (你 may 使用 这个 \"gpt2-small (124M)\" to lower 这个 memory 依赖 和 execution time of 这个 笔记本)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tMuhCYaVI0w7"
      },
      "outputs": [],
      "source": [
        "from previous_chapters import GPTModel\n# 如果 这个 `previous_chapters.py` file is not available locally,\n# 你 can 导入 它 from 这个 `llms-from-scratch` PyPI 包.\n# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n# E.g.,\n# from llms_from_scratch.ch04 导入 GPTModel\n\n\n\nBASE_CONFIG = {\n    \"vocab_size\": 50257,     # Vocabulary size\n    \"context_length\": 1024,  # Context length\n    \"drop_rate\": 0.0,        # Dropout rate\n    \"qkv_bias\": True         # Query-key-value 偏置\n}\n\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\nCHOOSE_MODEL = \"gpt2-xl (1558M)\"\n\nBASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWYoo1z5y8aX"
      },
      "source": [
        "- 现在, 让我们 see 这个 GPU memory functions in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK3NEA3eJv3f",
        "outputId": "60573d6e-c603-45e7-8283-b1e92e2a0013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum GPU memory allocated: 6.4 GB\n"
          ]
        }
      ],
      "source": [
        "start_memory_tracking()\n\n\nmodel = GPTModel(BASE_CONFIG)\ndevice = torch.device(\"cuda\")\nmodel.to(device)\n\nprint_memory_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIhwBEBxzBsF"
      },
      "source": [
        "- Additionally, 让我们 make sure 那个 这个 模型 runs okay by passing in some 示例 tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i_j6nZruUd7g"
      },
      "outputs": [],
      "source": [
        "# 测试 如果 这个 模型 works (no need to track memory 这里)\ntest_input = torch.tensor([[1, 2, 3]]).to(device)\nmodel.eval()\n\nwith torch.no_grad():\n    model(test_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgNb8c32zh4g"
      },
      "source": [
        "- 接下来, imagine 我们 were pretraining 这个 模型 和 saving 它 for later 使用\n",
        "- 我们 skip 这个 actual pretraining 这里 for simplicity 和 just 保存 这个 initialized 模型 (但是 这个 same concept applies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wUIXjcsimXU7"
      },
      "outputs": [],
      "source": [
        "# 训练 代码 would go 这里...\n\nmodel.train()\ntorch.save(model.state_dict(), \"model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9tBS4HUzz1g"
      },
      "source": [
        "- Lastly, 我们 删除 这个 模型 和 示例 tensor in 这个 Python session to 重置 这个 GPU memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqmTzztqKnTs",
        "outputId": "1198afb9-2d97-4b6a-9bdb-41551f25749d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum GPU memory allocated: 0.0 GB\n"
          ]
        }
      ],
      "source": [
        "del model, test_input\ncleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EnO8beUJ6Sb"
      },
      "source": [
        "&nbsp;\n",
        "## 3. 权重 loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtAXKjsG0AVL"
      },
      "source": [
        "- 现在 begins 这个 interesting part 哪里 我们 加载 这个 pretrained 模型 weights\n",
        "- 让我们 see 如何 much GPU memory is required to 加载 这个 previously saved 模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCrQNbSJJO9w",
        "outputId": "9b203868-a8ef-4011-fc2b-611cc0d10994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum GPU memory allocated: 12.8 GB\n"
          ]
        }
      ],
      "source": [
        "# 然后 加载 pretrained weights\n\nstart_memory_tracking()\n\nmodel = GPTModel(BASE_CONFIG)\nmodel.to(device)\n\nmodel.load_state_dict(\n    torch.load(\"model.pth\", map_location=device, weights_only=True)\n)\nmodel.to(device)\nmodel.eval();\n\nprint_memory_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AGvOrcN0KdJ"
      },
      "source": [
        "- Notice 那个 这个 memory is 2x as large as in 这个 previous session\n",
        "- 这个 is because 我们 have 这个 same 模型 in memory twice, for 一个 short period of time:\n",
        "  - 这个 首先 time via `模型.to(device)`\n",
        "  - 这个 second time via 这个 代码 line `模型.load_state_dict(torch.加载(\"模型.pth\", map_location=device, weights_only=True))`; eventually, 这个 loaded 模型 weights will be copied into 这个 模型, 和 这个 `state_dict` will be discarded, 但是 for 一个 brief amount of time, 我们 have both 这个 main 模型 和 这个 loaded `state_dict` in memory\n",
        "- 这个 remaining sections focus on addressing 这个\n",
        "- 但是 首先, 让我们 测试 这个 模型 和 重置 这个 GPU memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvlUn-nmmbuj",
        "outputId": "11d3ab68-f570-4c1e-c631-fe5547026799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum GPU memory allocated: 0.0 GB\n"
          ]
        }
      ],
      "source": [
        "# 测试 如果 这个 模型 works (no need to track memory 这里)\ntest_input = torch.tensor([[1, 2, 3]]).to(device)\nmodel.eval()\n\nwith torch.no_grad():\n    model(test_input)\n\ndel model, test_input\ncleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdPnW3iLLrjX"
      },
      "source": [
        "&nbsp;\n",
        "## 4. Loading weights sequentially"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYqtUON602TD"
      },
      "source": [
        "- One workaround for 这个 problem of having 这个 模型 weights in GPU memory twice, as highlighted in 这个 previous section, is to 加载 这个 模型 sequentially\n",
        "- Below, 我们:\n",
        "  - 首先 加载 这个 模型 into GPU memory\n",
        "  - 然后 加载 这个 模型 weights into CPU memory\n",
        "  - 和 最后 copy each 参数 one by one into GPU memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOIGTNWTmx9G",
        "outputId": "145162e6-aaa6-4c2a-ed8f-f1cf068adb80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum GPU memory allocated: 6.4 GB\n",
            "Maximum GPU memory allocated: 6.7 GB\n"
          ]
        }
      ],
      "source": [
        "start_memory_tracking()\n\nmodel = GPTModel(BASE_CONFIG).to(device)\n\nstate_dict = torch.load(\"model.pth\", map_location=\"cpu\", weights_only=True)\n\nprint_memory_usage()\n\n# Sequentially copy weights to 这个 模型's parameters\nwith torch.no_grad():\n    for name, param in model.named_parameters():\n        if name in state_dict:\n            param.copy_(state_dict[name].to(device))\n        else:\n            print(f\"Warning: {name} not found in state_dict.\")\n\nprint_memory_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn9xD_xL1ZzM"
      },
      "source": [
        "- As 我们 can see above, 这个 memory usage is much lower than before\n",
        "- Notice 那个 这个 memory increases from 6.4 to 6.7 GB because initially, 我们 only have 这个 模型 in memory, 和 然后 我们 have 这个 模型 plus 1 参数 tensor in memory (我们 temporarily move 这个 参数 tensor to 这个 GPU so 我们 can assign 它 using `\".to\"` 这个 模型)\n",
        "- Overall, 这个 is 一个 significant improvement\n",
        "- Again, 让我们 briefly 测试 这个 模型 和 然后 重置 这个 GPU memory for 这个 接下来 section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRHnjA48nJgw",
        "outputId": "dcd6b1b2-538f-4862-96a6-a5fcbf3326a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum GPU memory allocated: 0.0 GB\n"
          ]
        }
      ],
      "source": [
        "# 测试 如果 这个 模型 works (no need to track memory 这里)\ntest_input = torch.tensor([[1, 2, 3]]).to(device)\nmodel.eval()\n\nwith torch.no_grad():\n    model(test_input)\n\ndel model, test_input, state_dict, param\ncleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M92LK7usb-Z"
      },
      "source": [
        "&nbsp;\n",
        "## 5. Loading 这个 模型 with low CPU memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R45qgeB613e2"
      },
      "source": [
        "- In 这个 previous session, 我们 reduced GPU memory 使用 by loading 这个 weights (`state_dict`) into CPU memory 首先 before copying them one-by-one into 这个 模型\n",
        "- However, 什么 do 我们 do 如果 我们 have limited CPU memory?\n",
        "- 这个 section uses PyTorch's so-called `\"meta\"` device approach to 加载 一个 模型 on machines with large GPU memory 但是 small CPU memory\n",
        "- 但是 首先, 让我们 定义 一个 convenience 函数 to monitor CPU memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BrcWy0q-3Bbe"
      },
      "outputs": [],
      "source": [
        "import os\nimport psutil\nfrom threading import Thread\n\n\ndef memory_usage_in_gb(func, *args, **kwargs):\n    process = psutil.Process(os.getpid())\n\n    # Measure 这个 baseline memory usage before running 这个 函数\n    baseline_mem = process.memory_info().rss / 1024 ** 3  # in GB\n\n    # 开始 monitoring memory in 一个 separate thread\n    mem_usage = []\n    done = False\n\n    def monitor_memory():\n        while not done:\n            mem_usage.append(process.memory_info().rss / 1024 ** 3)  # 转换 to GB\n            time.sleep(0.1)\n\n    t = Thread(target=monitor_memory)\n    t.start()\n\n    # 运行 这个 函数\n    func(*args, **kwargs)\n\n    # 停止 monitoring\n    done = True\n    t.join()\n\n    peak_mem_usage_gb = max(mem_usage) - baseline_mem\n    return peak_mem_usage_gb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayy30Ytd5hjF"
      },
      "source": [
        "- To 开始 with, 让我们 track 这个 CPU memory of 这个 sequential 权重 loading approach from 这个 previous section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCkV6IbQtpVn",
        "outputId": "26c0435a-1e3d-4e8f-fbe2-f9655bad61b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum GPU memory allocated: 6.4 GB\n",
            "Maximum GPU memory allocated: 6.7 GB\n",
            "-> Maximum CPU memory allocated: 6.3 GB\n"
          ]
        }
      ],
      "source": [
        "def load_sequentially():\n    start_memory_tracking()\n\n    model = GPTModel(BASE_CONFIG).to(device)\n\n    state_dict = torch.load(\"model.pth\", map_location=\"cpu\", weights_only=True)\n\n    print_memory_usage()\n\n    # Sequentially copy weights to 这个 模型's parameters\n    with torch.no_grad():\n        for name, param in model.named_parameters():\n            if name in state_dict:\n                param.copy_(state_dict[name].to(device))\n            else:\n                print(f\"Warning: {name} not found in state_dict.\")\n\n    print_memory_usage()\n\n\npeak_memory_used = memory_usage_in_gb(load_sequentially)\nprint(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWrmnCML5oKy"
      },
      "source": [
        "- 现在, suppose 我们 have 一个 machine with low CPU memory 但是 large GPU memory\n",
        "- 我们 can trade off CPU memory 和 GPU memory usage by introducing PyTorch's so-called \"meta\" device\n",
        "- PyTorch's meta device is 一个 special device type 那个 allows 你 to 创建 tensors without allocating actual memory for their data, effectively creating \"meta\" tensors\n",
        "- 这个 is useful for tasks like 模型 analysis 或者 architecture definition, 哪里 你 need tensor shapes 和 types without 这个 overhead of memory allocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBErC_5Yt8ly",
        "outputId": "8799db06-191c-47c4-92fa-fbb95d685aa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum GPU memory allocated: 12.8 GB\n",
            "Maximum GPU memory allocated: 12.8 GB\n",
            "-> Maximum CPU memory allocated: 1.3 GB\n"
          ]
        }
      ],
      "source": [
        "def load_sequentially_with_meta():\n    start_memory_tracking()\n\n    with torch.device(\"meta\"):\n        model = GPTModel(BASE_CONFIG)\n\n    model = model.to_empty(device=device)\n\n    state_dict = torch.load(\"model.pth\", map_location=device, weights_only=True)\n\n    print_memory_usage()\n\n    # Sequentially copy weights to 这个 模型's parameters\n    with torch.no_grad():\n        for name, param in model.named_parameters():\n            if name in state_dict:\n                param.copy_(state_dict[name])\n            else:\n                print(f\"Warning: {name} not found in state_dict.\")\n\n    print_memory_usage()\n\npeak_memory_used = memory_usage_in_gb(load_sequentially_with_meta)\nprint(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpnCABp75-VQ"
      },
      "source": [
        "- As 我们 can see above, by creating 这个 模型 on 这个 meta-device 和 loading 这个 weights directly into GPU memory, 我们 effectively reduced 这个 CPU memory 依赖\n",
        "- One might ask: \"Is 这个 sequential 权重 loading still necessary 然后, 和 如何 does 那个 compare to 这个 original approach?\"\n",
        "- 让我们 检查 这个 simple PyTorch 权重 loading approach for comparison (from 这个 首先 权重 loading section in 这个 笔记本):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f-bqBNRuR39",
        "outputId": "f7c0a901-b404-433a-9b93-2bbfa8183c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum GPU memory allocated: 12.8 GB\n",
            "-> Maximum CPU memory allocated: 4.4 GB\n"
          ]
        }
      ],
      "source": [
        "def baseline():\n    start_memory_tracking()\n\n    model = GPTModel(BASE_CONFIG)\n    model.to(device)\n\n    model.load_state_dict(torch.load(\"model.pth\", map_location=device, weights_only=True))\n    model.to(device)\n    model.eval();\n\n    print_memory_usage()\n\npeak_memory_used = memory_usage_in_gb(baseline)\nprint(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKAjxbX86xnb"
      },
      "source": [
        "- As 我们 can see above, 这个 \"simple\" 权重 loading without 这个 meta device uses more memory\n",
        "- In other words, 如果 你 have 一个 machine with limited CPU memory, 你 can 使用 这个 meta device approach to directly 加载 这个 模型 weights into GPU memory to reduce peak CPU memory usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "&nbsp;\n",
        "## 6. Using `mmap=True` (recommmended)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- As 一个 intermediate 或者 advanced `torch.加载` user, 你 may wonder 如何 these approaches compare to 这个 `mmap=True` setting in PyTorch\n",
        "- 这个 `mmap=True` setting in PyTorch enables memory-mapped file I/O, 哪个 allows 这个 tensor to access data directly from disk storage, thus reducing memory usage by not loading 这个 entire file into RAM 如果 RAM is limited\n",
        "- Also, see 这个 helpful comment by [mikaylagawarecki](https://github.com/rasbt/LLMs-from-scratch/issues/402)\n",
        "- At 首先 glance, 它 may look less efficient than 这个 sequential approaches above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKwV0AMNemuR",
        "outputId": "e207f2bf-5c87-498e-80fe-e8c4016ac711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum GPU memory allocated: 6.4 GB\n",
            "-> Maximum CPU memory allocated: 5.9 GB\n"
          ]
        }
      ],
      "source": [
        "def best_practices():\n  with torch.device(\"meta\"):\n      model = GPTModel(BASE_CONFIG)\n\n  model.load_state_dict(\n      torch.load(\"model.pth\", map_location=device, weights_only=True, mmap=True),\n      assign=True\n  )\n\n  print_memory_usage()\n\npeak_memory_used = memory_usage_in_gb(best_practices)\nprint(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 这个 reason 为什么 这个 CPU RAM usage is so high is 那个 那里's enough CPU RAM available on 这个 machine\n",
        "- However, 如果 你 were to 运行 这个 on 一个 machine with limited CPU RAM, 这个 `mmap` approach would 使用 less memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "&nbsp;\n",
        "## 7. Other methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 这个 笔记本 is focused on simple, built-in methods for loading weights in PyTorch\n",
        "- 这个 recommended approach for limited CPU memory cases is 这个 `mmap=True` approach explained enough\n",
        "- Alternatively, one other option is 一个 brute-force approach 那个 saves 和 loads each 权重 tensor separately:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2CgPEZUIb00w"
      },
      "outputs": [],
      "source": [
        "model = GPTModel(BASE_CONFIG)\n# Assume `模型` is your trained 模型\nstate_dict = model.state_dict()\n\n# 创建 一个 directory to store individual 参数 files\nos.makedirs(\"model_parameters\", exist_ok=True)\n\n# 保存 each 参数 tensor separately\nfor name, param in state_dict.items():\n    torch.save(param.cpu(), f\"model_parameters/{name}.pt\")\n\ndel model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTsmtJK-b4yy",
        "outputId": "d361e2d3-e34c-48d7-9047-846c9bfd291e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum GPU memory allocated: 6.4 GB\n",
            "Maximum GPU memory allocated: 6.4 GB\n",
            "-> Maximum CPU memory allocated: 0.3 GB\n"
          ]
        }
      ],
      "source": [
        "def load_individual_weights():\n\n    start_memory_tracking()\n\n    with torch.device(\"meta\"):\n        model = GPTModel(BASE_CONFIG)\n\n    model = model.to_empty(device=device)\n\n    print_memory_usage()\n    param_dir = \"model_parameters\"\n\n    with torch.no_grad():\n        for name, param in model.named_parameters():\n            weight_path = os.path.join(param_dir, f\"{name}.pt\")\n            if os.path.exists(weight_path):\n                param_data = torch.load(weight_path, map_location=\"cpu\", weights_only=True)\n                param.copy_(param_data)\n                del param_data  # Free memory\n            else:\n                print(f\"Warning: {name} not found in {param_dir}.\")\n\n    print_memory_usage()\n\n\npeak_memory_used = memory_usage_in_gb(load_individual_weights)\nprint(f\"-> Maximum CPU memory allocated: {peak_memory_used:.1f} GB\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}