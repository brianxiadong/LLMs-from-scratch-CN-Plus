{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cbbc1fe3-bff1-4631-bf35-342e19c54cc0",
      "metadata": {},
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary 代码 for 这个 <一个 href=\"http://mng.bz/orYv\">构建 一个 大语言模型 From Scratch</一个> book by <一个 href=\"https://sebastianraschka.com\">Sebastian Raschka</一个><br>\n",
        "<br>代码 repository: <一个 href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</一个>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<一个 href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></一个>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b022374-e3f6-4437-b86f-e6f8f94cbebc",
      "metadata": {},
      "source": [
        "# Extending 这个 Tiktoken BPE 分词器 with New Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcd624b1-2060-49af-bbf6-40517a58c128",
      "metadata": {},
      "source": [
        "- 这个 笔记本 explains 如何 我们 can extend 一个 existing BPE 分词器; specifically, 我们 will focus on 如何 to do 它 for 这个 popular [tiktoken](https://github.com/openai/tiktoken) 实现\n",
        "- For 一个 general 介绍 to tokenization, please refer to [第 2](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-第-代码/ch02.ipynb) 和 这个 BPE from Scratch [link] 教程\n",
        "- For 示例, suppose 我们 have 一个 GPT-2 分词器 和 want to encode 这个 following text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "798d4355-a146-48a8-a1a5-c5cec91edf2c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15496, 11, 2011, 3791, 30642, 62, 16, 318, 257, 649, 11241, 13, 220, 50256]\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n\nbase_tokenizer = tiktoken.get_encoding(\"gpt2\")\nsample_text = \"Hello, MyNewToken_1 is a new token. <|endoftext|>\"\n\ntoken_ids = base_tokenizer.encode(sample_text, allowed_special={\"<|endoftext|>\"})\nprint(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b09b19b-772d-4449-971b-8ab052ee726d",
      "metadata": {},
      "source": [
        "- Iterating over each 词元 ID can give us 一个 better understanding of 如何 这个 词元 IDs are decoded via 这个 vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "21fd634b-bb4c-4ba3-8b69-9322b727bf58",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15496 -> Hello\n",
            "11 -> ,\n",
            "2011 ->  My\n",
            "3791 -> New\n",
            "30642 -> Token\n",
            "62 -> _\n",
            "16 -> 1\n",
            "318 ->  is\n",
            "257 ->  a\n",
            "649 ->  new\n",
            "11241 ->  token\n",
            "13 -> .\n",
            "220 ->  \n",
            "50256 -> <|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "for token_id in token_ids:\n    print(f\"{token_id} -> {base_tokenizer.decode([token_id])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd5b1b9b-b1a9-489e-9711-c15a8e081813",
      "metadata": {},
      "source": [
        "- As 我们 can see above, 这个 `\"MyNewToken_1\"` is broken down into 5 individual subword tokens -- 这个 is normal behavior for BPE 当 handling unknown words\n",
        "- However, suppose 那个 它's 一个 special 词元 那个 我们 want to encode as 一个 single 词元, similar to some of 这个 other words 或者 `\"<|endoftext|>\"`; 这个 笔记本 explains 如何"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65f62ab6-df96-4f88-ab9a-37702cd30f5f",
      "metadata": {},
      "source": [
        "&nbsp;\n",
        "## 1. Adding special tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4379fdb-57ba-4a75-9183-0aee0836c391",
      "metadata": {},
      "source": [
        "- Note 那个 我们 have to 添加 new tokens as special tokens; 这个 reason is 那个 我们 don't have 这个 \"merges\" for 这个 new tokens 那个 are created during 这个 分词器 训练 处理 -- even 如果 我们 had them, 它 would be very challenging to incorporate them without breaking 这个 existing tokenization scheme (see 这个 BPE from scratch 笔记本 [link] to understand 这个 \"merges\")\n",
        "- Suppose 我们 want to 添加 2 new tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "265f1bba-c478-497d-b7fc-f4bd191b7d55",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 定义 custom tokens 和 their 词元 IDs\ncustom_tokens = [\"MyNewToken_1\", \"MyNewToken_2\"]\ncustom_token_ids = {\n    token: base_tokenizer.n_vocab + i for i, token in enumerate(custom_tokens)\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c6f3d98-1ab6-43cf-9ae2-2bf53860f99e",
      "metadata": {},
      "source": [
        "- 接下来, 我们 创建 一个 custom `Encoding` object 那个 holds our special tokens as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1f519852-59ea-4069-a8c7-0f647bfaea09",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建 一个 new Encoding object with extended tokens\nextended_tokenizer = tiktoken.Encoding(\n    name=\"gpt2_custom\",\n    pat_str=base_tokenizer._pat_str,\n    mergeable_ranks=base_tokenizer._mergeable_ranks,\n    special_tokens={**base_tokenizer._special_tokens, **custom_token_ids},\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90af6cfa-e0cc-4c80-89dc-3a824e7bdeb2",
      "metadata": {},
      "source": [
        "- 那个's 它, 我们 can 现在 检查 那个 它 can encode 这个 sample text:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "153e8e1d-c4cb-41ff-9c55-1701e9bcae1c",
      "metadata": {},
      "source": [
        "- As 我们 can see, 这个 new tokens `50257` 和 `50258` are 现在 encoded in 这个 输出:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "eccc78a4-1fd4-47ba-a114-83ee0a3aec31",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[36674, 2420, 351, 220, 50257, 290, 220, 50258, 13, 220, 50256]\n"
          ]
        }
      ],
      "source": [
        "special_tokens_set = set(custom_tokens) | {\"<|endoftext|>\"}\n\ntoken_ids = extended_tokenizer.encode(\n    \"Sample text with MyNewToken_1 and MyNewToken_2. <|endoftext|>\",\n    allowed_special=special_tokens_set\n)\nprint(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc0547c1-bbb5-4915-8cf4-caaebcf922eb",
      "metadata": {},
      "source": [
        "- Again, 我们 can also look at 它 on 一个 per-词元 level:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7583eff9-b10d-4e3d-802c-f0464e1ef030",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36674 -> Sample\n",
            "2420 ->  text\n",
            "351 ->  with\n",
            "220 ->  \n",
            "50257 -> MyNewToken_1\n",
            "290 ->  and\n",
            "220 ->  \n",
            "50258 -> MyNewToken_2\n",
            "13 -> .\n",
            "220 ->  \n",
            "50256 -> <|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "for token_id in token_ids:\n    print(f\"{token_id} -> {extended_tokenizer.decode([token_id])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17f0764e-e5a9-4226-a384-18c11bd5fec3",
      "metadata": {},
      "source": [
        "- As 我们 can see above, 我们 have successfully updated 这个 分词器\n",
        "- However, to 使用 它 with 一个 pretrained 大语言模型, 我们 also have to 更新 这个 嵌入 和 输出 layers of 这个 大语言模型, 哪个 is discussed in 这个 接下来 section"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ec7f98d-8f09-4386-83f0-9bec68ef7f66",
      "metadata": {},
      "source": [
        "&nbsp;\n",
        "## 2. Updating 一个 pretrained 大语言模型"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8a4f68b-04e9-4524-8df4-8718c7b566f2",
      "metadata": {},
      "source": [
        "- In 这个 section, 我们 will take 一个 look at 如何 我们 have to 更新 一个 existing pretrained 大语言模型 after updating 这个 分词器\n",
        "- For 这个, 我们 are using 这个 original pretrained GPT-2 模型 那个 is used in 这个 main book"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a9b252e-1d1d-4ddf-b9f3-95bd6ba505a9",
      "metadata": {},
      "source": [
        "&nbsp;\n",
        "### 2.1 Loading 一个 pretrained GPT 模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ded29b4e-9b39-4191-b61c-29d6b2360bae",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "checkpoint: 100%|███████████████████████████| 77.0/77.0 [00:00<00:00, 34.4kiB/s]\n",
            "encoder.json: 100%|███████████████████████| 1.04M/1.04M [00:00<00:00, 4.78MiB/s]\n",
            "hparams.json: 100%|█████████████████████████| 90.0/90.0 [00:00<00:00, 24.7kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|███████| 498M/498M [00:33<00:00, 14.7MiB/s]\n",
            "model.ckpt.index: 100%|███████████████████| 5.21k/5.21k [00:00<00:00, 1.05MiB/s]\n",
            "model.ckpt.meta: 100%|██████████████████████| 471k/471k [00:00<00:00, 2.33MiB/s]\n",
            "vocab.bpe: 100%|████████████████████████████| 456k/456k [00:00<00:00, 2.45MiB/s]\n"
          ]
        }
      ],
      "source": [
        "from llms_from_scratch.ch05 import download_and_load_gpt2\n# For llms_from_scratch 安装 instructions, see:\n# https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n\nsettings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "93dc0d8e-b549-415b-840e-a00023bddcf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llms_from_scratch.ch04 import GPTModel\n# For llms_from_scratch 安装 instructions, see:\n# https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,   # Vocabulary size\n    \"context_length\": 256, # Shortened context length (orig: 1024)\n    \"emb_dim\": 768,        # 嵌入 dimension\n    \"n_heads\": 12,         # Number of 注意力机制 heads\n    \"n_layers\": 12,        # Number of layers\n    \"drop_rate\": 0.1,      # Dropout rate\n    \"qkv_bias\": False      # Query-key-value 偏置\n}\n\n# 定义 模型 configurations in 一个 dictionary for compactness\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\n# Copy 这个 base 配置 和 更新 with specific 模型 settings\nmodel_name = \"gpt2-small (124M)\"  # 示例 模型 name\nNEW_CONFIG = GPT_CONFIG_124M.copy()\nNEW_CONFIG.update(model_configs[model_name])\nNEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n\ngpt = GPTModel(NEW_CONFIG)\ngpt.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f898c0-18f4-49ce-9b1f-3203a277b29e",
      "metadata": {},
      "source": [
        "### 2.2 Using 这个 pretrained GPT 模型"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a1f5e1-e806-4c60-abaa-42ae8564908c",
      "metadata": {},
      "source": [
        "- 接下来, consider our sample text below, 哪个 我们 tokenize using 这个 original 和 这个 new 分词器:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9a88017d-cc8f-4ba1-bba9-38161a30f673",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "sample_text = \"Sample text with MyNewToken_1 and MyNewToken_2. <|endoftext|>\"\n\noriginal_token_ids = base_tokenizer.encode(\n    sample_text, allowed_special={\"<|endoftext|>\"}\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1ee01bc3-ca24-497b-b540-3d13c52c29ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "new_token_ids = extended_tokenizer.encode(\n    \"Sample text with MyNewToken_1 and MyNewToken_2. <|endoftext|>\",\n    allowed_special=special_tokens_set\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1143106b-68fe-4234-98ad-eaff420a4d08",
      "metadata": {},
      "source": [
        "- 现在, 让我们 feed 这个 original 词元 IDs to 这个 GPT 模型:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6b06827f-b411-42cc-b978-5c1d568a3200",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.2204,  0.8901,  1.0138,  ...,  0.2585, -0.9192, -0.2298],\n",
            "         [ 0.6745, -0.0726,  0.8218,  ..., -0.1768, -0.4217,  0.0703],\n",
            "         [-0.2009,  0.0814,  0.2417,  ...,  0.3166,  0.3629,  1.3400],\n",
            "         ...,\n",
            "         [ 0.1137, -0.1258,  2.0193,  ..., -0.0314, -0.4288, -0.1487],\n",
            "         [-1.1983, -0.2050, -0.1337,  ..., -0.0849, -0.4863, -0.1076],\n",
            "         [-1.0675, -0.5905,  0.2873,  ..., -0.0979, -0.8713,  0.8415]]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n\nwith torch.no_grad():\n    out = gpt(torch.tensor([original_token_ids]))\n\nprint(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "082c7a78-35a8-473e-a08d-b099a6348a74",
      "metadata": {},
      "source": [
        "- As 我们 can see above, 这个 works without problems (note 那个 这个 代码 shows 这个 raw 输出 without converting 这个 outputs back into text for simplicity; for more details on 那个, please 检查 out 这个 `生成` 函数 in 第 5 [link] section 5.3.3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "628265b5-3dde-44e7-bde2-8fc594a2547d",
      "metadata": {},
      "source": [
        "- 什么 happens 如果 我们 try 这个 same on 这个 词元 IDs generated by 这个 updated 分词器 现在?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9796ad09-787c-4c25-a7f5-6d1dfe048ac3",
      "metadata": {},
      "source": [
        "```python\n",
        "with torch.no_grad():\n",
        "    GPT(torch.tensor([new_token_ids]))\n",
        "\n",
        "打印(out)\n",
        "\n",
        "...\n",
        "# IndexError: index out of range in self\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77d00244-7e40-4de0-942e-e15cdd8e3b18",
      "metadata": {},
      "source": [
        "- As 我们 can see, 这个 results in 一个 index error\n",
        "- 这个 reason is 那个 这个 GPT 模型 expects 一个 fixed vocabulary size via its 输入 嵌入 层 和 its 输出 层:\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/extend-tiktoken/GPT-updates.webp\" width=\"400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dec38b24-c845-4090-96a4-0d3c4ec241d6",
      "metadata": {},
      "source": [
        "&nbsp;\n",
        "### 2.3 Updating 这个 嵌入 层"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1328726-8297-4162-878b-a5daff7de742",
      "metadata": {},
      "source": [
        "- 让我们 开始 with updating 这个 嵌入 层\n",
        "- 首先, notice 那个 这个 嵌入 层 has 50,257 entries, 哪个 corresponds to 这个 vocabulary size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "23ecab6e-1232-47c7-a318-042f90e1dff3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding(50257, 768)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpt.tok_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d760c683-d082-470a-bff8-5a08b30d3b61",
      "metadata": {},
      "source": [
        "- 我们 want to extend 这个 嵌入 层 by adding 2 more entries\n",
        "- In short, 我们 创建 一个 new 嵌入 层 with 一个 bigger size, 和 然后 我们 copy over 这个 old 嵌入 层 values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4ec5c48e-c6fe-4e84-b290-04bd4da9483f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding(50259, 768)\n"
          ]
        }
      ],
      "source": [
        "num_tokens, emb_size = gpt.tok_emb.weight.shape\nnew_num_tokens = num_tokens + 2\n\n# 创建 一个 new 嵌入 层\nnew_embedding = torch.nn.Embedding(new_num_tokens, emb_size)\n\n# Copy weights from 这个 old 嵌入 层\nnew_embedding.weight.data[:num_tokens] = gpt.tok_emb.weight.data\n\n# Replace 这个 old 嵌入 层 with 这个 new one in 这个 模型\ngpt.tok_emb = new_embedding\n\nprint(gpt.tok_emb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63954928-31a5-4e7e-9688-2e0c156b7302",
      "metadata": {},
      "source": [
        "- As 我们 can see above, 我们 现在 have 一个 increased 嵌入 层"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e68bea5-255b-47bb-b352-09ea9539bc25",
      "metadata": {},
      "source": [
        "&nbsp;\n",
        "### 2.4 Updating 这个 输出 层"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90a4a519-bf0f-4502-912d-ef0ac7a9deab",
      "metadata": {},
      "source": [
        "- 接下来, 我们 have to extend 这个 输出 层, 哪个 has 50,257 输出 features corresponding to 这个 vocabulary size similar to 这个 嵌入 层 (by 这个 way, 你 may find 这个 bonus material, 哪个 discusses 这个 similarity between Linear 和 嵌入 layers in PyTorch, useful)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6105922f-d889-423e-bbcc-bc49156d78df",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Linear(in_features=768, out_features=50257, bias=False)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpt.out_head"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29f1ff24-9c00-40f6-a94f-82d03aaf0890",
      "metadata": {},
      "source": [
        "- 这个 procedure for extending 这个 输出 层 is similar to extending 这个 嵌入 层:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "354589db-b148-4dae-8068-62132e3fb38e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear(in_features=768, out_features=50259, bias=True)\n"
          ]
        }
      ],
      "source": [
        "original_out_features, original_in_features = gpt.out_head.weight.shape\n\n# 定义 这个 new number of 输出 features (e.g., adding 2 new tokens)\nnew_out_features = original_out_features + 2\n\n# 创建 一个 new linear 层 with 这个 extended 输出 size\nnew_linear = torch.nn.Linear(original_in_features, new_out_features)\n\n# Copy 这个 weights 和 biases from 这个 original linear 层\nwith torch.no_grad():\n    new_linear.weight[:original_out_features] = gpt.out_head.weight\n    if gpt.out_head.bias is not None:\n        new_linear.bias[:original_out_features] = gpt.out_head.bias\n\n# Replace 这个 original linear 层 with 这个 new one\ngpt.out_head = new_linear\n\nprint(gpt.out_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df5d2205-1fae-4a4f-a7bd-fa8fc37eeec2",
      "metadata": {},
      "source": [
        "- 让我们 try 这个 updated 模型 on 这个 original 词元 IDs 首先:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "df604bbc-6c13-4792-8ba8-ecb692117c25",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.2267,  0.9132,  1.0494,  ..., -0.2330, -0.3008, -1.1458],\n",
            "         [ 0.6808, -0.0495,  0.8574,  ...,  0.0671,  0.5572, -0.7873],\n",
            "         [-0.1947,  0.1045,  0.2773,  ...,  1.3368,  0.8479, -0.9660],\n",
            "         ...,\n",
            "         [ 0.1200, -0.1027,  2.0549,  ..., -0.1519, -0.2096,  0.5651],\n",
            "         [-1.1920, -0.1819, -0.0981,  ..., -0.1108,  0.8435, -0.3771],\n",
            "         [-1.0612, -0.5674,  0.3229,  ...,  0.8383, -0.7121, -0.4850]]])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n    output = gpt(torch.tensor([original_token_ids]))\nprint(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d80717e-50e6-4927-8129-0aadfa2628f5",
      "metadata": {},
      "source": [
        "- 接下来, 让我们 try 它 on 这个 updated tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "75f11ec9-bdd2-440f-b8c8-6646b75891c6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.2267,  0.9132,  1.0494,  ..., -0.2330, -0.3008, -1.1458],\n",
            "         [ 0.6808, -0.0495,  0.8574,  ...,  0.0671,  0.5572, -0.7873],\n",
            "         [-0.1947,  0.1045,  0.2773,  ...,  1.3368,  0.8479, -0.9660],\n",
            "         ...,\n",
            "         [-0.0656, -1.2451,  0.7957,  ..., -1.2124,  0.1044,  0.5088],\n",
            "         [-1.1561, -0.7380, -0.0645,  ..., -0.4373,  1.1401, -0.3903],\n",
            "         [-0.8961, -0.6437, -0.1667,  ...,  0.5663, -0.5862, -0.4020]]])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n    output = gpt(torch.tensor([new_token_ids]))\nprint(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d88a1bba-db01-4090-97e4-25dfc23ed54c",
      "metadata": {},
      "source": [
        "- As 我们 can see, 这个 模型 works on 这个 extended 词元 设置\n",
        "- In practice, 我们 want to 现在 finetune (或者 continually pretrain) 这个 模型 (specifically 这个 new 嵌入 和 输出 layers) on data containing 这个 new tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de573ad-0338-40d9-9dad-de60ae349c4f",
      "metadata": {},
      "source": [
        "**一个 note about 权重 tying**\n",
        "\n",
        "- 如果 这个 模型 uses 权重 tying, 哪个 means 那个 这个 嵌入 层 和 输出 层 share 这个 same weights, similar to Llama 3 [link], updating 这个 输出 层 is much simpler\n",
        "- In 这个 case, 我们 can simply copy over 这个 weights from 这个 嵌入 层:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4cbc5f51-c7a8-49d0-b87f-d3d87510953b",
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt.out_head.weight = gpt.tok_emb.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d0d553a8-edff-40f0-bdc4-dff900e16caf",
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n    output = gpt(torch.tensor([new_token_ids]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}