{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ba450fb1-8a26-4894-ab7a-5d7bfefe90ce",
      "metadata": {},
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary 代码 for 这个 <一个 href=\"http://mng.bz/orYv\">构建 一个 大语言模型 From Scratch</一个> book by <一个 href=\"https://sebastianraschka.com\">Sebastian Raschka</一个><br>\n",
        "<br>代码 repository: <一个 href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</一个>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<一个 href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></一个>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
      "metadata": {},
      "source": [
        "# 第 5 练习 解答"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "37aa4692-2357-4d88-b072-6d2d988d7f4f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numpy version: 1.26.4\n",
            "tiktoken version: 0.7.0\n",
            "torch version: 2.4.0\n",
            "tensorflow version: 2.16.1\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n\npkgs = [\"numpy\", \n        \"tiktoken\", \n        \"torch\",\n        \"tensorflow\" # For OpenAI's pretrained weights\n       ]\nfor p in pkgs:\n    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
      "metadata": {},
      "source": [
        "# 练习 5.1: Temperature-scaled softmax scores 和 sampling probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5860ba9f-2db3-4480-b96b-4be1c68981eb",
      "metadata": {},
      "source": [
        "- 我们 can 打印 这个 number of times 这个 word \"pizza\" is sampled using 这个 `print_sampled_tokens` 函数 我们 defined in 这个 section\n",
        "- 让我们 开始 with 这个 代码 我们 defined in section 5.3.1\n",
        "\n",
        "- 它 is sampled 0x 如果 这个 temperature is 0 或者 0.1, 和 它 is sampled 32x 如果 这个 temperature is scaled up to 5. 这个 estimated probability is 32/1000 * 100% = 3.2%\n",
        "\n",
        "- 这个 actual probability is 4.3% 和 contained in 这个 rescaled softmax probability tensor (`scaled_probas[2][6]`)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cba59c2-a8a3-4af3-add4-70230795225e",
      "metadata": {},
      "source": [
        "- Below is 一个 self-contained 示例 using 代码 from 第 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "42dda298-3014-4c36-8d63-97c210bcf4e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n\nvocab = { \n    \"closer\": 0,\n    \"every\": 1, \n    \"effort\": 2, \n    \"forward\": 3,\n    \"inches\": 4,\n    \"moves\": 5, \n    \"pizza\": 6,\n    \"toward\": 7,\n    \"you\": 8,\n} \ninverse_vocab = {v: k for k, v in vocab.items()}\n\nnext_token_logits = torch.tensor(\n    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n)\n\ndef print_sampled_tokens(probas):\n    torch.manual_seed(123)\n    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n    sampled_ids = torch.bincount(torch.tensor(sample))\n    for i, freq in enumerate(sampled_ids):\n        print(f\"{freq} x {inverse_vocab[i]}\")\n\n\ndef softmax_with_temperature(logits, temperature):\n    scaled_logits = logits / temperature\n    return torch.softmax(scaled_logits, dim=0)\n\n\ntemperatures = [1, 0.1, 5]  # Original, higher, 和 lower temperature\nscaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ee0f9f3-4132-42c7-8324-252fd8f59145",
      "metadata": {},
      "source": [
        "- 现在, 我们 can iterate over 这个 `scaled_probas` 和 打印 这个 sampling frequencies in each case:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b5605236-e300-4844-aea7-509d868efbdd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Temperature: 1\n",
            "73 x closer\n",
            "0 x every\n",
            "0 x effort\n",
            "582 x forward\n",
            "2 x inches\n",
            "0 x moves\n",
            "0 x pizza\n",
            "343 x toward\n",
            "\n",
            "\n",
            "Temperature: 0.1\n",
            "0 x closer\n",
            "0 x every\n",
            "0 x effort\n",
            "985 x forward\n",
            "0 x inches\n",
            "0 x moves\n",
            "0 x pizza\n",
            "15 x toward\n",
            "\n",
            "\n",
            "Temperature: 5\n",
            "165 x closer\n",
            "75 x every\n",
            "42 x effort\n",
            "239 x forward\n",
            "71 x inches\n",
            "46 x moves\n",
            "32 x pizza\n",
            "227 x toward\n",
            "103 x you\n"
          ]
        }
      ],
      "source": [
        "for i, probas in enumerate(scaled_probas):\n    print(\"\\n\\nTemperature:\", temperatures[i])\n    print_sampled_tokens(probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbf88c97-19c4-462c-924a-411c8c765d2c",
      "metadata": {},
      "source": [
        "- Note 那个 sampling offers 一个 approximation of 这个 actual probabilities 当 这个 word \"pizza\" is sampled\n",
        "- E.g., 如果 它 is sampled 32/1000 times, 这个 estimated probability is 3.2%\n",
        "- To obtain 这个 actual probability, 我们 can 检查 这个 probabilities directly by accessing 这个 corresponding entry in `scaled_probas`\n",
        "\n",
        "- Since \"pizza\" is 这个 7th entry in 这个 vocabulary, for 这个 temperature of 5, 我们 obtain 它 as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1d4163c0-22ad-4f5b-8e20-b7420e9dbfc6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.0430)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp5_idx = 2\npizza_idx = 6\n\nscaled_probas[temp5_idx][pizza_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3dcb438-5f18-4332-9627-66009f30a1a4",
      "metadata": {},
      "source": [
        "那里 is 一个 4.3% probability 那个 这个 word \"pizza\" is sampled 如果 这个 temperature is 设置 to 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b510ffb0-adca-4d64-8a12-38c4646fd736",
      "metadata": {},
      "source": [
        "# 练习 5.2: Different temperature 和 top-k settings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "884990db-d1a6-4c4e-8e36-2c1e4c1e67c7",
      "metadata": {},
      "source": [
        "- Both temperature 和 top-k settings have to be adjusted based on 这个 individual 大语言模型 (一个 kind of trial 和 error 处理 until 它 generates desirable outputs)\n",
        "- 这个 desirable outcomes are also application-specific, though\n",
        "  - Lower top-k 和 temperatures result in less random outcomes, 哪个 is desired 当 creating educational content, technical writing 或者 question answering, data analyses, 代码 generation, 和 so forth\n",
        "  - Higher top-k 和 temperatures result in more diverse 和 random outputs, 哪个 is more desirable for brainstorming tasks, creative writing, 和 so forth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f35425d-529d-4179-a1c4-63cb8b25b156",
      "metadata": {},
      "source": [
        "# 练习 5.3: Deterministic behavior in 这个 decoding functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d12229a2-1d52-46ff-b1e8-198f2e58a7d2",
      "metadata": {},
      "source": [
        "那里 are multiple ways to force deterministic behavior with 这个 `生成` 函数:\n",
        "\n",
        "1. Setting to `temperature=0.0`;\n",
        "2. Setting `top_k=1`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391c5dc8-8dd7-4a0a-90bd-519b72f528c7",
      "metadata": {},
      "source": [
        "Below is 一个 self-contained 示例 using 代码 from 第 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a61a4034-797a-4635-bf42-ddfff1b07125",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\nimport torch\nfrom previous_chapters import GPTModel\n\n\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,  # Vocabulary size\n    \"context_length\": 256,       # Shortened context length (orig: 1024)\n    \"emb_dim\": 768,       # 嵌入 dimension\n    \"n_heads\": 12,        # Number of 注意力机制 heads\n    \"n_layers\": 12,       # Number of layers\n    \"drop_rate\": 0.1,     # Dropout rate\n    \"qkv_bias\": False     # Query-key-value 偏置\n}\n\n\ntorch.manual_seed(123)\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(torch.load(\"model.pth\", weights_only=True))\nmodel.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ee95a272-b852-43b4-9827-ea7e1dbd5724",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpt_generate import generate, text_to_token_ids, token_ids_to_text\nfrom previous_chapters import generate_text_simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4ab43658-3240-484a-9072-a40a0ed85be6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
          ]
        }
      ],
      "source": [
        "# Deterministic 函数 那个 used torch.argmax\n\nstart_context = \"Every effort moves you\"\n\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=25,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ebb22d06-393a-42d3-ab64-66646d33b39b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
          ]
        }
      ],
      "source": [
        "# Deterministic behavior: No top_k, no temperature scaling\n\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n    max_new_tokens=25,\n    context_size=GPT_CONFIG_124M[\"context_length\"],\n    top_k=None,\n    temperature=0.0\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c85b1f11-37a5-477d-9c2d-170a6865e669",
      "metadata": {},
      "source": [
        "- Note 那个 re-executing 这个 previous 代码 cell will 产生 这个 exact same generated text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "75469f24-47cc-458d-a200-fe64c648131d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
          ]
        }
      ],
      "source": [
        "# Deterministic behavior: No top_k, no temperature scaling\n\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n    max_new_tokens=25,\n    context_size=GPT_CONFIG_124M[\"context_length\"],\n    top_k=None,\n    temperature=0.0\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d0480e5-fb4e-41f8-a161-7ac980d71d47",
      "metadata": {},
      "source": [
        "# 练习 5.4: Continued pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f40044e8-a0f5-476c-99fd-489b999fd80a",
      "metadata": {},
      "source": [
        "- 如果 我们 are still in 这个 Python session 哪里 你 首先 trained 这个 模型 in 第 5, to continue 这个 pretraining for one more 轮次, 我们 just have to 加载 这个 模型 和 优化器 那个 我们 saved in 这个 main 第 和 调用 这个 `train_model_simple` 函数 again\n",
        "\n",
        "- 它 takes 一个 couple more steps to make 这个 reproducible in 这个 new 代码 环境\n",
        "- 首先, 我们 加载 这个 分词器, 模型, 和 优化器:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "94eae6ba-d9fd-417a-8e31-fc39e9299870",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\nimport torch\nfrom previous_chapters import GPTModel\n\n\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,   # Vocabulary size\n    \"context_length\": 256, # Shortened context length (orig: 1024)\n    \"emb_dim\": 768,        # 嵌入 dimension\n    \"n_heads\": 12,         # Number of 注意力机制 heads\n    \"n_layers\": 12,        # Number of layers\n    \"drop_rate\": 0.1,      # Dropout rate\n    \"qkv_bias\": False      # Query-key-value 偏置\n}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\ncheckpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\nmodel.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\nmodel.train();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "688fce4a-9ab2-4d97-a95c-fef02c32b4f3",
      "metadata": {},
      "source": [
        "- 接下来, 我们 初始化 这个 数据加载器:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b5a78470-0652-4abd-875a-664e23c07c36",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport urllib.request\nfrom previous_chapters import create_dataloader_v1\n\n\nfile_path = \"the-verdict.txt\"\nurl = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n\nif not os.path.exists(file_path):\n    with urllib.request.urlopen(url) as response:\n        text_data = response.read().decode('utf-8')\n    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(text_data)\nelse:\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        text_data = file.read()\n\n\n# Train/验证 ratio\ntrain_ratio = 0.90\nsplit_idx = int(train_ratio * len(text_data))\ntrain_data = text_data[:split_idx]\nval_data = text_data[split_idx:]\n\n\ntorch.manual_seed(123)\n\ntrain_loader = create_dataloader_v1(\n    train_data,\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=True,\n    shuffle=True,\n    num_workers=0\n)\n\nval_loader = create_dataloader_v1(\n    val_data,\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=False,\n    shuffle=False,\n    num_workers=0\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76598ef8-165c-4bcc-af5e-b6fe72398365",
      "metadata": {},
      "source": [
        "- Lastly, 我们 使用 这个 `train_model_simple` 函数 to train 这个 模型:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ab4693dc-1359-47a7-8110-1e90f514a49e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 0.271, Val loss 6.545\n",
            "Ep 1 (Step 000005): Train loss 0.244, Val loss 6.614\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
          ]
        }
      ],
      "source": [
        "from gpt_train import train_model_simple\n\nnum_epochs = 1\ntrain_losses, val_losses, tokens_seen = train_model_simple(\n    model, train_loader, val_loader, optimizer, device,\n    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n    start_context=\"Every effort moves you\", tokenizer=tokenizer\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3384e788-f5a1-407c-8dd1-87959b75026d",
      "metadata": {},
      "source": [
        "# 练习 5.5: 训练 和 验证 设置 losses of 这个 pretrained 模型"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cb1140b-2027-4156-8d19-600ac849edbe",
      "metadata": {},
      "source": [
        "- 我们 can 使用 这个 following 代码 to 计算 这个 训练 和 验证 设置 losses of 这个 GPT 模型:\n",
        "\n",
        "```python\n",
        "train_loss = calc_loss_loader(train_loader, GPT, device)\n",
        "val_loss = calc_loss_loader(val_loader, GPT, device)\n",
        "```\n",
        "\n",
        "- 这个 resulting losses for 这个 124M 参数 are as follows:\n",
        "\n",
        "```\n",
        "训练 loss: 3.754748503367106\n",
        "验证 loss: 3.559617757797241\n",
        "```\n",
        "\n",
        "- 这个 main observation is 那个 这个 训练 和 验证 设置 performances are in 这个 same ballpark\n",
        "- 这个 can have multiple explanations:\n",
        "\n",
        "1. 这个 Verdict was not part of 这个 pretraining 数据集 当 OpenAI trained GPT-2. Hence, 这个 模型 is not explicitly overfitting to 这个 训练 设置 和 performs similarly well on 这个 Verdict's 训练 和 验证 设置 portions. (这个 验证 设置 loss is slightly lower than 这个 训练 设置 loss, 哪个 is unusual in 深度学习. However, 它's likely due to random noise since 这个 数据集 is relatively small. In practice, 如果 那里 is no overfitting, 这个 训练 和 验证 设置 performances are expected to be roughly identical).\n",
        "\n",
        "2. 这个 Verdict was part of GPT -2's 训练 数据集. In 这个 case, 我们 can't tell whether 这个 模型 is overfitting 这个 训练 data because 这个 验证 设置 would have been used for 训练 as well. To evaluate 这个 degree of overfitting, 我们'd need 一个 new 数据集 generated after OpenAI finished 训练 GPT-2 to make sure 那个 它 couldn't have been part of 这个 pretraining."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66bb4316-a57c-437f-9a01-fe99b1678524",
      "metadata": {},
      "source": [
        "这个 代码 below is 一个 reproducible standalone 示例 for 这个 new 笔记本."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "68d162d6-bbb9-4d6d-82ee-1c410694f872",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\nimport torch\nfrom previous_chapters import GPTModel\n\n\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,   # Vocabulary size\n    \"context_length\": 256, # Shortened context length (orig: 1024)\n    \"emb_dim\": 768,        # 嵌入 dimension\n    \"n_heads\": 12,         # Number of 注意力机制 heads\n    \"n_layers\": 12,        # Number of layers\n    \"drop_rate\": 0.1,      # Dropout rate\n    \"qkv_bias\": False      # Query-key-value 偏置\n}\n\n\ntorch.manual_seed(123)\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d8373461-7dad-47da-a489-3e23f0799b23",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "from gpt_download import download_and_load_gpt2\n\nsettings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "cdd44873-d6c2-4471-a20f-f639b09fdcd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 定义 模型 configurations in 一个 dictionary for compactness\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\n# Copy 这个 base 配置 和 更新 with specific 模型 settings\nmodel_name = \"gpt2-small (124M)\"  # 示例 模型 name\nNEW_CONFIG = GPT_CONFIG_124M.copy()\nNEW_CONFIG.update(model_configs[model_name])\nNEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n\ngpt = GPTModel(NEW_CONFIG)\ngpt.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c7d562e4-33f6-4611-9b75-6ad1cb441d3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpt_generate import load_weights_into_gpt\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nload_weights_into_gpt(gpt, params)\ngpt.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "46eda9ea-ccb0-46ee-931b-3c07502b2544",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport urllib.request\nfrom previous_chapters import create_dataloader_v1\n\n\nfile_path = \"the-verdict.txt\"\nurl = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n\nif not os.path.exists(file_path):\n    with urllib.request.urlopen(url) as response:\n        text_data = response.read().decode('utf-8')\n    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(text_data)\nelse:\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        text_data = file.read()\n\n\n# Train/验证 ratio\ntrain_ratio = 0.90\nsplit_idx = int(train_ratio * len(text_data))\ntrain_data = text_data[:split_idx]\nval_data = text_data[split_idx:]\n\n\ntorch.manual_seed(123)\n\ntrain_loader = create_dataloader_v1(\n    train_data,\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=True,\n    shuffle=True,\n    num_workers=0\n)\n\nval_loader = create_dataloader_v1(\n    val_data,\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=False,\n    shuffle=False,\n    num_workers=0\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4e3574a2-687d-47a2-a2f6-457fe9d595f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 3.7547486888037787\n",
            "Validation loss: 3.5596182346343994\n"
          ]
        }
      ],
      "source": [
        "from gpt_train import calc_loss_loader\n\ntorch.manual_seed(123) # For reproducibility due to 这个 shuffling in 这个 数据加载器\ntrain_loss = calc_loss_loader(train_loader, gpt, device)\nval_loss = calc_loss_loader(val_loader, gpt, device)\n\nprint(\"Training loss:\", train_loss)\nprint(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96485d6b-bf1f-4bc0-a53f-73b08d85726e",
      "metadata": {},
      "source": [
        "我们 can also repeat 这个 for 这个 largest GPT-2 模型, 但是 don't forget to 更新 这个 context length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1a79a4b6-fe8f-40c2-a018-e731dcf391b3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "checkpoint: 100%|███████████████████████████| 77.0/77.0 [00:00<00:00, 43.5kiB/s]\n",
            "encoder.json: 100%|███████████████████████| 1.04M/1.04M [00:00<00:00, 2.75MiB/s]\n",
            "hparams.json: 100%|█████████████████████████| 91.0/91.0 [00:00<00:00, 60.2kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|█████| 6.23G/6.23G [06:02<00:00, 17.2MiB/s]\n",
            "model.ckpt.index: 100%|████████████████████| 20.7k/20.7k [00:00<00:00, 171kiB/s]\n",
            "model.ckpt.meta: 100%|████████████████████| 1.84M/1.84M [00:00<00:00, 4.27MiB/s]\n",
            "vocab.bpe: 100%|████████████████████████████| 456k/456k [00:00<00:00, 1.73MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 3.3046312861972384\n",
            "Validation loss: 3.1195147037506104\n"
          ]
        }
      ],
      "source": [
        "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n\nmodel_name = \"gpt2-xl (1558M)\"\nNEW_CONFIG = GPT_CONFIG_124M.copy()\nNEW_CONFIG.update(model_configs[model_name])\nNEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n\ngpt = GPTModel(NEW_CONFIG)\ngpt.eval()\n\nload_weights_into_gpt(gpt, params)\ngpt.to(device)\n\ntorch.manual_seed(123)\ntrain_loss = calc_loss_loader(train_loader, gpt, device)\nval_loss = calc_loss_loader(val_loader, gpt, device)\n\nprint(\"Training loss:\", train_loss)\nprint(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a76a1e0-9635-480a-9391-3bda7aea402d",
      "metadata": {},
      "source": [
        "# 练习 5.6: Trying larger models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3d313f4-0038-4bc9-a340-84b3b55dc0e3",
      "metadata": {},
      "source": [
        "- In 这个 main 第, 我们 experimented with 这个 smallest GPT-2 模型, 哪个 has only 124M parameters\n",
        "- 这个 reason was to keep 这个 resource 依赖 as low as possible\n",
        "- However, 你 can easily experiment with larger models with minimal 代码 changes\n",
        "- For 示例, instead of loading 这个 1558M instead of 124M 模型 in 第 5, 这个 only 2 lines of 代码 那个 我们 have to 改变 are\n",
        "\n",
        "```python\n",
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
        "model_name = \"gpt2-small (124M)\"\n",
        "```\n",
        "\n",
        "- 这个 updated 代码 becomes\n",
        "\n",
        "\n",
        "```python\n",
        "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
        "model_name = \"gpt2-xl (1558M)\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "31e0972b-e85e-4904-a0f5-24c3eacd5fa2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\nimport torch\nfrom previous_chapters import GPTModel\n\n\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,   # Vocabulary size\n    \"context_length\": 256, # Shortened context length (orig: 1024)\n    \"emb_dim\": 768,        # 嵌入 dimension\n    \"n_heads\": 12,         # Number of 注意力机制 heads\n    \"n_layers\": 12,        # Number of layers\n    \"drop_rate\": 0.1,      # Dropout rate\n    \"qkv_bias\": False      # Query-key-value 偏置\n}\n\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b641ee88-f9d4-43ec-a787-e34199eed356",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File already exists and is up-to-date: gpt2/1558M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/1558M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/1558M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/1558M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/1558M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/1558M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/1558M/vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "from gpt_download import download_and_load_gpt2\nfrom gpt_generate import load_weights_into_gpt\n\n\nmodel_configs = {\n    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n}\n\nmodel_name = \"gpt2-xl (1558M)\"\nNEW_CONFIG = GPT_CONFIG_124M.copy()\nNEW_CONFIG.update(model_configs[model_name])\nNEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n\ngpt = GPTModel(NEW_CONFIG)\ngpt.eval()\n\nsettings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\nload_weights_into_gpt(gpt, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c98f56f4-98fc-43b4-9ee5-726e9d17c73f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpt_generate import generate, text_to_token_ids, token_ids_to_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b1f7853c-6e81-4f1f-a1d0-61e2c7d33a20",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you toward finding an ideal life. You don't have to accept your current one at once, because if you do you'll never\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n\ntoken_ids = generate(\n    model=gpt,\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n    max_new_tokens=25,\n    context_size=NEW_CONFIG[\"context_length\"],\n    top_k=50,\n    temperature=1.5\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}