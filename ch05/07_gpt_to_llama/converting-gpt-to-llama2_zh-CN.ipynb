{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0_xya1nyDHfY",
      "metadata": {
        "id": "0_xya1nyDHfY"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary 代码 for 这个 <一个 href=\"http://mng.bz/orYv\">构建 一个 大语言模型 From Scratch</一个> book by <一个 href=\"https://sebastianraschka.com\">Sebastian Raschka</一个><br>\n",
        "<br>代码 repository: <一个 href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</一个>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<一个 href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></一个>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l62zIRRSBy_R",
      "metadata": {
        "id": "l62zIRRSBy_R"
      },
      "source": [
        "# Converting 一个 From-Scratch GPT Architecture to Llama 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aFmxTQbwCUMl",
      "metadata": {
        "id": "aFmxTQbwCUMl"
      },
      "source": [
        "- In 这个 笔记本, 我们 转换 这个 original GPT architecture into 一个 Llama 2 模型 step by step (note 这个 GPT 和 GPT-2 share 这个 same architecture)\n",
        "- 为什么 not Llama 1 或者 Llama 3?\n",
        "   - 这个 Llama 1 architecture is similar to Llama 2, except 那个 Llama 2 has 一个 larger context window (哪个 is nice); 这个 Llama 1 weights are not readily available 和 have more usage restrictions, so 它 makes more sense to focus on Llama 2\n",
        "   - Regarding Llama 3, I will share 一个 separate 笔记本 to 转换 Llama 2 to Llama 3 (那里 are only 一个 few small additional changes)\n",
        "- 这个 explanations are purposefully kept minimal in 这个 笔记本 not to bloat 它 unnecessarily 和 focus on 这个 main 代码\n",
        "- For more information, please see 这个 Llama 2 paper: [Llama 2: Open Foundation 和 Fine-Tuned Chat Models (2023)](https://arxiv.org/abs/2307.09288)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ohhMKUWvGm9z",
      "metadata": {
        "id": "ohhMKUWvGm9z"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/GPT-to-llama/gpt2-to-llama2-llama3.webp?1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JBpQwU89ETA1",
      "metadata": {
        "id": "JBpQwU89ETA1"
      },
      "source": [
        "- Packages 那个 are being used in 这个 笔记本:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "34a9a440-84c2-42cc-808b-38677cb6af8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34a9a440-84c2-42cc-808b-38677cb6af8a",
        "outputId": "8118963b-3c72-43af-874b-439ffebdc94c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface_hub version: 0.24.7\n",
            "sentencepiece version: 0.2.0\n",
            "torch version: 2.4.1+cu121\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n\npkgs = [\n    \"huggingface_hub\",  # to download pretrained weights\n    \"sentencepiece\",    # to 实现 这个 分词器\n    \"torch\",            # to 实现 这个 模型\n]\nfor p in pkgs:\n    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UJJneXpTEg4W",
      "metadata": {
        "id": "UJJneXpTEg4W"
      },
      "source": [
        "&nbsp;\n",
        "# 1. 转换 这个 GPT 模型 实现 step by step"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v1zpfX2GHBKa",
      "metadata": {
        "id": "v1zpfX2GHBKa"
      },
      "source": [
        "- In 这个 section, 我们 go through 这个 GPT 模型 代码 from [第 4](../../ch04/01_main-第-代码/ch04.ipynb) 和 修改 它 step by step to 实现 这个 Llama 2 architecture\n",
        "- Later, 我们 加载 这个 original Llama 2 weights shared by Meta AI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "979c7b6d-1370-4da1-8bfb-a2b27537bf2f",
      "metadata": {
        "id": "979c7b6d-1370-4da1-8bfb-a2b27537bf2f"
      },
      "source": [
        "&nbsp;\n",
        "## 1.1 Replace LayerNorm with RMSNorm 层"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b27fc8-23a1-4e0e-a1ea-792e0428e5e6",
      "metadata": {
        "id": "f8b27fc8-23a1-4e0e-a1ea-792e0428e5e6"
      },
      "source": [
        "- 首先, 我们 replace LayerNorm by Root Mean Square 层 归一化 (RMSNorm)\n",
        "- LayerNorm normalizes inputs using mean 和 variance, while RMSNorm uses only 这个 root mean square, 哪个 improves computational efficiency\n",
        "- 这个 RMSNorm operation is as follows, 哪里 $x$ is 这个 输入 $\\gamma$ is 一个 trainable 参数 (vector), 和 $\\epsilon$ is 一个 small constant to avoid zero-division errors:\n",
        "\n",
        "$$y_i = \\frac{x_i}{\\text{RMS}(x)} \\gamma_i, \\quad \\text{哪里} \\quad \\text{RMS}(x) = \\sqrt{\\epsilon + \\frac{1}{n} \\sum x_i^2}$$\n",
        "\n",
        "- For more details, please see 这个 paper [Root Mean Square 层 归一化 (2019)](https://arxiv.org/abs/1910.07467)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d7094381-9499-4e9e-93f9-b79470da3771",
      "metadata": {
        "id": "d7094381-9499-4e9e-93f9-b79470da3771"
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\n\n\n#####################################\n# 第 4\n#####################################\n\n# 类 LayerNorm(nn.模块):\n#     def __init__(self, emb_dim):\n#         super().__init__()\n#         self.eps = 1e-5\n#         self.scale = nn.参数(torch.ones(emb_dim))\n#         self.shift = nn.参数(torch.zeros(emb_dim))\n\n#     def forward(self, x):\n#         mean = x.mean(dim=-1, keepdim=True)\n#         var = x.var(dim=-1, keepdim=True, unbiased=False)\n#         norm_x = (x - mean) / torch.sqrt(var + self.eps)\n#         返回 self.scale * norm_x + self.shift\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, emb_dim, eps=1e-5):\n        super().__init__()\n        self.eps = eps\n        self.emb_dim = emb_dim\n        self.weight = nn.Parameter(torch.ones(emb_dim)).float()\n\n    def forward(self, x):\n        means = x.pow(2).mean(dim=-1, keepdim=True)\n        x_normed = x * torch.rsqrt(means + self.eps)\n        return (x_normed * self.weight).to(dtype=x.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mtWC8DOmIu0F",
      "metadata": {
        "id": "mtWC8DOmIu0F"
      },
      "source": [
        "- 这个 following 代码 cell checks 那个 这个 实现 works 这个 same as PyTorch's built-in 实现:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e41ade7a-bf06-48b1-8b7e-0e4037d5753f",
      "metadata": {
        "id": "e41ade7a-bf06-48b1-8b7e-0e4037d5753f"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n\nexample_batch = torch.randn(2, 3, 4)\n\nrms_norm = RMSNorm(emb_dim=example_batch.shape[-1])\nrmsnorm_pytorch = torch.nn.RMSNorm(example_batch.shape[-1], eps=1e-5)\n\nassert torch.allclose(rms_norm(example_batch), rmsnorm_pytorch(example_batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eb81f83-c38c-46a4-b763-aa630a32e357",
      "metadata": {
        "id": "5eb81f83-c38c-46a4-b763-aa630a32e357"
      },
      "source": [
        "&nbsp;\n",
        "## 1.2 Replace GELU with SiLU activation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b8aa702-f118-4ff6-9135-90725ec8756c",
      "metadata": {
        "id": "0b8aa702-f118-4ff6-9135-90725ec8756c"
      },
      "source": [
        "- Llama uses 这个 SiLU 激活函数 (instead of GELU), 哪个 is also known as 这个 Swish 函数:\n",
        "\n",
        "$$\n",
        "\\text{silu}(x) = x \\cdot \\sigma(x), \\quad \\text{哪里} \\quad \\sigma(x) \\text{ is 这个 logistic sigmoid.}\n",
        "$$\n",
        "\n",
        "- For more information, see 这个 SiLU paper: [Sigmoid-Weighted Linear Units for 神经网络 函数 Approximation in Reinforcement Learning (2017)](https://arxiv.org/abs/1702.03118)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a74f3757-c634-4a3a-a8f3-6334cde454fe",
      "metadata": {
        "id": "a74f3757-c634-4a3a-a8f3-6334cde454fe"
      },
      "outputs": [],
      "source": [
        "#####################################\n# 第 4\n#####################################\n\n# 类 GELU(nn.模块):\n#     def __init__(self):\n#         super().__init__()\n\n#     def forward(self, x):\n#         返回 0.5 * x * (1 + torch.tanh(\n#             torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n#             (x + 0.044715 * torch.pow(x, 3))\n#         ))\n\n\nclass SiLU(nn.Module):\n    def __init__(self):\n        super(SiLU, self).__init__()\n\n    def forward(self, x):\n        return x * torch.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "72ecbe2e-b6b7-4319-972b-1a7fefa3368c",
      "metadata": {
        "id": "72ecbe2e-b6b7-4319-972b-1a7fefa3368c"
      },
      "outputs": [],
      "source": [
        "silu = SiLU()\n\nassert torch.allclose(silu(example_batch), torch.nn.functional.silu(example_batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f9b5167-1da9-46c8-9964-8036b3b1deb9",
      "metadata": {
        "id": "4f9b5167-1da9-46c8-9964-8036b3b1deb9"
      },
      "source": [
        "&nbsp;\n",
        "## 1.3 更新 这个 FeedForward 模块"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a381e7a-b807-472e-91c9-3e4e3fc5ad91",
      "metadata": {
        "id": "3a381e7a-b807-472e-91c9-3e4e3fc5ad91"
      },
      "source": [
        "- In fact, Llama uses 一个 \"Gates Linear Unit\" (GLU) variant of SiLU called SwiGLU, 哪个 essentially results in 一个 slightly differently structured `FeedForward` 模块\n",
        "- SwiGLU uses 一个 gating mechanism in 这个 feedforward 层, with 这个 formula:\n",
        "\n",
        "$$\\text{SwiGLU}(x) = \\text{SiLU}(\\text{Linear}_1(x)) * (\\text{Linear}_2(x))$$\n",
        "\n",
        "- 这里, $\\text{Linear}_1$ 和 $\\text{Linear}_2$ are two linear layers, 和 $*$ denotes element-wise multiplication\n",
        "- 这个 third linear 层, $\\text{Linear}_3$, is applied after 这个 gated activation\n",
        "\n",
        "- For more information, see SwiGLU paper: [GLU Variants Improve Transformer (2020)](https://arxiv.org/abs/2002.05202)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d25fbe3d-b7c9-4772-ad67-bc0527e4e20a",
      "metadata": {
        "id": "d25fbe3d-b7c9-4772-ad67-bc0527e4e20a"
      },
      "outputs": [],
      "source": [
        "#####################################\n# 第 4\n#####################################\n# 类 FeedForward(nn.模块):\n#     def __init__(self, cfg):\n#         super().__init__()\n#         self.layers = nn.Sequential(\n#             nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n#             GELU(),\n#             nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n#         )\n\n#     def forward(self, x):\n#         返回 self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "477568cb-03cd-4510-b663-a42ce3ec64a2",
      "metadata": {
        "id": "477568cb-03cd-4510-b663-a42ce3ec64a2"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n        self.silu = SiLU()\n\n    def forward(self, x):\n        x_fc1 = self.fc1(x)\n        x_fc2 = self.fc2(x)\n        x = self.silu(x_fc1) * x_fc2\n        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qcD8LSHNhBRW",
      "metadata": {
        "id": "qcD8LSHNhBRW"
      },
      "source": [
        "- Note 那个 我们 also added 一个 `dtype=cfg[\"dtype\"]` setting above, 哪个 will allow us to 加载 这个 模型 directly in lower precision formats later to reduce memory usage (versus instantiating 它 in 这个 original 32-bit precision format 和 然后 converting 它)\n",
        "- 我们 also 设置 `偏置=False` since Llama doesn't 使用 any 偏置 units"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6b7bf4f-99d0-42c1-807c-5074d2cc1949",
      "metadata": {
        "id": "f6b7bf4f-99d0-42c1-807c-5074d2cc1949"
      },
      "source": [
        "&nbsp;\n",
        "## 1.4 实现 RoPE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3487a6f-0373-49d8-b2eb-f8ee05d42884",
      "metadata": {
        "id": "d3487a6f-0373-49d8-b2eb-f8ee05d42884"
      },
      "source": [
        "- In 这个 GPT 模型, 这个 positional embeddings are implemented as follows:\n",
        "\n",
        "```python\n",
        "self.pos_emb = nn.嵌入(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "```\n",
        "\n",
        "- Unlike traditional absolute positional embeddings, Llama uses rotary position embeddings (RoPE), 哪个 enable 它 to capture both absolute 和 relative positional information simultaneously\n",
        "- 这个 reference paper for RoPE is [RoFormer: Enhanced Transformer with Rotary Position 嵌入 (2021)](https://arxiv.org/abs/2104.09864)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a34180fb-448f-44e9-a244-0c736051687b",
      "metadata": {
        "id": "a34180fb-448f-44e9-a244-0c736051687b"
      },
      "outputs": [],
      "source": [
        "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096):\n    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n\n    # 计算 这个 inverse frequencies\n    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n\n    # 生成 position indices\n    positions = torch.arange(context_length)\n\n    # 计算 这个 angles\n    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n\n    # Expand angles to match 这个 head_dim\n    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n\n    # Precompute sine 和 cosine\n    cos = torch.cos(angles)\n    sin = torch.sin(angles)\n\n    return cos, sin\n\ndef compute_rope(x, cos, sin):\n    # x: (batch_size, num_heads, seq_len, head_dim)\n    batch_size, num_heads, seq_len, head_dim = x.shape\n    assert head_dim % 2 == 0, \"Head dimension must be even\"\n\n    # Split x into 首先 half 和 second half\n    x1 = x[..., : head_dim // 2]  # 首先 half\n    x2 = x[..., head_dim // 2 :]  # Second half\n\n    # Adjust sin 和 cos shapes\n    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n\n    # 应用 这个 rotary transformation\n    rotated = torch.cat((-x2, x1), dim=-1)\n    x_rotated = (x * cos) + (rotated * sin)\n\n    return x_rotated.to(dtype=x.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e841b8e-75aa-49db-b1a7-d5c2116dc299",
      "metadata": {
        "id": "8e841b8e-75aa-49db-b1a7-d5c2116dc299"
      },
      "source": [
        "- 这个 following is 一个 示例 of applying RoPE to 这个 `q` 和 `k` tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8c89f022-7167-4001-8c21-8e012878733f",
      "metadata": {
        "id": "8c89f022-7167-4001-8c21-8e012878733f"
      },
      "outputs": [],
      "source": [
        "# Settings\nbatch_size = 2\ncontext_len = 5\nnum_heads = 4\nhead_dim = 16\n\n# Instantiate RoPE parameters\ncos, sin = precompute_rope_params(head_dim=head_dim, context_length=context_len)\n\n# Dummy query 和 key tensors\ntorch.manual_seed(123)\nqueries = torch.randn(batch_size, num_heads, context_len, head_dim)\nkeys = torch.randn(batch_size, num_heads, context_len, head_dim)\n\n# 应用 rotary position embeddings\nqueries_rot = compute_rope(queries, cos, sin)\nkeys_rot = compute_rope(keys, cos, sin)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f78127b0-dda2-4c5a-98dd-bae8f5fe8297",
      "metadata": {
        "id": "f78127b0-dda2-4c5a-98dd-bae8f5fe8297"
      },
      "source": [
        "&nbsp;\n",
        "## 1.5 添加 RoPE to MultiHeadAttention 模块"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RnmSHROLhhR3",
      "metadata": {
        "id": "RnmSHROLhhR3"
      },
      "source": [
        "- 它's important to note 那个 GPT applies 这个 positional embeddings to 这个 inputs, whereas Llama applies rotations to 这个 query 和 key vectors in 这个 self-注意力机制 mechanism itself\n",
        "- 这里, 我们 修改 这个 `MultiHeadAttention` 类 with 这个 appropriate RoPE 代码\n",
        "- In addition, 我们 移除 这个 `qkv_bias` option 和 hardcode 这个 `偏置=False` setting\n",
        "- Also, 我们 添加 一个 dtype setting to be able to instantiate 这个 模型 with 一个 lower precision later\n",
        " - Tip: since 这个 `TransformerBlock`s (in 这个 接下来 section) are repeated exactly, 我们 could simplify 这个 代码 和 only 初始化 这个 buffers once instead for each `MultiHeadAttention` 模块; however, 我们 添加 这个 precomputed RoPE parameters to 这个 `MultiHeadAttention` 类 so 那个 它 can 函数 as 一个 standalone 模块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d81a441e-0b79-4a8b-8291-ea7f55d58c84",
      "metadata": {
        "id": "d81a441e-0b79-4a8b-8291-ea7f55d58c84"
      },
      "outputs": [],
      "source": [
        "#####################################\n# 第 3\n#####################################\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, num_heads, dtype=None):  # ,dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads  # Reduce 这个 projection dim to match desired 输出 dim\n\n        ################################### NEW ###################################\n        # 设置 偏置=False 和 dtype=dtype for all linear layers below\n        ###########################################################################\n        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n        self.W_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n        self.W_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)  # Linear 层 to combine head outputs\n        # self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n\n        ################################### NEW ###################################\n        cos, sin = precompute_rope_params(head_dim=self.head_dim, context_length=context_length)\n        self.register_buffer(\"cos\", cos)\n        self.register_buffer(\"sin\", sin)\n        ###########################################################################\n\n\n    def forward(self, x):\n\n        b, num_tokens, d_in = x.shape\n\n        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # 我们 implicitly split 这个 matrix by adding 一个 `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        ################################### NEW ###################################\n        keys = compute_rope(keys, self.cos, self.sin)\n        queries = compute_rope(queries, self.cos, self.sin)\n        ###########################################################################\n\n        # 计算 scaled dot-product 注意力机制 (aka self-注意力机制) with 一个 causal mask\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to 这个 number of tokens 和 converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # 使用 这个 mask to fill 注意力机制 scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        # attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2)\n\n        # Combine heads, 哪里 self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec)  # optional projection\n\n        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-lt9SfnVioB3",
      "metadata": {
        "id": "-lt9SfnVioB3"
      },
      "source": [
        "- Below is 一个 示例 using 这个 `MultiHeadAttention` 模块 on 一个 示例 输入:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "03f15755-0083-483f-963b-99b599651638",
      "metadata": {
        "id": "03f15755-0083-483f-963b-99b599651638"
      },
      "outputs": [],
      "source": [
        "# Settings\nbatch_size = 1\ncontext_len = 100\nmax_context_len = 4096\nembed_dim = 128\nnum_heads = 4\n\n\nexample_batch = torch.randn((batch_size, context_len, embed_dim))\n\nmha = MultiHeadAttention(\n    d_in=embed_dim,\n    d_out=embed_dim,\n    context_length=max_context_len,\n    num_heads=num_heads\n)\n\nmha(example_batch)\n\ndel mha  # 删除 to free up memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a1a272-a038-4b8f-aaaa-f4b241e7f23f",
      "metadata": {
        "id": "e5a1a272-a038-4b8f-aaaa-f4b241e7f23f"
      },
      "source": [
        "&nbsp;\n",
        "## 1.6 更新 这个 TransformerBlock 模块"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "255f70ac-9c2e-4328-8af7-1c298b8d4a18",
      "metadata": {
        "id": "255f70ac-9c2e-4328-8af7-1c298b8d4a18"
      },
      "source": [
        "- At 这个 stage, most of 这个 hard work is already done; 我们 can 现在 更新 这个 `TransformerBlock` to 使用 这个 代码 我们 implemented above\n",
        "- 这个 means 我们\n",
        " - replace LayerNorm with RMSNorm\n",
        " - 移除 dropout\n",
        " - 移除 这个 `qkv_bias` setting\n",
        " - 添加 这个 `dtype` setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2e110721-bf2b-42b3-989a-1635b1658af0",
      "metadata": {
        "id": "2e110721-bf2b-42b3-989a-1635b1658af0"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention(\n            d_in=cfg[\"emb_dim\"],\n            d_out=cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            num_heads=cfg[\"n_heads\"],\n            dtype=cfg[\"dtype\"]  # NEW\n            # dropout=cfg[\"drop_rate\"],\n            # qkv_bias=cfg[\"qkv_bias\"]\n        )\n        self.ff = FeedForward(cfg)\n\n        ################################### NEW ###################################\n        # self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        # self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm1 = RMSNorm(cfg[\"emb_dim\"])\n        self.norm2 = RMSNorm(cfg[\"emb_dim\"])\n        ###########################################################################\n\n        # self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, x):\n        # Shortcut connection for 注意力机制 block\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n        # x = self.drop_shortcut(x)\n        x = x + shortcut  # 添加 这个 original 输入 back\n\n        # Shortcut connection for feed-forward block\n        shortcut = x\n        x = self.norm2(x)\n        x = self.ff(x)\n        # x = self.drop_shortcut(x)\n        x = x + shortcut  # 添加 这个 original 输入 back\n\n        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada953bc-e2c0-4432-a32d-3f7efa3f6e0f",
      "metadata": {
        "id": "ada953bc-e2c0-4432-a32d-3f7efa3f6e0f"
      },
      "source": [
        "&nbsp;\n",
        "## 1.7 更新 这个 模型 类"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba5d991a-559b-47be-96f4-31b881ab2da8",
      "metadata": {
        "id": "ba5d991a-559b-47be-96f4-31b881ab2da8"
      },
      "source": [
        "- As 你 may recall from [第 5](../01_main-第-代码/ch05.ipynb), 这个 `TransformerBlock` is 一个 repeated block within 这个 main 模型\n",
        "- Our Llama 模型 is almost 完成; 我们 just have to 更新 这个 模型 代码 surrounding 这个 `TransformerBlock`\n",
        "- 这个 means 我们\n",
        "  - 移除 absolute positional embeddings since 我们 have RoPE embeddings 现在\n",
        "  - replace LayerNorm with RMSNorm\n",
        "  - 移除 dropout\n",
        "  - 添加 这个 dtype setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cf8240fe-5d7f-4e7e-b1ac-e0755aab5e79",
      "metadata": {
        "id": "cf8240fe-5d7f-4e7e-b1ac-e0755aab5e79"
      },
      "outputs": [],
      "source": [
        "# 类 GPTModel(nn.模块):\nclass Llama2Model(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n        # self.pos_emb = nn.嵌入(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        # self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n\n        ################################### NEW ###################################\n        # self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n        ###########################################################################\n        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n\n    def forward(self, in_idx):\n        # batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        # pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n        x = tok_embeds  # + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n        # x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc94940-aaeb-45b9-9399-3a69b8043e60",
      "metadata": {
        "id": "4bc94940-aaeb-45b9-9399-3a69b8043e60"
      },
      "source": [
        "&nbsp;\n",
        "## 2. 初始化 模型"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bG--zY-Ljj1f",
      "metadata": {
        "id": "bG--zY-Ljj1f"
      },
      "source": [
        "- 这个 模型 代码 is 现在 完成, 和 我们 are ready to 初始化 它\n",
        "- In [第 5](../01_main-第-代码/ch05.ipynb), 我们 used 这个 following config file to specify 这个 124M-参数 GPT 模型:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4b7428df-3d02-4ccd-97b5-a629bdabbe8f",
      "metadata": {
        "id": "4b7428df-3d02-4ccd-97b5-a629bdabbe8f"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n    \"vocab_size\": 50257,     # Vocabulary size\n    \"context_length\": 1024,  # Context length\n    \"emb_dim\": 768,          # 嵌入 dimension\n    \"n_heads\": 12,           # Number of 注意力机制 heads\n    \"n_layers\": 12,          # Number of layers\n    \"drop_rate\": 0.1,        # Dropout rate\n    \"qkv_bias\": False        # Query-Key-Value 偏置\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bVi8uiBjw2T",
      "metadata": {
        "id": "8bVi8uiBjw2T"
      },
      "source": [
        "- For reference, 这个 1.5B 参数 GPT 模型 config is shown below as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "tAOojV_mkEnd",
      "metadata": {
        "id": "tAOojV_mkEnd"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_1558M = {\n    \"vocab_size\": 50257,     # Vocabulary size\n    \"context_length\": 1024,  # Context length\n    \"emb_dim\": 1600,         # 嵌入 dimension\n    \"n_heads\": 25,           # Number of 注意力机制 heads\n    \"n_layers\": 48,          # Number of layers\n    \"drop_rate\": 0.1,        # Dropout rate\n    \"qkv_bias\": False        # Query-Key-Value 偏置\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HoGGRAGykQTE",
      "metadata": {
        "id": "HoGGRAGykQTE"
      },
      "source": [
        "- Similarly, 我们 can 定义 一个 Llama 2 config file for 这个 7B 模型 (我们 ignore 这个 other larger models for simplicity 这里):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e0564727-2d35-4f0c-b0fc-cde1e9134a18",
      "metadata": {
        "id": "e0564727-2d35-4f0c-b0fc-cde1e9134a18"
      },
      "outputs": [],
      "source": [
        "LLAMA2_CONFIG_7B = {\n    \"vocab_size\": 32000,     # Vocabulary size\n    \"context_length\": 4096,  # Context length\n    \"emb_dim\": 4096,         # 嵌入 dimension\n    \"n_heads\": 32,           # Number of 注意力机制 heads\n    \"n_layers\": 32,          # Number of layers\n    \"hidden_dim\": 11008,     # NEW: Size of 这个 intermediate dimension in FeedForward\n    \"dtype\": torch.bfloat16  # NEW: Lower-precision dtype to reduce memory usage\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FAP7fiBzkaBz",
      "metadata": {
        "id": "FAP7fiBzkaBz"
      },
      "source": [
        "- Using these settings, 我们 can 现在 初始化 一个 Llama 2 7B 模型 (note 那个 这个 requires ~26 GB of memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7004d785-ac9a-4df5-8760-6807fc604686",
      "metadata": {
        "id": "7004d785-ac9a-4df5-8760-6807fc604686"
      },
      "outputs": [],
      "source": [
        "model = Llama2Model(LLAMA2_CONFIG_7B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6079f747-8f20-4c6b-8d38-7156f1101729",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6079f747-8f20-4c6b-8d38-7156f1101729",
        "outputId": "0a0eb34b-1a21-4c11-804f-b40007bda5a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 6,738,415,616\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total number of parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bx14NtzWk2wj",
      "metadata": {
        "id": "Bx14NtzWk2wj"
      },
      "source": [
        "- As shown above, 这个 模型 contains 6.7 billion parameters (commonly rounded 和 referred to as 一个 7B 模型)\n",
        "- Additionally, 我们 can 计算 这个 memory 依赖 for 这个 模型 using 这个 代码 below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0df1c79e-27a7-4b0f-ba4e-167fe107125a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0df1c79e-27a7-4b0f-ba4e-167fe107125a",
        "outputId": "11ced939-556d-4511-d5c0-10a94ed3df32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float32 (PyTorch default): 52.33 GB\n",
            "bfloat16: 26.17 GB\n"
          ]
        }
      ],
      "source": [
        "def model_memory_size(model, input_dtype=torch.float32):\n    total_params = 0\n    total_grads = 0\n    for param in model.parameters():\n        # 计算 total number of elements per 参数\n        param_size = param.numel()\n        total_params += param_size\n        # 检查 如果 gradients are stored for 这个 参数\n        if param.requires_grad:\n            total_grads += param_size\n\n    # 计算 buffer size (non-parameters 那个 require memory)\n    total_buffers = sum(buf.numel() for buf in model.buffers())\n\n    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n    # 我们 assume parameters 和 gradients are stored in 这个 same type as 输入 dtype\n    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n\n    # 转换 bytes to gigabytes\n    total_memory_gb = total_memory_bytes / (1024**3)\n\n    return total_memory_gb\n\nprint(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\nprint(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zudd-5PulKFL",
      "metadata": {
        "id": "zudd-5PulKFL"
      },
      "source": [
        "- Lastly, 我们 can also transfer 这个 模型 to 一个 NVIDIA 或者 Apple Silicon GPU 如果 applicable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a4c50e19-1402-45b6-8ccd-9077b2ba836d",
      "metadata": {
        "id": "a4c50e19-1402-45b6-8ccd-9077b2ba836d"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\nmodel.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dc64a06-27dc-46ec-9e6d-1700a8227d34",
      "metadata": {
        "id": "5dc64a06-27dc-46ec-9e6d-1700a8227d34"
      },
      "source": [
        "&nbsp;\n",
        "## 3. 加载 分词器"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eb30f0c-6144-4bed-87d9-6b2bac377005",
      "metadata": {
        "id": "0eb30f0c-6144-4bed-87d9-6b2bac377005"
      },
      "source": [
        "- In 这个 section, 我们 are going to 加载 这个 分词器 for 这个 模型\n",
        "- Llama 2 uses Google's [SentencePiece](https://github.com/google/sentencepiece) 分词器 instead of OpenAI's [Tiktoken](https://github.com/openai/tiktoken) (但是 Llama 3 uses Tiktoken)\n",
        "- Meta AI shared 这个 original Llama 2 模型 weights 和 分词器 vocabulary on 这个 Hugging Face Hub\n",
        "- 我们 will download 这个 分词器 vocabulary from 这个 Hub 和 加载 它 into SentencePiece\n",
        "- Uncomment 和 运行 这个 following 代码 to 安装 这个 required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "768989ea-dc60-4dc8-ae84-cbb3fd224422",
      "metadata": {
        "id": "768989ea-dc60-4dc8-ae84-cbb3fd224422"
      },
      "outputs": [],
      "source": [
        "# !pip 安装 huggingface_hub sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KbnlzsbYmJU6",
      "metadata": {
        "id": "KbnlzsbYmJU6"
      },
      "source": [
        "- Please note 那个 Meta AI requires 那个 你 accept 这个 Llama 2 licensing terms before 你 can download 这个 files; to do 这个, 你 have to 创建 一个 Hugging Face Hub account 和 visit 这个 [meta-llama/Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b) repository to accept 这个 terms\n",
        "- 接下来, 你 will need to 创建 一个 access 词元; to 生成 一个 access 词元 with READ permissions, click on 这个 profile picture in 这个 upper right 和 click on \"Settings\"\n",
        "\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/GPT-to-llama/settings.webp?1\" width=\"300px\">\n",
        "\n",
        "- 然后, 创建 和 copy 这个 access 词元 so 你 can copy & paste 它 into 这个 接下来 代码 cell\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/GPT-to-llama/access-词元.webp?1\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3357a230-b678-4691-a238-257ee4e80185",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3357a230-b678-4691-a238-257ee4e80185",
        "outputId": "768ed6af-ce14-40bc-ca18-117b4b448269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\nimport json\n\nwith open(\"config.json\", \"r\") as config_file:\n    config = json.load(config_file)\n    access_token = config[\"HF_ACCESS_TOKEN\"]\n\nlogin(token=access_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IxGh6ZYQo0VN",
      "metadata": {
        "id": "IxGh6ZYQo0VN"
      },
      "source": [
        "- After login via 这个 access 词元, 哪个 is necessary to 验证 那个 我们 accepted 这个 Llama 2 licensing terms, 我们 can 现在 download 这个 分词器 vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "69714ea8-b9b8-4687-8392-f3abb8f93a32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "e6c75a6aa7b942fe84160e286e3acb3d",
            "08f0bf9459bd425498a5cb236f9d4a72",
            "10251d6f724e43788c41d4b7879cbfd3",
            "53a973c0853b44418698136bd04df039",
            "bdb071e7145a4007ae01599333e72612",
            "6b1821a7f4574e3aba09c1e410cc81e4",
            "8c2873eaec3445888ad3d54ad7387950",
            "0c8f7044966e4207b12352503c67dcbb",
            "8b5951213c9e4798a258146d61d02d11",
            "2c05df3f91e64df7b33905b1065a76f7",
            "742ae5487f2648fcae7ca8e22c7f8db9"
          ]
        },
        "id": "69714ea8-b9b8-4687-8392-f3abb8f93a32",
        "outputId": "c230fec9-5c71-4a41-90ab-8a34d114ea01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6c75a6aa7b942fe84160e286e3acb3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n\ntokenizer_file = hf_hub_download(\n    repo_id=\"meta-llama/Llama-2-7b\",\n    filename=\"tokenizer.model\",\n    local_dir=\"Llama-2-7b\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gp7iQ8cXAJLv",
      "metadata": {
        "id": "gp7iQ8cXAJLv"
      },
      "source": [
        "- To provide 一个 more familiar interface for 这个 分词器, 我们 定义 一个 small `LlamaTokenizer` wrapper 类:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "Ef4WxhjOBOOc",
      "metadata": {
        "id": "Ef4WxhjOBOOc"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n\n\nclass LlamaTokenizer:\n    def __init__(self, tokenizer_file):\n        sp = spm.SentencePieceProcessor()\n        sp.load(tokenizer_file)\n        self.tokenizer = sp\n\n    def encode(self, text):\n        return self.tokenizer.encode_as_ids(text)\n\n    def decode(self, ids):\n        return self.tokenizer.decode_pieces(ids)\n\n\ntokenizer = LlamaTokenizer(tokenizer_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NVhmFeX3pT_M",
      "metadata": {
        "id": "NVhmFeX3pT_M"
      },
      "source": [
        "- 我们 can 现在 使用 这个 `生成` 函数 to have 这个 Llama 2 模型 生成 new text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e0a2b5cd-6cba-4d72-b8ff-04d8315d483e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0a2b5cd-6cba-4d72-b8ff-04d8315d483e",
        "outputId": "acd5065d-8900-4ba8-ef85-968365f3a0cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort movesαllRadius deletingpretcc否']; future eer napulate lackус während inter DES издаSchéon로жа Bass differencespadxsnu ;; ctx始\n"
          ]
        }
      ],
      "source": [
        "from previous_chapters import generate, text_to_token_ids, token_ids_to_text\n# 如果 这个 `previous_chapters.py` file is not available locally,\n# 你 can 导入 它 from 这个 `llms-from-scratch` PyPI 包.\n# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n# E.g.,\n# from llms_from_scratch.ch05 导入 生成, text_to_token_ids, token_ids_to_text\n\n\n\ntorch.manual_seed(123)\n\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(\"Every effort moves\", tokenizer).to(device),\n    max_new_tokens=30,\n    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n    top_k=1,\n    temperature=0.\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93WTtAA5paYV",
      "metadata": {
        "id": "93WTtAA5paYV"
      },
      "source": [
        "- Of course, as 我们 can see above, 这个 text is nonsensical since 我们 haven't trained 这个 Llama 2 模型 yet\n",
        "- In 这个 接下来 section, instead of 训练 它 ourselves, 哪个 would cost tens to hundreds of thousands of dollars, 我们 加载 这个 pretrained weights from Meta AI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f63cc248-1d27-4eb6-aa50-173b436652f8",
      "metadata": {
        "id": "f63cc248-1d27-4eb6-aa50-173b436652f8"
      },
      "source": [
        "&nbsp;\n",
        "## 4. 加载 pretrained weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aKeN7rUfqZMI",
      "metadata": {
        "id": "aKeN7rUfqZMI"
      },
      "source": [
        "- 我们 are loading 这个 [\"meta-llama/Llama-2-7b\"](https://huggingface.co/meta-llama/Llama-2-7b) base 模型 below, 哪个 is 一个 simple text completion 模型 before finetuning\n",
        "- Alternatively, 你 can 加载 这个 instruction-finetuned 和 aligned [\"meta-llama/Llama-2-7b-chat\"](https://huggingface.co/meta-llama/Llama-2-7b-chat) 模型 by modifying 这个 string in 这个 接下来 代码 cell accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "5fa9c06c-7a53-4b4d-9ce4-acc027322ee4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "66e777955e8748df878f118f07f38dab",
            "da89ae3ea4d2474e98f64ada608f3cea",
            "93e6da39c25f4edfaa72056c89df1f7f",
            "b628603e4cb0405398c916587ee96756",
            "93bedcb9245e44a0a1eb7e4155070f66",
            "0723f467d37b4904819a8bb33ebda10f",
            "e54928776bc649339002adced63738b0",
            "d8e0f42068af4cb094e2f115f76e06e0",
            "0a939565b6e94f08bee0a66e0f9827d4",
            "a5fedbb7ec2e43d99711bb4cd84b9486",
            "0c186f6539714d8eab023969ce47c500"
          ]
        },
        "id": "5fa9c06c-7a53-4b4d-9ce4-acc027322ee4",
        "outputId": "0d8942cc-e5e2-4e77-ec41-1ac7bec7d94f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66e777955e8748df878f118f07f38dab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "consolidated.00.pth:   0%|          | 0.00/13.5G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "weights_file = hf_hub_download(\n   repo_id=\"meta-llama/Llama-2-7b\",\n   filename=\"consolidated.00.pth\",\n   local_dir=\"Llama-2-7b\"\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e67cca5c-ba4b-4be5-85c7-fdceae8a5701",
      "metadata": {
        "id": "e67cca5c-ba4b-4be5-85c7-fdceae8a5701"
      },
      "outputs": [],
      "source": [
        "weights = torch.load(weights_file, weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-15SJ7btq2zE",
      "metadata": {
        "id": "-15SJ7btq2zE"
      },
      "source": [
        "- 这个 `weights` contains 这个 following tensors (only 这个 首先 15 are shown for simplicity):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ee26bd0b-fea9-4924-97f7-409c14f28e49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee26bd0b-fea9-4924-97f7-409c14f28e49",
        "outputId": "fa83d38a-bb41-4cb2-d3c7-c573bfe1f8a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tok_embeddings.weight',\n",
              " 'norm.weight',\n",
              " 'output.weight',\n",
              " 'layers.0.attention.wq.weight',\n",
              " 'layers.0.attention.wk.weight',\n",
              " 'layers.0.attention.wv.weight',\n",
              " 'layers.0.attention.wo.weight',\n",
              " 'layers.0.feed_forward.w1.weight',\n",
              " 'layers.0.feed_forward.w2.weight',\n",
              " 'layers.0.feed_forward.w3.weight',\n",
              " 'layers.0.attention_norm.weight',\n",
              " 'layers.0.ffn_norm.weight',\n",
              " 'layers.1.attention.wq.weight',\n",
              " 'layers.1.attention.wk.weight',\n",
              " 'layers.1.attention.wv.weight']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(weights.keys())[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UeeSpnunrDFB",
      "metadata": {
        "id": "UeeSpnunrDFB"
      },
      "source": [
        "- 这个 following 函数, modeled after 这个 `load_weights_into_gpt` 函数 in [第 5](../01_main-第-代码/ch05.ipynb), loads 这个 pretrained weights into our Llama 2 模型:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "3820e2a7-4f26-41bc-953b-f3879b0aff65",
      "metadata": {
        "id": "3820e2a7-4f26-41bc-953b-f3879b0aff65"
      },
      "outputs": [],
      "source": [
        "def assign(left, right):\n    if left.shape != right.shape:\n        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n\n    if isinstance(right, torch.Tensor):\n        return torch.nn.Parameter(right.clone().detach())\n    else:\n        return torch.nn.Parameter(torch.tensor(right))\n\n\ndef load_weights_into_llama(model, param_config, params):\n    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"tok_embeddings.weight\"])\n\n    for l in range(param_config[\"n_layers\"]):\n\n        # 加载 注意力机制 weights\n        model.trf_blocks[l].att.W_query.weight = assign(\n            model.trf_blocks[l].att.W_query.weight,\n            params[f\"layers.{l}.attention.wq.weight\"]\n        )\n        model.trf_blocks[l].att.W_key.weight = assign(\n            model.trf_blocks[l].att.W_key.weight,\n            params[f\"layers.{l}.attention.wk.weight\"]\n        )\n        model.trf_blocks[l].att.W_value.weight = assign(\n            model.trf_blocks[l].att.W_value.weight,\n            params[f\"layers.{l}.attention.wv.weight\"]\n        )\n        model.trf_blocks[l].att.out_proj.weight = assign(\n            model.trf_blocks[l].att.out_proj.weight,\n            params[f\"layers.{l}.attention.wo.weight\"]\n        )\n        model.trf_blocks[l].norm1.weight = assign(\n            model.trf_blocks[l].norm1.weight,\n            params[f\"layers.{l}.attention_norm.weight\"]\n        )\n\n        # 加载 FeedForward weights\n        model.trf_blocks[l].ff.fc1.weight = assign(\n            model.trf_blocks[l].ff.fc1.weight,\n            params[f\"layers.{l}.feed_forward.w1.weight\"]\n        )\n        # For some reason w2 和 w3 are provided in 这个 wrong order in 这个 weights file\n        model.trf_blocks[l].ff.fc2.weight = assign(\n            model.trf_blocks[l].ff.fc2.weight,\n            params[f\"layers.{l}.feed_forward.w3.weight\"]\n        )\n        model.trf_blocks[l].ff.fc3.weight = assign(\n            model.trf_blocks[l].ff.fc3.weight,\n            params[f\"layers.{l}.feed_forward.w2.weight\"]\n        )\n        model.trf_blocks[l].norm2.weight = assign(\n            model.trf_blocks[l].norm2.weight,\n            params[f\"layers.{l}.ffn_norm.weight\"]\n        )\n\n    # 加载 输出 层 weights\n    model.final_norm.weight = assign(model.final_norm.weight, params[\"norm.weight\"])\n    model.out_head.weight = assign(model.out_head.weight, params[\"output.weight\"])\n\n\nload_weights_into_llama(model, LLAMA2_CONFIG_7B, weights)\nmodel.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TDuv_Us2rNvk",
      "metadata": {
        "id": "TDuv_Us2rNvk"
      },
      "source": [
        "- 接下来, 我们 are ready to 使用 这个 模型 for text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "240987e8-a023-462e-9376-9edfb27559ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "240987e8-a023-462e-9376-9edfb27559ec",
        "outputId": "044f24b3-4018-4860-834d-6c2731b9e47c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort has been made to ensure that the information contained in this website is accurate and up to date and correct at the time of publication\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n    max_new_tokens=25,\n    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n    top_k=1,\n    temperature=0.\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d72ed949-b6c0-4966-922f-eb0da732c404",
      "metadata": {},
      "source": [
        "&nbsp;\n",
        "## 5. Using 这个 instruction-finetuned 模型"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "akyo7WNyF_YL",
      "metadata": {
        "id": "akyo7WNyF_YL"
      },
      "source": [
        "- As mentioned earlier, above 我们 used 这个 pretrained base 模型; 如果 你 want to 使用 一个 模型 capable of following instructions, 使用 这个 `\"meta-llama/Llama-2-7b-chat\"` 模型 instead, as shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "nbvAV7vaz6yc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "3b2448a60f5f4ba5b2c686037c8ecd78",
            "60c5932944f24f5fad1d8da89c8e5ae9",
            "aa31aed1b8854a4281fd7e81c60e1205",
            "d4acf06c2414412f8f2fb4f48981c954",
            "693d69251d3d48219c084af17b54b851",
            "ff36d28c55dd4db3a0f76a87640fdfe2",
            "71c49ef820494d5f8908a3daf39f0755",
            "525dc406534f4369b11208816f8fd0d7",
            "865f39213a7341b68f2fe73caaf801b1",
            "eaf4c0231b6d4993b2f8e9e63d8b6921",
            "a11edf3b018e42c88a63a515cf7fe478"
          ]
        },
        "id": "nbvAV7vaz6yc",
        "outputId": "724f5508-d976-4e31-b3d7-95fa65b2c1e8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b2448a60f5f4ba5b2c686037c8ecd78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "consolidated.00.pth:   0%|          | 0.00/13.5G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " What do llamas eat?\n",
            "Llamas and alpacas are herbivores, which means they eat grasses, leaves, grass\n"
          ]
        }
      ],
      "source": [
        "del model  # to free up memory\n\nweights_file = hf_hub_download(\n   repo_id=\"meta-llama/Llama-2-7b-chat\",\n   filename=\"consolidated.00.pth\",\n   local_dir=\"Llama-2-7b-chat\"\n)\n\nmodel = Llama2Model(LLAMA2_CONFIG_7B)\nload_weights_into_llama(model, LLAMA2_CONFIG_7B, weights)\nmodel.to(device);\n\ntorch.manual_seed(123)\n\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(\"What do llamas eat?\", tokenizer).to(device),\n    max_new_tokens=25,\n    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n    top_k=1,\n    temperature=0.\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f693da1-a07c-4e1d-af5a-c3923525f1e2",
      "metadata": {},
      "source": [
        "&nbsp;\n",
        "# 什么's 接下来?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fae93739-ca12-46ba-8ca7-7c07c59f669b",
      "metadata": {},
      "source": [
        "- 这个 笔记本 converted 这个 original GPT-2 architecture into 一个 Llama 2 模型\n",
        "- 如果 你 are interested in 如何 to 转换 Llama 2 into Llama 3, Llama 3.1, 和 Llama 3.2, 检查 out 这个 [converting-llama2-to-llama3.ipynb](converting-llama2-to-llama3.ipynb) 笔记本"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}