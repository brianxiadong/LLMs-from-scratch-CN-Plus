{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0_xya1nyDHfY",
      "metadata": {
        "id": "0_xya1nyDHfY"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary 代码 for 这个 <一个 href=\"http://mng.bz/orYv\">构建 一个 大语言模型 From Scratch</一个> book by <一个 href=\"https://sebastianraschka.com\">Sebastian Raschka</一个><br>\n",
        "<br>代码 repository: <一个 href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</一个>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<一个 href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></一个>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l62zIRRSBy_R",
      "metadata": {
        "id": "l62zIRRSBy_R"
      },
      "source": [
        "# Converting Llama 2 to Llama 3.2 From Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aFmxTQbwCUMl",
      "metadata": {
        "id": "aFmxTQbwCUMl"
      },
      "source": [
        "- 这个 is 一个 follow-up 笔记本 to [Converting 一个 From-Scratch GPT Architecture to Llama 2](./converting-GPT-to-llama2.ipynb), converting Meta AI's Llama 2 architecture 模型 step by step to Llama 3, Llama 3.1, 和 Llama 3.2\n",
        "- 这个 explanations are purposefully kept minimal in 这个 笔记本 so as not to bloat 它 unnecessarily 和 focus on 这个 main 代码\n",
        "- For more information about 这个 architectures, please see 这个 Llama 2 和 Llama 3 papers\n",
        " - [Llama 2: Open Foundation 和 Fine-Tuned Chat Models (2023)](https://arxiv.org/abs/2307.09288)\n",
        " - [这个 Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ohhMKUWvGm9z",
      "metadata": {
        "id": "ohhMKUWvGm9z"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/GPT-to-llama/gpt2-to-llama2-llama3.webp?1\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ws0wsUzwLH2k",
      "metadata": {
        "id": "ws0wsUzwLH2k"
      },
      "outputs": [],
      "source": [
        "# pip 安装 -r 依赖-extra.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JBpQwU89ETA1",
      "metadata": {
        "id": "JBpQwU89ETA1"
      },
      "source": [
        "- Packages 那个 are being used in 这个 笔记本:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "34a9a440-84c2-42cc-808b-38677cb6af8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34a9a440-84c2-42cc-808b-38677cb6af8a",
        "outputId": "e3d3d4b6-ee63-4e28-d794-e8b0bdd931fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "blobfile version: 3.0.0\n",
            "huggingface_hub version: 0.24.7\n",
            "tiktoken version: 0.8.0\n",
            "torch version: 2.4.1+cu121\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n\npkgs = [\n    \"blobfile\",         # to download pretrained weights\n    \"huggingface_hub\",  # to download pretrained weights\n    \"tiktoken\",         # to 实现 这个 分词器\n    \"torch\",            # to 实现 这个 模型\n]\nfor p in pkgs:\n    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UJJneXpTEg4W",
      "metadata": {
        "id": "UJJneXpTEg4W"
      },
      "source": [
        "&nbsp;\n",
        "# 1. 转换 这个 Llama 模型 实现 step by step"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v1zpfX2GHBKa",
      "metadata": {
        "id": "v1zpfX2GHBKa"
      },
      "source": [
        "- 如果 你 are new to implementing 大语言模型 architectures, I recommend starting with [第 4](../../ch04/01_main-第-代码/ch04.ipynb), 哪个 walks 你 through 这个 实现 of 这个 original GPT architecture step by step\n",
        "- 这个 [Converting 一个 From-Scratch GPT Architecture to Llama 2](./converting-GPT-to-llama2.ipynb) 然后 implements 这个 Llama-specific components, such as RMSNorm layers, SiLU 和 SwiGLU activations, RoPE (rotary position embeddings), 和 这个 SentencePiece 分词器\n",
        "- 这个 笔记本 takes 这个 Llama 2 architecture 和 transforms 它 into Llama 3 architecture by\n",
        "    1. modifying 这个 rotary embeddings\n",
        "    2. implementing grouped-query 注意力机制\n",
        "    3. 和 using 一个 customized version of 这个 GPT-4 分词器\n",
        "- Later, 我们 然后 加载 这个 original Llama 3 weights shared by Meta AI into 这个 architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c14b9121-abe1-4a46-99b8-acdef71e5b41",
      "metadata": {
        "id": "c14b9121-abe1-4a46-99b8-acdef71e5b41"
      },
      "source": [
        "&nbsp;\n",
        "## 1.1 Reusing Llama 2 components"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dgDhJGJ6xR4e",
      "metadata": {
        "id": "dgDhJGJ6xR4e"
      },
      "source": [
        "- Llama 2 is actually quite similar to Llama 3, as mentioned above 和 illustrated in 这个 figure at 这个 top of 这个 笔记本\n",
        "- 这个 means 那个 我们 can 导入 several building blocks from 这个 [Llama 2 笔记本](./converting-GPT-to-llama2.ipynb) using 这个 following 代码"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a5bc3948-231b-4f1f-8d41-24ad0b7643d0",
      "metadata": {
        "id": "a5bc3948-231b-4f1f-8d41-24ad0b7643d0"
      },
      "outputs": [],
      "source": [
        "import os\nimport sys\nimport io\nimport nbformat\nimport types\n\ndef import_from_notebook():\n    def import_definitions_from_notebook(fullname, names):\n        current_dir = os.getcwd()\n        path = os.path.join(current_dir, fullname + \".ipynb\")\n        path = os.path.normpath(path)\n\n        # 加载 这个 笔记本\n        if not os.path.exists(path):\n            raise FileNotFoundError(f\"Notebook file not found at: {path}\")\n\n        with io.open(path, \"r\", encoding=\"utf-8\") as f:\n            nb = nbformat.read(f, as_version=4)\n\n        # 创建 一个 模块 to store 这个 imported functions 和 classes\n        mod = types.ModuleType(fullname)\n        sys.modules[fullname] = mod\n\n        # Go through 这个 笔记本 cells 和 only 执行 函数 或者 类 definitions\n        for cell in nb.cells:\n            if cell.cell_type == \"code\":\n                cell_code = cell.source\n                for name in names:\n                    # 检查 for 函数 或者 类 definitions\n                    if f\"def {name}\" in cell_code or f\"class {name}\" in cell_code:\n                        exec(cell_code, mod.__dict__)\n        return mod\n\n    fullname = \"converting-gpt-to-llama2\"\n    names = [\"precompute_rope_params\", \"compute_rope\", \"SiLU\", \"FeedForward\", \"RMSNorm\", \"MultiHeadAttention\"]\n\n    return import_definitions_from_notebook(fullname, names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d546032d-fce4-47cf-8d0e-682b78b21c61",
      "metadata": {
        "id": "d546032d-fce4-47cf-8d0e-682b78b21c61"
      },
      "outputs": [],
      "source": [
        "imported_module = import_from_notebook()\n\n# 我们 need to redefine precompute_rope_params\n# precompute_rope_params = getattr(imported_module, \"precompute_rope_params\", None)\ncompute_rope = getattr(imported_module, \"compute_rope\", None)\nSiLU = getattr(imported_module, \"SiLU\", None)\nFeedForward = getattr(imported_module, \"FeedForward\", None)\nRMSNorm = getattr(imported_module, \"RMSNorm\", None)\n\n# MultiHeadAttention only for comparison purposes\nMultiHeadAttention = getattr(imported_module, \"MultiHeadAttention\", None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "979c7b6d-1370-4da1-8bfb-a2b27537bf2f",
      "metadata": {
        "id": "979c7b6d-1370-4da1-8bfb-a2b27537bf2f"
      },
      "source": [
        "&nbsp;\n",
        "## 1.2 Modified RoPE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m9_oDcHCx8VI",
      "metadata": {
        "id": "m9_oDcHCx8VI"
      },
      "source": [
        "- Llama 3 uses rotary position embeddings (RoPE) similar to Llama 2 (for 一个 detailed explanation, please see 这个 [RoPE paper](https://arxiv.org/abs/2104.09864))\n",
        "- 那里 are some subtle differences in 这个 RoPE settings, though\n",
        " - Llama 3 现在 supports up to 8,192 tokens, twice as many as Llama 2 (4,096)\n",
        " - 这个 base value for 这个 so-called RoPE $\\theta$ (see equation below) was increased from 10,000 (Llama 2) to 500,000 (Llama 3) in 这个 following equation (adapted from 这个 [RoPE paper](https://arxiv.org/abs/2104.09864))\n",
        "\n",
        "$$\\Theta = \\left\\{\\theta_i = \\text{base}^{\\frac{-2(i-1)}{d}}, i \\in \\left[1, 2, ..., d/2\\right]\\right\\}$$\n",
        "\n",
        "- These $\\theta$ values are 一个 设置 of predefined parameters 那个 are used to determine 这个 rotational angles in 这个 rotary matrix, 哪里 $d$ is 这个 dimensionality of 这个 嵌入 space\n",
        "- Increasing 这个 base from 10,000 to 500,000 makes 这个 frequencies (或者 rotation angles) decay more slowly across 这个 dimensions, 哪个 means 那个 higher dimensions will be associated with larger angles than before (essentially, 它's 一个 decompression of 这个 frequencies)\n",
        "- In addition, 我们 introduce 一个 `freq_config` section in 这个 代码 below 那个 adjusts 这个 frequency; however, 我们 won't be needing 它 in Llama 3 (only Llama 3.1 和 Llama 3.2), so 我们 will revisit 这个 `freq_config` later (它's 设置 to `None` 和 ignored by default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6Upl109OOAcu",
      "metadata": {
        "id": "6Upl109OOAcu"
      },
      "outputs": [],
      "source": [
        "import torch\n\ndef precompute_rope_params(head_dim, theta_base=10_000, context_length=4096, freq_config=None):\n    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n\n    # 计算 这个 inverse frequencies\n    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n\n    ################################ NEW ###############################################\n    # Frequency adjustments\n    if freq_config is not None:\n        low_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"low_freq_factor\"]\n        high_freq_wavelen = freq_config[\"original_context_length\"] / freq_config[\"high_freq_factor\"]\n\n        wavelen = 2 * torch.pi / inv_freq\n\n        inv_freq_llama = torch.where(\n            wavelen > low_freq_wavelen, inv_freq / freq_config[\"factor\"], inv_freq\n        )\n\n        smooth_factor = (freq_config[\"original_context_length\"] / wavelen - freq_config[\"low_freq_factor\"]) / (\n            freq_config[\"high_freq_factor\"] - freq_config[\"low_freq_factor\"]\n        )\n\n        smoothed_inv_freq = (\n            (1 - smooth_factor) * (inv_freq / freq_config[\"factor\"]) + smooth_factor * inv_freq\n        )\n\n        is_medium_freq = (wavelen <= low_freq_wavelen) & (wavelen >= high_freq_wavelen)\n        inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n        inv_freq = inv_freq_llama\n    ####################################################################################\n\n\n    # 生成 position indices\n    positions = torch.arange(context_length)\n\n    # 计算 这个 angles\n    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n\n    # Expand angles to match 这个 head_dim\n    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n\n    # Precompute sine 和 cosine\n    cos = torch.cos(angles)\n    sin = torch.sin(angles)\n\n    return cos, sin"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jJBvO0YMJBXR",
      "metadata": {
        "id": "jJBvO0YMJBXR"
      },
      "source": [
        "- To summarize, 什么's new so far for Llama 3 compared to Llama 2 are 这个 context length 和 theta base 参数:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "56c37216-e022-4603-be16-f9d3eaeaf4a1",
      "metadata": {
        "id": "56c37216-e022-4603-be16-f9d3eaeaf4a1"
      },
      "outputs": [],
      "source": [
        "# Instantiate RoPE parameters\n\nllama_2_context_len = 4096\nllama_3_context_len = 8192\n\nllama_2_theta_base = 10_000\nllama_3_theta_base = 500_000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_V8v6i7MJItU",
      "metadata": {
        "id": "_V8v6i7MJItU"
      },
      "source": [
        "- 这个 usage remains 这个 same as before in Llama 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dae70c8a-eb18-40f9-a2e5-a6af2a57628b",
      "metadata": {
        "id": "dae70c8a-eb18-40f9-a2e5-a6af2a57628b"
      },
      "outputs": [],
      "source": [
        "# Settings\nbatch_size = 2\nnum_heads = 4\nhead_dim = 16\n\n# Instantiate RoPE parameters\ncos, sin = precompute_rope_params(\n    head_dim=head_dim,\n    theta_base=llama_3_theta_base,\n    context_length=llama_3_context_len\n)\n\n# Dummy query 和 key tensors\ntorch.manual_seed(123)\nqueries = torch.randn(batch_size, num_heads, llama_3_context_len, head_dim)\nkeys = torch.randn(batch_size, num_heads, llama_3_context_len, head_dim)\n\n# 应用 rotary position embeddings\nqueries_rot = compute_rope(queries, cos, sin)\nkeys_rot = compute_rope(keys, cos, sin)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd19b75c-cf25-47b8-a010-6733fc0e9a8a",
      "metadata": {
        "id": "cd19b75c-cf25-47b8-a010-6733fc0e9a8a"
      },
      "source": [
        "&nbsp;\n",
        "## 1.3 Grouped-query 注意力机制"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "111c7d3f-fded-49e8-a617-9fe67b81dddc",
      "metadata": {
        "id": "111c7d3f-fded-49e8-a617-9fe67b81dddc"
      },
      "source": [
        "- In 这个 section, 我们 replace multi-head 注意力机制 (MHA) with 一个 alternative mechanism called grouped-query 注意力机制 (GQA)\n",
        "- In short, one can think of GQA as 一个 more 计算- 和 参数-efficient version of MHA\n",
        "- In GQA, 我们 reduce 这个 number of key 和 value projections by sharing them among multiple 注意力机制 heads\n",
        "- Each 注意力机制 head still has its unique query, 但是 these queries attend to 这个 same group of keys 和 values\n",
        "- Below is 一个 illustration of GQA with 2 key-value-groups (kv-groups):\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/GPT-to-llama/grouped-query-注意力机制.webp\" width=\"500px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "perAYa2R_KW2",
      "metadata": {
        "id": "perAYa2R_KW2"
      },
      "source": [
        "- 这个 main idea behind GQA is to reduce 这个 number of unique query groups 那个 attend to 这个 key-value pairs, reducing 这个 size of some of 这个 matrix multiplications 和 这个 number of parameters in MHA without significantly reducing modeling 性能\n",
        "- 这个 GQA 代码 is very similar to MHA (I highlighted 这个 changes below via 这个 \"NEW\" sections)\n",
        "- In short, 这个 main 改变 in GQA is 那个 each query group needs to be repeated to match 这个 number of heads 它 is associated with, as implemented below"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "842aa71a-4659-424e-8830-392bd6ae86af",
      "metadata": {},
      "source": [
        "- In addition, 我们 also introduce 一个 `SharedBuffers` 类 那个 will allow us to reuse 这个 `mask`, `cos`, 和 `sin` tensors in 这个 Transformer blocks to improve efficiency (这个 will be crucial 当 working with models such as Llama 3.1 和 3.2 later, 哪个 support up to 131k 输入 tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9b12e674-ef08-4dd7-8843-615b65b39c91",
      "metadata": {
        "id": "9b12e674-ef08-4dd7-8843-615b65b39c91"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n\n\n############################# NEW  #############################\nclass SharedBuffers:\n    _buffers = {}\n\n    @staticmethod\n    def get_buffers(context_length, head_dim, rope_base, freq_config, dtype=torch.float32):\n        key = (context_length, head_dim, rope_base, tuple(freq_config.values()) if freq_config else freq_config, dtype)\n\n        if key not in SharedBuffers._buffers:\n            # 创建 或者 fetch 这个 buffers\n            mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n            cos, sin = precompute_rope_params(head_dim, rope_base, context_length, freq_config)\n            if dtype is not None:\n                cos = cos.to(dtype)\n                sin = sin.to(dtype)\n            SharedBuffers._buffers[key] = (mask, cos, sin)\n\n        return SharedBuffers._buffers[key]\n############################# NEW  #############################\n\n\nclass GroupedQueryAttention(nn.Module):\n    def __init__(\n            self, d_in, d_out, context_length, num_heads,\n            num_kv_groups,       # NEW\n            rope_base=10_000,    # NEW\n            rope_config=None,    # NEW\n            dtype=None\n        ):\n        super().__init__()\n        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"  # NEW\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads\n\n        ############################# NEW  #############################\n        # self.W_key = nn.Linear(d_in, d_out, 偏置=False, dtype=dtype)\n        # self.W_value = nn.Linear(d_in, d_out, 偏置=False, dtype=dtype)\n        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n        self.num_kv_groups = num_kv_groups\n        self.group_size = num_heads // num_kv_groups\n        ################################################################\n\n        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n\n        ############################# NEW  #############################\n        # Fetch buffers using SharedBuffers\n        mask, cos, sin = SharedBuffers.get_buffers(context_length, self.head_dim, rope_base, rope_config, dtype)\n        ############################# NEW  #############################\n        \n        self.register_buffer(\"mask\", mask)\n        self.register_buffer(\"cos\", cos)\n        self.register_buffer(\"sin\", sin)\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n\n        queries = self.W_query(x)  # Shape: (b, num_tokens, d_out)\n        keys = self.W_key(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n        values = self.W_value(x)  # Shape: (b, num_tokens, num_kv_groups * head_dim)\n\n        # Reshape queries, keys, 和 values\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        ##################### NEW  #####################\n        # keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        # values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim)\n        ################################################\n\n        # Transpose keys, values, 和 queries\n        keys = keys.transpose(1, 2)  # Shape: (b, num_heads, num_tokens, head_dim)\n        values = values.transpose(1, 2)  # Shape: (b, num_heads, num_tokens, head_dim)\n        queries = queries.transpose(1, 2)  # Shape: (b, num_query_groups, num_tokens, head_dim)\n\n        # 应用 RoPE\n        keys = compute_rope(keys, self.cos, self.sin)\n        queries = compute_rope(queries, self.cos, self.sin)\n\n        ##################### NEW  #####################\n        # Expand keys 和 values to match 这个 number of heads\n        # Shape: (b, num_heads, num_tokens, head_dim)\n\n        keys = keys.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n        values = values.repeat_interleave(self.group_size, dim=1)  # Shape: (b, num_heads, num_tokens, head_dim)\n        # For 示例, before repeat_interleave along dim=1 (query groups):\n        #   [K1, K2]\n        # After repeat_interleave (each query group is repeated group_size times):\n        #   [K1, K1, K2, K2]\n        # 如果 我们 used regular repeat instead of repeat_interleave, 我们'd 获取:\n        #   [K1, K2, K1, K2]\n        ################################################\n\n        # 计算 scaled dot-product 注意力机制 (aka self-注意力机制) with 一个 causal mask\n        # Shape: (b, num_heads, num_tokens, num_tokens)\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to 这个 number of tokens 和 converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # 使用 这个 mask to fill 注意力机制 scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        assert keys.shape[-1] == self.head_dim\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2)\n\n        # Combine heads, 哪里 self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec)  # optional projection\n\n        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "roAXSwJs9hR8",
      "metadata": {
        "id": "roAXSwJs9hR8"
      },
      "source": [
        "- To illustrate 这个 参数 savings, consider 这个 following multi-head 注意力机制 示例 from 这个 GPT 和 Llama 2 代码:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b4b8f085-349e-4674-a3f0-78fde0664fac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4b8f085-349e-4674-a3f0-78fde0664fac",
        "outputId": "9da09d72-43b1-45af-d46f-6928ea4af33a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W_key: torch.Size([4096, 4096])\n",
            "W_value: torch.Size([4096, 4096])\n",
            "W_query: torch.Size([4096, 4096])\n"
          ]
        }
      ],
      "source": [
        "# Settings\nbatch_size = 1\ncontext_len = 3000\nmax_context_len = 8192\nembed_dim = 4096\nnum_heads = 32\n\n\nexample_batch = torch.randn((batch_size, context_len, embed_dim))\n\nmha = MultiHeadAttention(\n    d_in=embed_dim,\n    d_out=embed_dim,\n    context_length=max_context_len,\n    num_heads=num_heads\n)\n\nmha(example_batch)\n\nprint(\"W_key:\", mha.W_key.weight.shape)\nprint(\"W_value:\", mha.W_value.weight.shape)\nprint(\"W_query:\", mha.W_query.weight.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IMQtFkcQ9sXC",
      "metadata": {
        "id": "IMQtFkcQ9sXC"
      },
      "source": [
        "- 现在, 如果 我们 使用 grouped-query 注意力机制 instead, with 8 kv-groups (那个's 如何 many Llama 3 8B uses), 我们 can see 那个 这个 number of rows of 这个 key 和 value matrices are reduced by 一个 factor of 4 (because 32 注意力机制 heads divided by 8 kv-groups is 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "15e65d3c-7b42-4ed3-bfee-bb09578657bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15e65d3c-7b42-4ed3-bfee-bb09578657bb",
        "outputId": "69709a78-2aaa-4597-8142-2f44eb59753f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W_key: torch.Size([1024, 4096])\n",
            "W_value: torch.Size([1024, 4096])\n",
            "W_query: torch.Size([4096, 4096])\n"
          ]
        }
      ],
      "source": [
        "gqa = GroupedQueryAttention(\n    d_in=embed_dim,\n    d_out=embed_dim,\n    context_length=max_context_len,\n    num_heads=num_heads,\n    num_kv_groups=8,\n    rope_base=llama_3_theta_base\n)\n\ngqa(example_batch)\n\nprint(\"W_key:\", gqa.W_key.weight.shape)\nprint(\"W_value:\", gqa.W_value.weight.shape)\nprint(\"W_query:\", gqa.W_query.weight.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a5d4c88-c66a-483b-b4e2-419ff9fd60d5",
      "metadata": {
        "id": "1a5d4c88-c66a-483b-b4e2-419ff9fd60d5"
      },
      "source": [
        "- As 一个 side note, to make 这个 GroupedQueryAttention equivalent to standard multi-head 注意力机制, 你 can 设置 这个 number of query groups (`num_kv_groups`) equal to 这个 number of heads (`num_heads`)\n",
        "- Lastly, 让我们 compare 这个 number of parameters below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "58f713aa-ac00-4e2f-8247-94609aa01350",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58f713aa-ac00-4e2f-8247-94609aa01350",
        "outputId": "486dfd9c-9f3a-4b9e-f9a2-35fb43b9a5fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters:\n",
            "MHA: 67,108,864\n",
            "GQA: 41,943,040\n"
          ]
        }
      ],
      "source": [
        "print(\"Total number of parameters:\")\n\nmha_total_params = sum(p.numel() for p in mha.parameters())\nprint(f\"MHA: {mha_total_params:,}\")\n\ngqa_total_params = sum(p.numel() for p in gqa.parameters())\nprint(f\"GQA: {gqa_total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "78b60dfd-6c0f-41f7-8f0c-8e57116f07f5",
      "metadata": {
        "id": "78b60dfd-6c0f-41f7-8f0c-8e57116f07f5"
      },
      "outputs": [],
      "source": [
        "# Free up memory:\ndel mha\ndel gqa"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fcd8802-2859-45a2-905a-f4fe96629dd9",
      "metadata": {
        "id": "8fcd8802-2859-45a2-905a-f4fe96629dd9"
      },
      "source": [
        "&nbsp;\n",
        "## 1.4 更新 这个 TransformerBlock 模块"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KABNccft_YnR",
      "metadata": {
        "id": "KABNccft_YnR"
      },
      "source": [
        "- 接下来, 我们 更新 这个 `TransformerBlock`\n",
        "- 这里, 我们 simply swap `MultiHeadAttention` with `GroupedQueryAttention` 和 添加 这个 new RoPE settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f9fa8eb4-7196-4dee-aec6-0dcbc70921c4",
      "metadata": {
        "id": "f9fa8eb4-7196-4dee-aec6-0dcbc70921c4"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att =  GroupedQueryAttention(  # MultiHeadAttention(\n            d_in=cfg[\"emb_dim\"],\n            d_out=cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            num_heads=cfg[\"n_heads\"],\n            num_kv_groups=cfg[\"n_kv_groups\"],  # NEW\n            rope_base=cfg[\"rope_base\"],        # NEW\n            rope_config=cfg[\"rope_freq\"],      # NEW\n            dtype=cfg[\"dtype\"]\n        )\n        self.ff = FeedForward(cfg)\n        self.norm1 = RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n        self.norm2 = RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n\n    def forward(self, x):\n        # Shortcut connection for 注意力机制 block\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x.to(torch.bfloat16))   # Shape [batch_size, num_tokens, emb_size]\n        x = x + shortcut  # 添加 这个 original 输入 back\n\n        # Shortcut connection for feed-forward block\n        shortcut = x\n        x = self.norm2(x)\n        x = self.ff(x.to(torch.bfloat16))\n        x = x + shortcut  # 添加 这个 original 输入 back\n\n        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd921ab5-c48c-4c52-bf41-b847b3b822b9",
      "metadata": {
        "id": "fd921ab5-c48c-4c52-bf41-b847b3b822b9"
      },
      "source": [
        "&nbsp;\n",
        "## 1.5 Defining 这个 模型 类"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M_tLAq_r_llN",
      "metadata": {
        "id": "M_tLAq_r_llN"
      },
      "source": [
        "- 当 setting up 这个 模型 类, 我们 fortunately don't have to do much; 我们 just 更新 这个 name to `Llama3Model`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "475755d6-01f7-4e6e-ad9a-cec6f031ebf6",
      "metadata": {
        "id": "475755d6-01f7-4e6e-ad9a-cec6f031ebf6"
      },
      "outputs": [],
      "source": [
        "# 类 Llama2Model(nn.模块):\nclass Llama3Model(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n\n        self.final_norm = RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n\n    def forward(self, in_idx):\n        tok_embeds = self.tok_emb(in_idx)\n        x = tok_embeds\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x.to(torch.bfloat16))\n        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc94940-aaeb-45b9-9399-3a69b8043e60",
      "metadata": {
        "id": "4bc94940-aaeb-45b9-9399-3a69b8043e60"
      },
      "source": [
        "&nbsp;\n",
        "## 2. 初始化 模型"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HoGGRAGykQTE",
      "metadata": {
        "id": "HoGGRAGykQTE"
      },
      "source": [
        "- 现在 我们 can 定义 一个 Llama 3 config file (这个 Llama 2 config file is shown for comparison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e0564727-2d35-4f0c-b0fc-cde1e9134a18",
      "metadata": {
        "id": "e0564727-2d35-4f0c-b0fc-cde1e9134a18"
      },
      "outputs": [],
      "source": [
        "LLAMA2_CONFIG_7B = {\n    \"vocab_size\": 32_000,    # Vocabulary size\n    \"context_length\": 4096,  # Context length\n    \"emb_dim\": 4096,         # 嵌入 dimension\n    \"n_heads\": 32,           # Number of 注意力机制 heads\n    \"n_layers\": 32,          # Number of layers\n    \"hidden_dim\": 11_008,    # Size of 这个 intermediate dimension in FeedForward\n    \"dtype\": torch.bfloat16  # Lower-precision dtype to reduce memory usage\n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2ad90f82-15c7-4806-b509-e45b56f57db5",
      "metadata": {
        "id": "2ad90f82-15c7-4806-b509-e45b56f57db5"
      },
      "outputs": [],
      "source": [
        "LLAMA3_CONFIG_8B = {\n    \"vocab_size\": 128_256,   # NEW: Larger vocabulary size\n    \"context_length\": 8192,  # NEW: Larger context length\n    \"emb_dim\": 4096,         # 嵌入 dimension\n    \"n_heads\": 32,           # Number of 注意力机制 heads\n    \"n_layers\": 32,          # Number of layers\n    \"hidden_dim\": 14_336,    # NEW: Larger size of 这个 intermediate dimension in FeedForward\n    \"n_kv_groups\": 8,        # NEW: Key-Value groups for grouped-query 注意力机制\n    \"rope_base\": 500_000.0,  # NEW: 这个 base in RoPE's \"theta\" was increased to 500_000\n    \"rope_freq\": None,       # NEW: Additional 配置 for adjusting 这个 RoPE frequencies\n    \"dtype\": torch.bfloat16  # Lower-precision dtype to reduce memory usage\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FAP7fiBzkaBz",
      "metadata": {
        "id": "FAP7fiBzkaBz"
      },
      "source": [
        "- Using these settings, 我们 can 现在 初始化 一个 Llama 3 8B 模型\n",
        "- Note 那个 这个 requires ~34 GB of memory (for comparison, Llama 2 7B required ~26 GB of memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7004d785-ac9a-4df5-8760-6807fc604686",
      "metadata": {
        "id": "7004d785-ac9a-4df5-8760-6807fc604686"
      },
      "outputs": [],
      "source": [
        "model = Llama3Model(LLAMA3_CONFIG_8B)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edea6334-d1fc-427d-9cf2-4af963ff4bfc",
      "metadata": {},
      "source": [
        "- 这个 following is expected to 打印 True to confirm buffers are reused instead of being (wastefully) recreated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee9625cc-9afa-4b11-8aab-d536fd170761",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 检查 buffers\nprint(model.trf_blocks[0].att.mask is model.trf_blocks[-1].att.mask)\nprint(model.trf_blocks[0].att.cos is model.trf_blocks[-1].att.cos)\nprint(model.trf_blocks[0].att.sin is model.trf_blocks[-1].att.sin) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8056a521-91a6-440f-8473-591409c3177b",
      "metadata": {},
      "source": [
        "- 让我们 现在 also 计算 这个 number of trainable parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6079f747-8f20-4c6b-8d38-7156f1101729",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6079f747-8f20-4c6b-8d38-7156f1101729",
        "outputId": "0a8cd23b-d9fa-4c2d-ca63-3fc79bc4de0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 8,030,261,248\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total number of parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bx14NtzWk2wj",
      "metadata": {
        "id": "Bx14NtzWk2wj"
      },
      "source": [
        "- As shown above, 这个 模型 contains 8 billion parameters\n",
        "- Additionally, 我们 can 计算 这个 memory 依赖 for 这个 模型 using 这个 代码 below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0df1c79e-27a7-4b0f-ba4e-167fe107125a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0df1c79e-27a7-4b0f-ba4e-167fe107125a",
        "outputId": "3425e9ce-d8c0-4b37-bded-a2c60b66a41a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float32 (PyTorch default): 68.08 GB\n",
            "bfloat16: 34.04 GB\n"
          ]
        }
      ],
      "source": [
        "def model_memory_size(model, input_dtype=torch.float32):\n    total_params = 0\n    total_grads = 0\n    for param in model.parameters():\n        # 计算 total number of elements per 参数\n        param_size = param.numel()\n        total_params += param_size\n        # 检查 如果 gradients are stored for 这个 参数\n        if param.requires_grad:\n            total_grads += param_size\n\n    # 计算 buffer size (non-parameters 那个 require memory)\n    total_buffers = sum(buf.numel() for buf in model.buffers())\n\n    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n    # 我们 assume parameters 和 gradients are stored in 这个 same type as 输入 dtype\n    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n\n    # 转换 bytes to gigabytes\n    total_memory_gb = total_memory_bytes / (1024**3)\n\n    return total_memory_gb\n\nprint(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\nprint(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zudd-5PulKFL",
      "metadata": {
        "id": "zudd-5PulKFL"
      },
      "source": [
        "- Lastly, 我们 can also transfer 这个 模型 to 一个 NVIDIA 或者 Apple Silicon GPU 如果 applicable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a4c50e19-1402-45b6-8ccd-9077b2ba836d",
      "metadata": {
        "id": "a4c50e19-1402-45b6-8ccd-9077b2ba836d"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\nmodel.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dc64a06-27dc-46ec-9e6d-1700a8227d34",
      "metadata": {
        "id": "5dc64a06-27dc-46ec-9e6d-1700a8227d34"
      },
      "source": [
        "&nbsp;\n",
        "## 3. 加载 分词器"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eb30f0c-6144-4bed-87d9-6b2bac377005",
      "metadata": {
        "id": "0eb30f0c-6144-4bed-87d9-6b2bac377005"
      },
      "source": [
        "- In 这个 section, 我们 are going to 加载 这个 分词器 for 这个 模型\n",
        "- Llama 2 used Google's [SentencePiece](https://github.com/google/sentencepiece) 分词器 instead of OpenAI's BPE 分词器 based on 这个 [Tiktoken](https://github.com/openai/tiktoken) 库\n",
        "- Llama 3, however, reverted back to using 这个 BPE 分词器 from Tiktoken; specifically, 它 uses 这个 GPT-4 分词器 with 一个 extended vocabulary\n",
        "- 你 can find 这个 original Tiktoken-adaptation by Meta AI [这里](https://github.com/meta-llama/llama3/blob/main/llama/分词器.py) in their official Llama 3 repository\n",
        "- Below, I rewrote 这个 分词器 代码 to make 它 more readable 和 minimal for 这个 笔记本 (但是 这个 behavior should be similar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5f390cbf-8f92-46dc-afe3-d90b5affae10",
      "metadata": {
        "id": "5f390cbf-8f92-46dc-afe3-d90b5affae10"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nimport tiktoken\nfrom tiktoken.load import load_tiktoken_bpe\n\n\nclass Tokenizer:\n    def __init__(self, model_path):\n        assert os.path.isfile(model_path), f\"Model file {model_path} not found\"\n        mergeable_ranks = load_tiktoken_bpe(model_path)\n\n        self.special_tokens = {\n            \"<|begin_of_text|>\": 128000,\n            \"<|end_of_text|>\": 128001,\n            \"<|start_header_id|>\": 128006,\n            \"<|end_header_id|>\": 128007,\n            \"<|eot_id|>\": 128009,\n        }\n        self.special_tokens.update({\n            f\"<|reserved_{i}|>\": 128002 + i for i in range(256) if (128002 + i) not in self.special_tokens.values()\n        })\n\n        self.model = tiktoken.Encoding(\n            name=Path(model_path).name,\n            pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n            mergeable_ranks=mergeable_ranks,\n            special_tokens=self.special_tokens\n        )\n\n\n    def encode(self, text, bos=False, eos=False, allowed_special=set(), disallowed_special=()):\n        if bos:\n            tokens = [self.special_tokens[\"<|begin_of_text|>\"]]\n        else:\n            tokens = []\n\n        tokens += self.model.encode(text, allowed_special=allowed_special, disallowed_special=disallowed_special)\n\n        if eos:\n            tokens.append(self.special_tokens[\"<|end_of_text|>\"])\n        return tokens\n\n    def decode(self, tokens):\n        return self.model.decode(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a1509f8-8778-4fec-ba32-14d95c646167",
      "metadata": {
        "id": "0a1509f8-8778-4fec-ba32-14d95c646167"
      },
      "source": [
        "- Meta AI shared 这个 original Llama 3 模型 weights 和 分词器 vocabulary on 这个 Hugging Face Hub\n",
        "- 我们 will 首先 download 这个 分词器 vocabulary from 这个 Hub 和 加载 它 into 这个 代码 above"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KbnlzsbYmJU6",
      "metadata": {
        "id": "KbnlzsbYmJU6"
      },
      "source": [
        "- Please note 那个 Meta AI requires 那个 你 accept 这个 Llama 3 licensing terms before 你 can download 这个 files; to do 这个, 你 have to 创建 一个 Hugging Face Hub account 和 visit 这个 [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B) repository to accept 这个 terms\n",
        "- 接下来, 你 will need to 创建 一个 access 词元; to 生成 一个 access 词元 with READ permissions, click on 这个 profile picture in 这个 upper right 和 click on \"Settings\"\n",
        "\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/GPT-to-llama/settings.webp?1\" width=\"300px\">\n",
        "\n",
        "- 然后, 创建 和 copy 这个 access 词元 so 你 can copy & paste 它 into 这个 接下来 代码 cell\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/GPT-to-llama/access-词元.webp?1\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3357a230-b678-4691-a238-257ee4e80185",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3357a230-b678-4691-a238-257ee4e80185",
        "outputId": "a3652def-ea7f-46fb-f293-2a59affb71a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\nimport json\n\nwith open(\"config.json\", \"r\") as config_file:\n    config = json.load(config_file)\n    access_token = config[\"HF_ACCESS_TOKEN\"]\n\nlogin(token=access_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IxGh6ZYQo0VN",
      "metadata": {
        "id": "IxGh6ZYQo0VN"
      },
      "source": [
        "- After login via 这个 access 词元, 哪个 is necessary to 验证 那个 我们 accepted 这个 Llama 3 licensing terms, 我们 can 现在 download 这个 分词器 vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "69714ea8-b9b8-4687-8392-f3abb8f93a32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69714ea8-b9b8-4687-8392-f3abb8f93a32",
        "outputId": "c9836ba8-5176-4dd5-b618-6cc36fdbe1f0"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n\ntokenizer_file_path = hf_hub_download(\n    repo_id=\"meta-llama/Meta-Llama-3-8B\",\n    filename=\"original/tokenizer.model\",\n    local_dir=\"Llama-3-8B\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F8BH1Nk0AYCS",
      "metadata": {
        "id": "F8BH1Nk0AYCS"
      },
      "source": [
        "- Note 那个 for using Llama 3 files, 我们 may need 这个 `blobfile` 包, 哪个 is used 当 handling datasets 或者 models stored in cloud storage 解答 like Google Cloud Storage (GCS), Azure Blob Storage, 或者 Amazon S3\n",
        "- 你 can 安装 这个 dependency by uncommenting 和 executing 这个 `pip` command below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5dm6Oz7uAytV",
      "metadata": {
        "id": "5dm6Oz7uAytV"
      },
      "outputs": [],
      "source": [
        "# pip 安装 blobfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "8b8c0ce6-a6fb-4b8a-8de2-ee7bb7646fd0",
      "metadata": {
        "id": "8b8c0ce6-a6fb-4b8a-8de2-ee7bb7646fd0"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(tokenizer_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NVhmFeX3pT_M",
      "metadata": {
        "id": "NVhmFeX3pT_M"
      },
      "source": [
        "- 我们 can 现在 使用 这个 `生成` 函数 to have 这个 Llama 3 模型 生成 new text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e0a2b5cd-6cba-4d72-b8ff-04d8315d483e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0a2b5cd-6cba-4d72-b8ff-04d8315d483e",
        "outputId": "990d7b74-cb35-476b-d8bd-d544006e00f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort_dead aeros Ingredients başında.extensionégor clangmissions güc như submodule.and report官方%，.Reader(\",\");\n",
            "ामल ندار Parliamentary !!! HigginsDynamicZhgmt writeln Globalsletion 사진------\n"
          ]
        }
      ],
      "source": [
        "from previous_chapters import generate, text_to_token_ids, token_ids_to_text\n# 如果 这个 `previous_chapters.py` file is not available locally,\n# 你 can 导入 它 from 这个 `llms-from-scratch` PyPI 包.\n# For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n# E.g.,\n# from llms_from_scratch.ch05 导入 生成, text_to_token_ids, token_ids_to_text\n\n\ntorch.manual_seed(123)\n\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n    max_new_tokens=30,\n    context_size=LLAMA3_CONFIG_8B[\"context_length\"],\n    top_k=1,\n    temperature=0.\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93WTtAA5paYV",
      "metadata": {
        "id": "93WTtAA5paYV"
      },
      "source": [
        "- Of course, as 我们 can see above, 这个 text is nonsensical since 我们 haven't trained 这个 Llama 3 模型 yet\n",
        "- In 这个 接下来 section, instead of 训练 它 ourselves, 哪个 would cost tens to hundreds of thousands of dollars, 我们 加载 这个 pretrained weights from Meta AI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f63cc248-1d27-4eb6-aa50-173b436652f8",
      "metadata": {
        "id": "f63cc248-1d27-4eb6-aa50-173b436652f8"
      },
      "source": [
        "&nbsp;\n",
        "## 4. 加载 pretrained weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aKeN7rUfqZMI",
      "metadata": {
        "id": "aKeN7rUfqZMI"
      },
      "source": [
        "- 我们 are loading 这个 [\"meta-llama/Meta-Llama-3-8B\"](https://huggingface.co/meta-llama/Meta-Llama-3-8B) base 模型 below, 哪个 is 一个 simple text completion 模型 before finetuning\n",
        "- Alternatively, 你 can 加载 这个 instruction-finetuned 和 aligned [\"meta-llama/Meta-Llama-3-8B-Instruct\"](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) 模型 by modifying 这个 string in 这个 接下来 代码 cell accordingly\n",
        "- Combined, 这个 权重 files are about 16 GB large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5fa9c06c-7a53-4b4d-9ce4-acc027322ee4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "f3788acce34f4956b0727b58d0cf38c6",
            "6022a9426683420690d9b41a0ca4f870",
            "e9aba3d53b4d45c485a7aad649c7b465",
            "f1a12d7929db4309b9881853135359fc",
            "58c9dec75a3346b1b787f88dd510d254",
            "9492edc02dee456f840325d913fa4e4f",
            "66dc94b23556499f985f8accbb1f89cb",
            "7c6658cfff1a4d27af3de148184f77d9",
            "7266a729edfb4a44b5b1c67dc79be146",
            "76dbab4873f342019c5d7624ae2c9775",
            "3cea4b431147441a8d9bd872811d5974",
            "8ae98969541849efa356cf912ac39b1e",
            "f9373112649945e3b446c3e1ec274dc1",
            "d49791082a304ade95c185c79fae1f41",
            "616e383bb3d442bcb6edb2721a8180b6",
            "87f474861e54432e9d533e0a89bb77da",
            "e805bb6dfee34dab8870f4618d8bffdb",
            "be3e9bf271f04eb0b119659e1af3a0ea",
            "00148825ce0248b7a23eb28e3eca6749",
            "f1a9b0c2431640298a6c1b258298b12d",
            "8ba9f009e92a46fcbcbb401dc444f12e",
            "d74186bb74d142dfb683fa347b6990f7",
            "9bb60a5a3710463ebe3a17f8d2a446be",
            "0a08fb81165748748ccb080e6df0600f",
            "603690f543114a7fb6aebd433c80bdc3",
            "773b802daed942f5a11f3eab3b83be08",
            "7989003a613e45f780d3f800e121543a",
            "9d49589118f5432cac49650251046429",
            "f114549fe8ce49638a791ca2fecb2d89",
            "0aa155b794a8426aa265f4a7670f43ad",
            "a06fbde549cc47fdaddfbdb82d35d823",
            "172c0c6955e1428b999dcb2d133704cd",
            "1bf7108774c34016a2193e2cd7639b7d",
            "ed28e180d94a4b7aa548581612e31232",
            "ff4338faded5494da1ccb660e1c441ed",
            "b46a08cf4929422eb0f76d8d9af11249",
            "f049eb4a50f54c34912ca959d2eaf353",
            "80dfd3e80ceb444a83ec1fd65f9af80e",
            "519147a10b984befbd0f255f78c1f66a",
            "562e82438dbe41b793ff488b8447c5bf",
            "1da83719e47c4196b06f3aa32056b560",
            "c4a2c88326d14fbca87cfde073755a2e",
            "f0ab5a46cbb0444c88ed137d8a95002b",
            "f8f28ac0e149428f9fef42373c6a87d0"
          ]
        },
        "id": "5fa9c06c-7a53-4b4d-9ce4-acc027322ee4",
        "outputId": "c05118ce-9f81-41c8-a1f2-72caa932ae86"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "245443330e4d40c887a5649cc1663e98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from safetensors.torch import load_file\n\ncombined_weights = {}\n\nfor i in range(1, 5):\n    weights_file = hf_hub_download(\n        repo_id=\"meta-llama/Meta-Llama-3-8B\",\n        filename=f\"model-0000{i}-of-00004.safetensors\",\n        local_dir=\"Llama-3-8B\"\n    )\n    current_weights = load_file(weights_file)\n    combined_weights.update(current_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-15SJ7btq2zE",
      "metadata": {
        "id": "-15SJ7btq2zE"
      },
      "source": [
        "- 这个 `weights` contains 这个 following tensors (only 这个 首先 15 are shown for simplicity):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ee26bd0b-fea9-4924-97f7-409c14f28e49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee26bd0b-fea9-4924-97f7-409c14f28e49",
        "outputId": "2fbc2786-677f-4fea-9472-5fb8542ff14b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['model.embed_tokens.weight',\n",
              " 'model.layers.0.input_layernorm.weight',\n",
              " 'model.layers.0.mlp.down_proj.weight',\n",
              " 'model.layers.0.mlp.gate_proj.weight',\n",
              " 'model.layers.0.mlp.up_proj.weight',\n",
              " 'model.layers.0.post_attention_layernorm.weight',\n",
              " 'model.layers.0.self_attn.k_proj.weight',\n",
              " 'model.layers.0.self_attn.o_proj.weight',\n",
              " 'model.layers.0.self_attn.q_proj.weight',\n",
              " 'model.layers.0.self_attn.v_proj.weight',\n",
              " 'model.layers.1.input_layernorm.weight',\n",
              " 'model.layers.1.mlp.down_proj.weight',\n",
              " 'model.layers.1.mlp.gate_proj.weight',\n",
              " 'model.layers.1.mlp.up_proj.weight',\n",
              " 'model.layers.1.post_attention_layernorm.weight']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(combined_weights.keys())[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UeeSpnunrDFB",
      "metadata": {
        "id": "UeeSpnunrDFB"
      },
      "source": [
        "- 这个 following 函数, modeled after 这个 `load_weights_into_gpt` 函数 in [第 5](../01_main-第-代码/ch05.ipynb), loads 这个 pretrained weights into our Llama 3 模型:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "3820e2a7-4f26-41bc-953b-f3879b0aff65",
      "metadata": {
        "id": "3820e2a7-4f26-41bc-953b-f3879b0aff65"
      },
      "outputs": [],
      "source": [
        "def assign(left, right, tensor_name=\"unknown\"):\n    if left.shape != right.shape:\n        raise ValueError(f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\")\n\n    if isinstance(right, torch.Tensor):\n        return torch.nn.Parameter(right.clone().detach())\n    else:\n        return torch.nn.Parameter(torch.tensor(right))\n\n\ndef load_weights_into_llama(model, param_config, params):\n    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n\n    for l in range(param_config[\"n_layers\"]):\n\n        # 加载 注意力机制 weights\n        model.trf_blocks[l].att.W_query.weight = assign(\n            model.trf_blocks[l].att.W_query.weight,\n            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n            f\"model.layers.{l}.self_attn.q_proj.weight\"\n        )\n        model.trf_blocks[l].att.W_key.weight = assign(\n            model.trf_blocks[l].att.W_key.weight,\n            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n            f\"model.layers.{l}.self_attn.k_proj.weight\"\n        )\n        model.trf_blocks[l].att.W_value.weight = assign(\n            model.trf_blocks[l].att.W_value.weight,\n            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n            f\"model.layers.{l}.self_attn.v_proj.weight\"\n        )\n        model.trf_blocks[l].att.out_proj.weight = assign(\n            model.trf_blocks[l].att.out_proj.weight,\n            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n            f\"model.layers.{l}.self_attn.o_proj.weight\"\n        )\n        model.trf_blocks[l].norm1.weight = assign(\n            model.trf_blocks[l].norm1.weight,\n            params[f\"model.layers.{l}.input_layernorm.weight\"],\n            f\"model.layers.{l}.input_layernorm.weight\"\n        )\n\n        # 加载 FeedForward weights\n        model.trf_blocks[l].ff.fc1.weight = assign(\n            model.trf_blocks[l].ff.fc1.weight,\n            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n            f\"model.layers.{l}.mlp.gate_proj.weight\"\n        )\n        model.trf_blocks[l].ff.fc2.weight = assign(\n            model.trf_blocks[l].ff.fc2.weight,\n            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n            f\"model.layers.{l}.mlp.up_proj.weight\"\n        )\n        model.trf_blocks[l].ff.fc3.weight = assign(\n            model.trf_blocks[l].ff.fc3.weight,\n            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n            f\"model.layers.{l}.mlp.down_proj.weight\"\n        )\n        model.trf_blocks[l].norm2.weight = assign(\n            model.trf_blocks[l].norm2.weight,\n            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n            f\"model.layers.{l}.post_attention_layernorm.weight\"\n        )\n\n    # 加载 输出 层 weights\n    model.final_norm.weight = assign(model.final_norm.weight, params[\"model.norm.weight\"], \"model.norm.weight\")\n\n    if \"lm_head.weight\" in params.keys():\n        model.out_head.weight = assign(model.out_head.weight, params[\"lm_head.weight\"], \"lm_head.weight\")\n    else:\n        model.out_head.weight = assign(model.out_head.weight, params[\"model.embed_tokens.weight\"], \"model.embed_tokens.weight\")\n        print(\"Model uses weight tying.\")\n\n\nload_weights_into_llama(model, LLAMA3_CONFIG_8B, combined_weights)\nmodel.to(device);\ndel combined_weights  # free up memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TDuv_Us2rNvk",
      "metadata": {
        "id": "TDuv_Us2rNvk"
      },
      "source": [
        "- 接下来, 我们 are ready to 使用 这个 模型 for text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "240987e8-a023-462e-9376-9edfb27559ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "240987e8-a023-462e-9376-9edfb27559ec",
        "outputId": "6dab0e56-40a8-45db-a096-ab2b9ee97a69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort has been made to trace copyright holders and to obtain their permission for the use of copyright material. The publisher apologizes for any\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n    max_new_tokens=25,\n    context_size=LLAMA3_CONFIG_8B[\"context_length\"],\n    top_k=1,\n    temperature=0.\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1203041e-4794-4157-a978-3ce80909da44",
      "metadata": {
        "id": "1203041e-4794-4157-a978-3ce80909da44"
      },
      "source": [
        "&nbsp;\n",
        "## 5. Using 这个 instruction-finetuned 模型"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "akyo7WNyF_YL",
      "metadata": {
        "id": "akyo7WNyF_YL"
      },
      "source": [
        "- Above, 我们 used 这个 pretrained base 模型; 如果 你 want to 使用 一个 模型 capable of following instructions, 使用 这个 `\"meta-llama/Llama-3-8B-Instruct\"` 模型 instead, as shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "hdA-xjjdS26J",
      "metadata": {
        "id": "hdA-xjjdS26J"
      },
      "outputs": [],
      "source": [
        "# to free up memory\n\nimport gc\n\ndel model\n\ngc.collect()  # 运行 Python garbage collector\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "nbvAV7vaz6yc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "409470784b6346a981920350de4f6f28",
            "9ba6a11ffd194bf9a0900f52a7ed4d4f",
            "acae8bbbb4a84ed49be72fecd11fb052",
            "e8a4b441281b4038bb0204d093411f68",
            "bdf8b693821344fc97918e6cbc31c8bf",
            "97e8877869cd4be68ff38ce745be5045",
            "cc3da88e93c4499993b7bbb7d3064326",
            "0d51fdc2c416474da04079db6579890f",
            "c4598300a77b4667b1117f9499f5ccb7",
            "77606cd2fe1b4d33a91ede944bb1dec0",
            "f1ba439c26d64c90af2f162c74348405",
            "d598f094c3ce4daeab19fac8094cba7e",
            "0afc2d23514b45c9890b5d2ee4e6fa0b",
            "3da5d38bf3314d3eaa7cedebae41c076",
            "55e6b727a4594078beb3853cc1891308",
            "f17fa78263414ef8b414c7bf3ac03192",
            "e8b187b40ec14db3af17a380830a35bf",
            "e94ca32eaa9f4714a3b05a5fdf24d02b",
            "3edd464991204b8690eae02f10b4cc00",
            "ac1e34f4bd6c420bb6cc2fdde5f3ed4d",
            "1cd5e07cad35450182004952de32c8e7",
            "a63351a6715643378491ba831b3fb05d",
            "98b4680141ee423bb5e43c47613d8440",
            "b02ffefca3f34252914e76f4a8a467dc",
            "31d27bf34a74432f8e0dbfe9ecb76130",
            "a3137f3669b54e84be91010c9654d985",
            "5a2886564d3f40ceaa30b743dbe81f45",
            "15ea8fcfe097471e8fc9502a162f5904",
            "c779e80c50ba4434bfa1d326c5cc9b0f",
            "eb94612785e64552aea8674dc8647a93",
            "279cffe683fe4e7383062162e07ed9ed",
            "6176990205cc499f8995c71fc6b9d4df",
            "66c23ae98bcc45f18fc5c91e0e73c3e4",
            "05b502e1e3a9436297dafbb1ce7af722",
            "25977b0d89084703ad787fe9208b5aad",
            "71a84ee5fc964ec89ff2832c84735cc2",
            "6aed783eccb942318e6384e253ad4924",
            "84c34bfecda64391a609e19f131d51d4",
            "20ecac7c646b45938ed393cb20977c37",
            "ebe04aeaaac042aaaa0885992e45793d",
            "ca81071ab07446df96795a482ce0c630",
            "e0550cab24c7492787af40dc4b8576bf",
            "7015bf6f85954036aaf8cc4f1c44ea0f",
            "2a2ba3d065634484a932b8d3c212af56"
          ]
        },
        "id": "nbvAV7vaz6yc",
        "outputId": "9e1badc9-a6c4-48b7-9125-e0810655528b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7df6bbf8e63448c8a6cb5d2f6208403",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:  36%|###6      | 1.81G/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4772f31a1c5b4c168c9aabe7a1d2bacc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad49eeb9e1204ea2bd2e371df8ccdea2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "951b9e81613a40a2a503f61e69677f0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "combined_weights = {}\n\nfor i in range(1, 5):\n    weights_file = hf_hub_download(\n        repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n        filename=f\"model-0000{i}-of-00004.safetensors\",\n        local_dir=\"Llama-3-8B-Instruct\"\n    )\n    current_weights = load_file(weights_file)\n    combined_weights.update(current_weights)\n\n\nmodel = Llama3Model(LLAMA3_CONFIG_8B)\nload_weights_into_llama(model, LLAMA3_CONFIG_8B, combined_weights)\nmodel.to(device)\ndel combined_weights  # free up memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VlH7qYVdDKQr",
      "metadata": {
        "id": "VlH7qYVdDKQr"
      },
      "source": [
        "- Note 那个 这个 Llama 3 模型 should ideally be used with 这个 correct prompt template 那个 was used during finetuning (as discussed in 第 7)\n",
        "- Below is 一个 wrapper 类 around 这个 分词器 based on Meta AI's Llama 3-specific [ChatFormat 代码](https://github.com/meta-llama/llama3/blob/11817d47e1ba7a4959b025eb1ca308572e0e3963/llama/分词器.py#L202) 那个 constructs 这个 prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "4be5b481-1110-46e8-a931-3988d890cf8c",
      "metadata": {
        "id": "4be5b481-1110-46e8-a931-3988d890cf8c"
      },
      "outputs": [],
      "source": [
        "class ChatFormat:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def encode_header(self, message):\n        tokens = []\n        tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n        tokens.extend(self.tokenizer.encode(message[\"role\"], bos=False, eos=False))\n        tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n        return tokens\n\n    def encode(self, text):\n        message = {\n            \"role\": \"user\",\n            \"content\": text\n        }\n\n        tokens = self.encode_header(message)\n        tokens.extend(\n            self.tokenizer.encode(message[\"content\"].strip(), bos=False, eos=False)\n        )\n        tokens.append(self.tokenizer.special_tokens[\"<|eot_id|>\"])\n        return tokens\n\n    def decode(self, token_ids):\n        return self.tokenizer.decode(token_ids)\n\n\nchat_tokenizer = ChatFormat(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M-dkSNvwDttN",
      "metadata": {
        "id": "M-dkSNvwDttN"
      },
      "source": [
        "- 这个 usage is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "nwBrTGTsUNhn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwBrTGTsUNhn",
        "outputId": "72a495b4-b872-429a-88ef-49a9b4577f0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[128006, 882, 128007, 271, 9906, 4435, 0, 128009]\n"
          ]
        }
      ],
      "source": [
        "token_ids = chat_tokenizer.encode(\"Hello World!\")\nprint(token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "0fpmpVgYVTRZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0fpmpVgYVTRZ",
        "outputId": "bb3e819a-112a-466c-ac51-5d14a9c3475b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|start_header_id|>user<|end_header_id|>\\n\\nHello World!<|eot_id|>'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wo-aUGeKDvqq",
      "metadata": {
        "id": "Wo-aUGeKDvqq"
      },
      "source": [
        "- 让我们 现在 see 这个 Llama 3 instruction 模型 in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "ozGOBu6XOkEW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozGOBu6XOkEW",
        "outputId": "4f689c70-bed9-46f3-a52a-aea47b641283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Llamas are herbivores, which means they primarily eat plants and plant-based foods. Here are some of the things llamas like to eat:\n",
            "\n",
            "1. Grass: Llamas love to graze on grass, especially in the spring and summer months.\n",
            "2. Hay: Hay is a staple in a llama's diet. They like to eat timothy hay, alfalfa hay, and other types of hay.\n",
            "3. Grains: Llamas may also be fed grains like oats, barley, and corn. However, grains should not make up more than 10-15% of a llama's diet.\n",
            "4. Fruits and vegetables: Llamas may enjoy fruits and vegetables as treats, such as\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(\"What do llamas eat?\", chat_tokenizer).to(device),\n    max_new_tokens=150,\n    context_size=LLAMA3_CONFIG_8B[\"context_length\"],\n    top_k=1,\n    temperature=0.\n)\n\noutput_text = token_ids_to_text(token_ids, tokenizer)\n\n\ndef clean_text(text, header_end=\"assistant<|end_header_id|>\\n\\n\"):\n    # Find 这个 index of 这个 首先 occurrence of \"<|end_header_id|>\"\n    index = text.find(header_end)\n\n    if index != -1:\n        # 返回 这个 substring starting after \"<|end_header_id|>\"\n        return text[index + len(header_end):].strip()  # Strip removes leading/trailing whitespace\n    else:\n        # 如果 这个 词元 is not found, 返回 这个 original text\n        return text\n\nprint(\"Output text:\\n\", clean_text(output_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2r5JKrO-ZOHK",
      "metadata": {
        "id": "2r5JKrO-ZOHK"
      },
      "source": [
        "&nbsp;\n",
        "# Llama 3.1 8B"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QiQxX0XnP_iC",
      "metadata": {
        "id": "QiQxX0XnP_iC"
      },
      "source": [
        "- 一个 few months after 这个 initial Llama 3 release, Meta AI followed up with their Llama 3.1 suite of models (see 这个 official [Introducing Llama 3.1: Our most capable models to date](https://ai.meta.com/blog/meta-llama-3-1/) announcement blog post for details)\n",
        "- Conveniently, 我们 can reuse our previous Llama 3 代码 from above to 实现 Llama 3.1 8B\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/GPT-to-llama/llama3-to-llama31.webp\" width=\"700px\">\n",
        "\n",
        "- 这个 architecture is identical, with 这个 only 改变 being 一个 rescaling of 这个 RoPE frequencies as indicated in 这个 配置 file below\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "X5Fg8XUHMv4M",
      "metadata": {
        "id": "X5Fg8XUHMv4M"
      },
      "outputs": [],
      "source": [
        "LLAMA3_CONFIG_8B = {\n    \"vocab_size\": 128_256,   # Vocabulary size\n    \"context_length\": 8192,  # Context length\n    \"emb_dim\": 4096,         # 嵌入 dimension\n    \"n_heads\": 32,           # Number of 注意力机制 heads\n    \"n_layers\": 32,          # Number of layers\n    \"hidden_dim\": 14_336,    # Size of 这个 intermediate dimension in FeedForward\n    \"n_kv_groups\": 8,        # Key-Value groups for grouped-query 注意力机制\n    \"rope_base\": 500_000.0,  # 这个 base in RoPE's \"theta\"\n    \"rope_freq\": None,       # Additional 配置 for adjusting 这个 RoPE frequencies\n    \"dtype\": torch.bfloat16  # Lower-precision dtype to reduce memory usage\n}\n\nLLAMA31_CONFIG_8B = {\n    \"vocab_size\": 128_256,      # Vocabulary size\n    \"context_length\": 131_072,  # NEW: Larger supported context length\n    \"emb_dim\": 4096,            # 嵌入 dimension\n    \"n_heads\": 32,              # Number of 注意力机制 heads\n    \"n_layers\": 32,             # Number of layers\n    \"hidden_dim\": 14_336,       # Size of 这个 intermediate dimension in FeedForward\n    \"n_kv_groups\": 8,           # Key-Value groups for grouped-query 注意力机制\n    \"rope_base\": 500_000.0,     # 这个 base in RoPE's \"theta\"\n    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n    \"rope_freq\": {              # NEW: RoPE frequency scaling\n        \"factor\": 8.0,\n        \"low_freq_factor\": 1.0,\n        \"high_freq_factor\": 4.0,\n        \"original_context_length\": 8192,\n    }\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d81ee464-c112-43b0-9ee8-70df6ac942d0",
      "metadata": {},
      "source": [
        "- Reduce 这个 context length so 这个 模型 would work fine on 一个 MacBook Air (如果 你 have more RAM, feel free to comment out 这个 lines below):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a55a8769-1a03-4265-8fd0-15f1c423da53",
      "metadata": {
        "id": "a8bc2370-39d2-4bfe-b4c1-6bdd75fe101c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New RoPE theta: 31250.0\n"
          ]
        }
      ],
      "source": [
        "old_context_length = LLAMA31_CONFIG_8B[\"context_length\"]\nLLAMA31_CONFIG_8B[\"context_length\"] = 8192\n\n\ndef rescale_theta(theta_old, context_length_old, context_length_new):\n    scaling_factor = context_length_new / context_length_old\n    theta_new = theta_old * scaling_factor\n    return theta_new\n\nLLAMA31_CONFIG_8B[\"rope_base\"] = rescale_theta(\n    LLAMA31_CONFIG_8B[\"rope_base\"],\n    old_context_length,\n    LLAMA31_CONFIG_8B[\"context_length\"]\n)\n\nprint(\"New RoPE theta:\", LLAMA31_CONFIG_8B[\"rope_base\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xa3bpMDtTdBs",
      "metadata": {
        "id": "xa3bpMDtTdBs"
      },
      "source": [
        "- As 我们've seen in 这个 代码 earlier, 这个 RoPE 方法 uses sinusoidal functions (sine 和 cosine) to embed positional information directly into 这个 注意力机制 mechanism\n",
        "- In Llama 3.1, via 这个 additional 配置, 我们 introduce additional adjustments to 这个 inverse frequency calculations\n",
        "- These adjustments influence 如何 different frequency components contribute to 这个 positional embeddings (一个 detailed explanation is 一个 topic for another time)\n",
        "- 让我们 try out 这个 Llama 3.1 模型 in practice; 首先, 我们 清除 out 这个 old 模型 to free up some GPU memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "7dUtYnNUOqhL",
      "metadata": {
        "id": "7dUtYnNUOqhL"
      },
      "outputs": [],
      "source": [
        "# free up memory\ndel model\n\ngc.collect()  # 运行 Python garbage collector\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DbbVsll6TYWR",
      "metadata": {
        "id": "DbbVsll6TYWR"
      },
      "source": [
        "- 接下来, 我们 download 这个 分词器\n",
        "- Note 那个 since 这个 Llama 3.1 family is distinct from 这个 Llama 3 family, 你'd have to go to 这个 [meta-llama/Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) repository 和 acknowledge 这个 license terms for your Hugging Face access 词元 to work for 这个 download\n",
        "- Tip: For simplicity, 我们 only 加载 这个 base 模型 below, 但是 那里's also 一个 instruction-finetuned version 你 can 使用 by replacing `\"meta-llama/Llama-3.1-8B\"` with `\"meta-llama/Llama-3.1-8B-Instruct\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "8xDk4chtPNU4",
      "metadata": {
        "id": "8xDk4chtPNU4"
      },
      "outputs": [],
      "source": [
        "tokenizer_file_path = hf_hub_download(\n    repo_id=\"meta-llama/Llama-3.1-8B\",\n    filename=\"original/tokenizer.model\",\n    local_dir=\"Llama-3.1-8B\"\n)\n\ntokenizer = Tokenizer(tokenizer_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "a7l21VE4Otcs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7l21VE4Otcs",
        "outputId": "3dd5cfba-bf3f-44d2-9be1-7cd42bfe4ba9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 8,030,261,248\n"
          ]
        }
      ],
      "source": [
        "model = Llama3Model(LLAMA31_CONFIG_8B)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total number of parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "u4J7IxOvOyPM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "5bbaa046d8934c8fae0a12c3d7bd991b",
            "e1e4125eac004bae92dc1f22f673bf0e",
            "d5b4bb4891ec4e44be46e9815c7e10dc",
            "4f6595a392b244bd8e887935defc06f0",
            "100c1b15cc4046cea1147f657eb2d8d0",
            "81458e7953a349cfafccaa213b370406",
            "a3dc9dfadae642b4a873705596739468",
            "f55b59efcefa4ad5955d082f4bf7c637",
            "1b02e0c7d1604b1c87a327c4c4f8b0e7",
            "02ad170019454fd096b37347de5c481d",
            "c52e0f34892b4daa84c1bf61500ac399",
            "af985cf6fa26475eb2c4dd81e0c79ff4",
            "8659c3eddb014c3bb5931fd9e6fadad8",
            "f5fa00d96c4c49e48e1806d23a5b8570",
            "080c484114f64f5591fa1287a35b46c9",
            "14dc6a3717484c55a116612e28447dbb",
            "00d3286c9c1d4161bb777b7b65ae744d",
            "66f27fb11edf453b8144c2dfcdc66baa",
            "5798e5118430439fb1f6bf29e1bafe58",
            "357f367cf74146b8825be371acd51d06",
            "94073be250cd42d5b82e196e30cbf22e",
            "0cd0724f825e480389a82f0c49f91e6d",
            "dffa208978f34e6a9aae94ecda92fe67",
            "b8a98f163ebd4ac89af08a49c0881c23",
            "f0d9febe1a634a0ba7e8e50fa104dcc2",
            "e23870f0c7ff40cc8fa6a1e862a4af99",
            "87da9905a0534c26ad0712ad426ca930",
            "b953419300604b8e86fc0ad003fdfd2f",
            "f1865ed0fbcc40eeabdca90a43d00069",
            "ea0128909a9d4801ba312a876b0cf183",
            "d160986df978416c9ad91d1e10fc90fc",
            "5e97f7c2e8f5453dafcdad0552060e60",
            "4b3e7b8774df4b458bb6c6146fe3226d",
            "2ffd8dbed00e46d2887b9a2590cad297",
            "a06dcb3bdfc84905a7222066c32fe500",
            "e7602abc26714ee890a0cf5c0c7b67e1",
            "dc5d555099f64a998514ebde90eeb6df",
            "ef93a2f58cc54373941f43658bb808cf",
            "fea1e2327d2944859af3d91c216b9008",
            "320c00a5d18c45ccae634d166f1bd810",
            "6c857e69d5204cd3b7c3bf426993ad1f",
            "2145e47428f1446fba3e62b3cde0a7f5",
            "3d519ce3562c4e249bf392c7f43d04c0",
            "cc20ffcf0c1a4656945959bf457dfd84"
          ]
        },
        "id": "u4J7IxOvOyPM",
        "outputId": "925348d7-fc69-4d1b-90f1-7029426bcfcf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eabfde3ef38b436ea750e6fb50a02b5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e117ad45771747ae95c16f9876e6dc19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "170185f2f046437dab57c2ad23163c5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e65f5d6c5af4ab78bc7b3778b98ef86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "combined_weights = {}\n\nfor i in range(1, 5):\n    weights_file = hf_hub_download(\n        repo_id=\"meta-llama/Llama-3.1-8B\",\n        filename=f\"model-0000{i}-of-00004.safetensors\",\n        local_dir=\"Llama-3.1-8B\"\n    )\n    current_weights = load_file(weights_file)\n    combined_weights.update(current_weights)\n\nload_weights_into_llama(model, LLAMA31_CONFIG_8B, combined_weights)\nmodel.to(device);\ndel combined_weights  # free up memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "wJFnF8ATPbtD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJFnF8ATPbtD",
        "outputId": "67d5cb66-3588-4fd4-ac75-39bfe3aa82d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort has been made to trace copyright holders and to obtain their permission for the use of copyright material. The publisher apologizes for any\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n    max_new_tokens=25,\n    context_size=LLAMA31_CONFIG_8B[\"context_length\"],\n    top_k=1,\n    temperature=0.\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DR9NBDUjPrDp",
      "metadata": {
        "id": "DR9NBDUjPrDp"
      },
      "source": [
        "&nbsp;\n",
        "# Llama 3.2 1B"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imoxFiDzJcxk",
      "metadata": {
        "id": "imoxFiDzJcxk"
      },
      "source": [
        "- As of 这个 writing, Meta AI's latest models are 这个 Llama 3.2 models announced [这里](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)\n",
        "- 这个 代码 for 这个 Llama 3.2 text 模型 is similar to 那个 of Llama 3.1, except 那个 这个 模型 has shrunk in size (那里 is 一个 1B 和 3B version)\n",
        "- 这个 other efficiency tweak was 那个 they added back 权重 tying (一个 concept 那个 was original used in 这个 GPT-2 architecture); 这里, they reuse 这个 same 权重 参数 values in 这个 输入 (词元) 嵌入 层 和 输出 层\n",
        "- 这个 small 模型 size of Llama 3.2 1B is quite convenient, since 它 can even 运行 on many mobile devices\n",
        "- 这个 architectural differences between Llama 3.1 8B 和 Llama 3.2 1B are illustrated in 这个 figure below"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OL1EoXQ6TPb7",
      "metadata": {
        "id": "OL1EoXQ6TPb7"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/GPT-to-llama/llama31-to-llama32.webp?1\" width=\"700px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K0KgjwCCJ9Fb",
      "metadata": {
        "id": "K0KgjwCCJ9Fb"
      },
      "source": [
        "- As 我们 can see based on 这个 figure above, 这个 main difference between 这个 Llama 3.1 8B 和 Llama 3.2 1B architectures are 这个 respective sizes\n",
        "- 一个 small additional 改变 is 一个 increased RoPE rescaling factor, 哪个 is reflected in 这个 配置 file below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "Yv_yF3NCQTBx",
      "metadata": {
        "id": "Yv_yF3NCQTBx"
      },
      "outputs": [],
      "source": [
        "LLAMA31_CONFIG_8B = {\n    \"vocab_size\": 128_256,      # Vocabulary size\n    \"context_length\": 131_072,  # NEW: Larger supported context length\n    \"emb_dim\": 4096,            # 嵌入 dimension\n    \"n_heads\": 32,              # Number of 注意力机制 heads\n    \"n_layers\": 32,             # Number of layers\n    \"hidden_dim\": 14_336,       # Size of 这个 intermediate dimension in FeedForward\n    \"n_kv_groups\": 8,           # Key-Value groups for grouped-query 注意力机制\n    \"rope_base\": 500_000.0,     # 这个 base in RoPE's \"theta\"\n    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usagey\n    \"rope_freq\": {              # NEW: RoPE frequency scaling\n        \"factor\": 8.0,\n        \"low_freq_factor\": 1.0,\n        \"high_freq_factor\": 4.0,\n        \"original_context_length\": 8192,\n    }\n}\n\n\nLLAMA32_CONFIG_1B = {\n    \"vocab_size\": 128_256,      # Vocabulary size\n    \"context_length\": 131_072,  # Context length\n    \"emb_dim\": 2048,            # NEW: Half 这个 嵌入 dimension\n    \"n_heads\": 32,              # Number of 注意力机制 heads\n    \"n_layers\": 16,             # NEW: Half 这个 number of layers\n    \"hidden_dim\": 8192,         # NEW: Almost half 这个 size of 这个 intermediate dimension in FeedForward\n    \"n_kv_groups\": 8,           # Key-Value groups for grouped-query 注意力机制\n    \"rope_base\": 500_000.0,     # 这个 base in RoPE's \"theta\"\n    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n    \"rope_freq\": {              # RoPE frequency scaling\n        \"factor\": 32.0,         # NEW: Adjustment of 这个 rescaling factor\n        \"low_freq_factor\": 1.0,\n        \"high_freq_factor\": 4.0,\n        \"original_context_length\": 8192,\n    }\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5cd351b-d883-460d-9cdc-47e15ddb884a",
      "metadata": {},
      "source": [
        "- Reduce 这个 context length so 这个 模型 would work fine on 一个 MacBook Air (如果 你 have more RAM, feel free to comment out 这个 lines below):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "73f001a6-7ae0-4204-aa83-a27a8878dfd2",
      "metadata": {
        "id": "a8bc2370-39d2-4bfe-b4c1-6bdd75fe101c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New RoPE theta: 31250.0\n"
          ]
        }
      ],
      "source": [
        "old_context_length = LLAMA32_CONFIG_1B[\"context_length\"]\nLLAMA32_CONFIG_1B[\"context_length\"] = 8192\n\nLLAMA32_CONFIG_1B[\"rope_base\"] = rescale_theta(\n    LLAMA32_CONFIG_1B[\"rope_base\"],\n    old_context_length,\n    LLAMA32_CONFIG_1B[\"context_length\"]\n)\n\nprint(\"New RoPE theta:\", LLAMA32_CONFIG_1B[\"rope_base\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dl4_0EoJKKYv",
      "metadata": {
        "id": "Dl4_0EoJKKYv"
      },
      "source": [
        "- Below, 我们 can reuse 这个 代码 from 这个 Llama 3.1 8B section to 加载 这个 Llama 3.2 1B 模型\n",
        "- Again, since 这个 Llama 3.2 family is distinct from 这个 Llama 3.1 family, 你'd have to go to 这个 [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B) repository 和 acknowledge 这个 license terms for your Hugging Face access 词元 to work for 这个 download\n",
        "- Tip: For simplicity, 我们 only 加载 这个 base 模型 below, 但是 那里's also 一个 instruction-finetuned version 你 can 使用 by replacing `\"meta-llama/Llama-3.2-1B\"` with `\"meta-llama/Llama-3.2-1B-Instruct\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "tCstHgyRRD2x",
      "metadata": {
        "id": "tCstHgyRRD2x"
      },
      "outputs": [],
      "source": [
        "# free up memory\ndel model\n\n\ngc.collect()  # 运行 Python garbage collector\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "jt8BKAHXRCPI",
      "metadata": {
        "id": "jt8BKAHXRCPI"
      },
      "outputs": [],
      "source": [
        "tokenizer_file_path = hf_hub_download(\n    repo_id=\"meta-llama/Llama-3.2-1B\",\n    filename=\"original/tokenizer.model\",\n    local_dir=\"Llama-3.2-1B\"\n)\n\ntokenizer = Tokenizer(tokenizer_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "uf8KjasmRFSt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf8KjasmRFSt",
        "outputId": "4e718852-2aa1-4b5a-bec3-3d5f866a4038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 1,498,482,688\n",
            "\n",
            "Total number of unique parameters: 1,235,814,400\n"
          ]
        }
      ],
      "source": [
        "model = Llama3Model(LLAMA32_CONFIG_1B)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total number of parameters: {total_params:,}\")\n\n# Account for 权重 tying\ntotal_params_normalized = total_params - model.tok_emb.weight.numel()\nprint(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "9FbCIYW7RIOe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FbCIYW7RIOe",
        "outputId": "35588405-e2e1-4871-a1db-1d4bcb852e49"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c309c56a6cdf426e8ba7967b6a21864e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model uses weight tying.\n"
          ]
        }
      ],
      "source": [
        "weights_file = hf_hub_download(\n    repo_id=\"meta-llama/Llama-3.2-1B\",\n    filename=\"model.safetensors\",\n    local_dir=\"Llama-3.2-1B\"\n)\ncurrent_weights = load_file(weights_file)\n\nload_weights_into_llama(model, LLAMA32_CONFIG_1B, current_weights)\nmodel.to(device);\ndel current_weights  # free up memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "pPp5yjir6FYJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPp5yjir6FYJ",
        "outputId": "6c8e79d2-0769-43a7-93b3-f04c030e1aac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weight tying: True\n"
          ]
        }
      ],
      "source": [
        "print(\"Weight tying:\", torch.equal(model.tok_emb.weight, model.out_head.weight))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "3kh7yrw2W4qr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kh7yrw2W4qr",
        "outputId": "b7e66a17-57ec-4b0e-c4ff-8d9a6b8e6ea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort is made to ensure that the information on this website is accurate. However, we cannot guarantee that the information is accurate, complete\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n    max_new_tokens=25,\n    context_size=LLAMA32_CONFIG_1B[\"context_length\"],\n    top_k=1,\n    temperature=0.\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VO4Qf0zyW1ZC",
      "metadata": {
        "id": "VO4Qf0zyW1ZC"
      },
      "source": [
        "&nbsp;\n",
        "# 什么's 接下来?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CjCewpo2XPAd",
      "metadata": {
        "id": "CjCewpo2XPAd"
      },
      "source": [
        "- 这个 笔记本 concludes 这个 conversion from GPT to Llama 3.2\n",
        "- 如果 你 are interested in 一个 more compact, standalone 笔记本, 哪个 only contains 这个 Llama 3.2 代码, 检查 out 这个 [standalone-llama32.ipynb](standalone-llama32.ipynb) 笔记本"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}